"""
Agent Orchestrator for Multi-Agent System

LangGraph ê¸°ë°˜ ì—ì´ì „íŠ¸ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ ì‹œìŠ¤í…œ
4ëŒ€ í•µì‹¬ ì—ì´ì „íŠ¸ë¥¼ ì¡°ìœ¨í•˜ì—¬ í˜‘ì—… ì›Œí¬í”Œë¡œìš° êµ¬ì¶•
"""

import asyncio
import logging
import json
import operator
import os
import re
from pathlib import Path
from typing import Dict, Any, List, Optional, Literal, Annotated
from datetime import datetime
from dataclasses import dataclass, field

from langchain_core.messages import AIMessage, HumanMessage, SystemMessage
from langgraph.graph import StateGraph, END
from langgraph.graph.message import add_messages
from pydantic import BaseModel, Field
from typing_extensions import TypedDict

from src.core.shared_memory import get_shared_memory, MemoryScope
from src.core.skills_manager import get_skill_manager
from src.core.skills_selector import get_skill_selector, SkillMatch
from src.core.skills_loader import Skill
from src.core.agent_result_sharing import SharedResultsManager, AgentDiscussionManager
from src.core.researcher_config import get_agent_config
from src.core.mcp_auto_discovery import FastMCPMulti
from src.core.mcp_tool_loader import MCPToolLoader
from src.core.agent_tool_selector import AgentToolSelector, AgentType
from src.core.session_manager import get_session_manager, SessionManager
from src.core.context_engineer import get_context_engineer
from src.core.memory_service import get_background_memory_service
# prompt refinerëŠ” execute_llm_taskì˜ decoratorì—ì„œ ìë™ ì ìš©ë¨

logger = logging.getLogger(__name__)

# HTTP ì—ëŸ¬ ë©”ì‹œì§€ í•„í„°ë§ í´ë˜ìŠ¤
class HTTPErrorFilter(logging.Filter):
    """HTML ì—ëŸ¬ ì‘ë‹µì„ í•„í„°ë§í•˜ì—¬ ê°„ë‹¨í•œ ë©”ì‹œì§€ë§Œ ì¶œë ¥"""
    def filter(self, record):
        message = record.getMessage()
        
        # HTML ì—ëŸ¬ í˜ì´ì§€ ê°ì§€ ë° í•„í„°ë§
        if '<!DOCTYPE html>' in message or '<html' in message.lower():
            # HTMLì—ì„œ ì—ëŸ¬ ë©”ì‹œì§€ ì¶”ì¶œ ì‹œë„
            
            # HTTP ìƒíƒœ ì½”ë“œ ì¶”ì¶œ
            status_match = re.search(r'HTTP (\d{3})', message)
            status_code = status_match.group(1) if status_match else "Unknown"
            
            # ì—ëŸ¬ ì œëª© ì¶”ì¶œ ì‹œë„
            title_match = re.search(r'<title>([^<]+)</title>', message, re.IGNORECASE)
            error_title = title_match.group(1).strip() if title_match else None
            
            # ê°„ë‹¨í•œ ì—ëŸ¬ ë©”ì‹œì§€ ìƒì„±
            if error_title:
                record.msg = f"HTTP {status_code}: {error_title}"
            else:
                # ìƒíƒœ ì½”ë“œì— ë”°ë¥¸ ê¸°ë³¸ ë©”ì‹œì§€
                if status_code == "502":
                    record.msg = f"HTTP {status_code}: Bad Gateway - Server temporarily unavailable"
                elif status_code == "504":
                    record.msg = f"HTTP {status_code}: Gateway Timeout - Server response timeout"
                elif status_code == "503":
                    record.msg = f"HTTP {status_code}: Service Unavailable - Server temporarily unavailable"
                elif status_code == "401":
                    record.msg = f"HTTP {status_code}: Unauthorized - Authentication failed"
                elif status_code == "404":
                    record.msg = f"HTTP {status_code}: Not Found"
                elif status_code == "500":
                    record.msg = f"HTTP {status_code}: Internal Server Error"
                else:
                    record.msg = f"HTTP {status_code}: Server Error"
            
            record.args = ()  # args ì´ˆê¸°í™”
        
        return True

# Loggerê°€ handlerê°€ ì—†ìœ¼ë©´ root loggerì˜ handler ì‚¬ìš©
if not logger.handlers:
    logger.setLevel(logging.INFO)
    # Root loggerì˜ handler ì‚¬ìš© (main.pyì—ì„œ ì„¤ì •ëœ handler)
    parent_logger = logging.getLogger()
    if parent_logger.handlers:
        logger.handlers = parent_logger.handlers
        logger.propagate = True
    else:
        # Fallback: ê¸°ë³¸ handler ì„¤ì •
        handler = logging.StreamHandler()
        handler.setFormatter(logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s'))
        handler.addFilter(HTTPErrorFilter())  # HTTP ì—ëŸ¬ í•„í„° ì¶”ê°€
        logger.addHandler(handler)
        logger.setLevel(logging.INFO)
else:
    # ê¸°ì¡´ handlerì— í•„í„° ì¶”ê°€
    for handler in logger.handlers:
        if not any(isinstance(f, HTTPErrorFilter) for f in handler.filters):
            handler.addFilter(HTTPErrorFilter())

# FastMCP Runner ë¡œê±°ì—ë„ í•„í„° ì¶”ê°€ (ì™¸ë¶€ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œê¹… í•„í„°ë§)
# Runner ë¡œê±°ëŠ” ë‚˜ì¤‘ì— ìƒì„±ë  ìˆ˜ ìˆìœ¼ë¯€ë¡œ, propagateë¥¼ í™œì„±í™”í•˜ê³  root loggerì˜ í•„í„° ì‚¬ìš©
def setup_runner_logger_filter():
    """Runner ë¡œê±°ì— HTML í•„í„° ì¶”ê°€ (ì§€ì—° ì´ˆê¸°í™”)"""
    runner_logger = logging.getLogger("Runner")
    if runner_logger:
        runner_logger.propagate = True  # Root loggerë¡œ ì „íŒŒí•˜ì—¬ í•„í„° ì ìš©
        # ê¸°ì¡´ handlerì— í•„í„° ì¶”ê°€ (í˜¹ì‹œ ì§ì ‘ handlerê°€ ìˆëŠ” ê²½ìš°)
        for handler in runner_logger.handlers:
            if not any(isinstance(f, HTTPErrorFilter) for f in handler.filters):
                handler.addFilter(HTTPErrorFilter())

# ì´ˆê¸° ì„¤ì •
setup_runner_logger_filter()


###################
# State Definitions
###################

def override_reducer(current_value, new_value):
    """Reducer function that allows overriding values in state."""
    if isinstance(new_value, dict) and new_value.get("type") == "override":
        return new_value.get("value", new_value)
    else:
        return operator.add(current_value, new_value)


class AgentState(TypedDict):
    """Main agent state containing messages and research data."""
    
    messages: Annotated[list, add_messages]
    user_query: str
    research_plan: Optional[str]
    research_tasks: Annotated[list, override_reducer]  # List of research tasks for parallel execution
    research_results: Annotated[list, override_reducer]  # Changed: supports both dict and str
    verified_results: Annotated[list, override_reducer]  # Changed: supports both dict and str
    final_report: Optional[str]
    
    # Human-in-the-loop ê´€ë ¨ í•„ë“œ
    pending_questions: Optional[List[Dict[str, Any]]]  # ëŒ€ê¸° ì¤‘ì¸ ì§ˆë¬¸ë“¤
    user_responses: Optional[Dict[str, Any]]  # ì§ˆë¬¸ ID -> ì‚¬ìš©ì ì‘ë‹µ
    clarification_context: Optional[Dict[str, Any]]  # ëª…í™•í™”ëœ ì •ë³´
    waiting_for_user: Optional[bool]  # ì‚¬ìš©ì ì‘ë‹µ ëŒ€ê¸° ì¤‘ì¸ì§€
    current_agent: Optional[str]
    iteration: int
    session_id: Optional[str]
    research_failed: bool
    verification_failed: bool
    report_failed: bool
    error: Optional[str]


###################
# Agent Definitions
###################

@dataclass
class AgentContext:
    """Agent execution context."""
    agent_id: str
    session_id: str
    shared_memory: Any
    config: Any = None
    shared_results_manager: Optional[SharedResultsManager] = None
    discussion_manager: Optional[AgentDiscussionManager] = None


class PlannerAgent:
    """Planner agent - creates research plans (YAML-based configuration)."""
    
    def __init__(self, context: AgentContext, skill: Optional[Skill] = None):
        self.context = context
        self.name = "planner"
        self.available_tools: list = []  # MCP ìë™ í• ë‹¹ ë„êµ¬
        self.tool_infos: list = []  # ë„êµ¬ ë©”íƒ€ë°ì´í„°
        self.skill = skill
        
        # YAML ì„¤ì • ë¡œë“œ
        from src.core.skills.agent_loader import load_agent_config
        self.config = load_agent_config("planner")
        self.instruction = self.config.instructions
    
    async def domain_exploration(self, query: str) -> Dict[str, Any]:
        """
        ë„ë©”ì¸ ë¶„ì„ ë° íƒìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
        
        Args:
            query: ì—°êµ¬ ì§ˆë¬¸
            
        Returns:
            ë„ë©”ì¸ ë¶„ì„ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        logger.info(f"[{self.name}] ğŸ” Starting domain exploration for query: {query[:100]}...")
        
        from src.core.llm_manager import execute_llm_task, TaskType
        from src.core.skills.agent_loader import get_prompt
        
        try:
            # ë„ë©”ì¸ ë¶„ì„ í”„ë¡¬í”„íŠ¸ ê°€ì ¸ì˜¤ê¸°
            domain_prompt = get_prompt("planner", "domain_analysis", query=query)
            system_message = "You are a domain analysis expert. Analyze the research domain to understand its characteristics, terminology, and requirements."
            
            # domain_promptì™€ system_messageëŠ” execute_llm_taskì˜ decoratorì—ì„œ ìë™ìœ¼ë¡œ ìµœì í™”ë¨
            domain_result = await execute_llm_task(
                prompt=domain_prompt,
                task_type=TaskType.ANALYSIS,
                model_name=None,
                system_message=system_message
            )
            
            # JSON íŒŒì‹± ì‹œë„
            domain_text = domain_result.content or "{}"
            
            # JSON ë¸”ë¡ ì¶”ì¶œ
            json_match = re.search(r'\{[\s\S]*\}', domain_text)
            if json_match:
                try:
                    domain_analysis = json.loads(json_match.group())
                except json.JSONDecodeError:
                    logger.warning(f"[{self.name}] Failed to parse domain analysis JSON, using default structure")
                    domain_analysis = {
                        "domain": "general",
                        "subdomains": [],
                        "characteristics": [],
                        "key_terminology": [],
                        "data_types": ["quantitative", "qualitative"],
                        "reliable_source_types": ["academic", "news", "government"],
                        "verification_criteria": ["source_reliability", "data_recency"],
                        "search_strategy": {
                            "keywords": [],
                            "related_topics": []
                        }
                    }
            else:
                logger.warning(f"[{self.name}] No JSON found in domain analysis result, using default structure")
                domain_analysis = {
                    "domain": "general",
                    "subdomains": [],
                    "characteristics": [],
                    "key_terminology": [],
                    "data_types": ["quantitative", "qualitative"],
                    "reliable_source_types": ["academic", "news", "government"],
                    "verification_criteria": ["source_reliability", "data_recency"],
                    "search_strategy": {
                        "keywords": [],
                        "related_topics": []
                    }
                }
            
            # ë©”íƒ€ë°ì´í„° ì¶”ê°€
            domain_analysis['_metadata'] = {
                'model_used': domain_result.model_used,
                'confidence': domain_result.confidence,
                'execution_time': domain_result.execution_time,
                'timestamp': domain_result.timestamp if hasattr(domain_result, 'timestamp') else None
            }
            
            logger.info(f"[{self.name}] âœ… Domain analysis completed: {domain_analysis.get('domain', 'unknown')}")
            logger.info(f"[{self.name}] Domain characteristics: {domain_analysis.get('characteristics', [])}")
            logger.info(f"[{self.name}] Reliable source types: {domain_analysis.get('reliable_source_types', [])}")
            
            return domain_analysis
            
        except Exception as e:
            logger.error(f"[{self.name}] Domain exploration failed: {e}")
            # ê¸°ë³¸ ë„ë©”ì¸ ë¶„ì„ ê²°ê³¼ ë°˜í™˜
            return {
                "domain": "general",
                "subdomains": [],
                "characteristics": [],
                "key_terminology": [],
                "data_types": ["quantitative", "qualitative"],
                "reliable_source_types": ["academic", "news", "government"],
                "verification_criteria": ["source_reliability", "data_recency"],
                "search_strategy": {
                    "keywords": [],
                    "related_topics": []
                },
                "_metadata": {
                    "error": str(e)
                }
            }
    
    async def _detect_economic_request(self, query: str, domain_analysis: Dict[str, Any]) -> bool:
        """
        LLMì„ ì‚¬ìš©í•˜ì—¬ ì‚¬ìš©ì ìš”ì²­ì´ ê²½ì œ/ê¸ˆìœµ ê´€ë ¨ì¸ì§€ íŒë‹¨í•©ë‹ˆë‹¤.
        
        Args:
            query: ì‚¬ìš©ì ìš”ì²­
            domain_analysis: ë„ë©”ì¸ ë¶„ì„ ê²°ê³¼
            
        Returns:
            bool: ê²½ì œ/ê¸ˆìœµ ê´€ë ¨ì´ë©´ True
        """
        try:
            from src.core.llm_manager import execute_llm_task, TaskType
            
            # ë„ë©”ì¸ ë¶„ì„ ê²°ê³¼ì—ì„œ ê²½ì œ ê´€ë ¨ í‚¤ì›Œë“œ í™•ì¸ (1ì°¨ í•„í„°ë§)
            domain = domain_analysis.get('domain', '').lower()
            subdomains = [s.lower() for s in domain_analysis.get('subdomains', [])]
            characteristics = [c.lower() for c in domain_analysis.get('characteristics', [])]
            key_terminology = [t.lower() for t in domain_analysis.get('key_terminology', [])]
            
            # ê²½ì œ ê´€ë ¨ í‚¤ì›Œë“œ
            economic_keywords = [
                'finance', 'financial', 'economy', 'economic', 'stock', 'stocks', 'market', 'markets',
                'investment', 'investing', 'trading', 'trade', 'portfolio', 'asset', 'assets',
                'revenue', 'profit', 'loss', 'earnings', 'dividend', 'bond', 'bonds',
                'currency', 'exchange', 'banking', 'bank', 'credit', 'debt', 'loan',
                'ì£¼ì‹', 'ì£¼ê°€', 'íˆ¬ì', 'ê²½ì œ', 'ê¸ˆìœµ', 'ì‹œì¥', 'ì¦ê¶Œ', 'ìì‚°', 'ìˆ˜ìµ', 'ì†ìµ',
                'í™˜ìœ¨', 'ì€í–‰', 'ëŒ€ì¶œ', 'ì±„ê¶Œ', 'ë°°ë‹¹', 'ê±°ë˜', 'í¬íŠ¸í´ë¦¬ì˜¤'
            ]
            
            # 1ì°¨ í•„í„°ë§: í‚¤ì›Œë“œ ê¸°ë°˜ ë¹ ë¥¸ ì²´í¬
            query_lower = query.lower()
            domain_text = ' '.join([domain] + subdomains + characteristics + key_terminology).lower()
            
            has_economic_keyword = any(
                keyword in query_lower or keyword in domain_text
                for keyword in economic_keywords
            )
            
            # í‚¤ì›Œë“œê°€ ì—†ìœ¼ë©´ ê²½ì œ ê´€ë ¨ì´ ì•„ë‹˜
            if not has_economic_keyword:
                logger.info(f"[{self.name}] No economic keywords found - not an economic request")
                return False
            
            # 2ì°¨ í•„í„°ë§: LLM ê¸°ë°˜ ì •í™•í•œ íŒë‹¨
            prompt = f"""
ì‚¬ìš©ì ìš”ì²­: {query}

ë„ë©”ì¸ ë¶„ì„ ê²°ê³¼:
- Domain: {domain_analysis.get('domain', 'unknown')}
- Subdomains: {', '.join(domain_analysis.get('subdomains', []))}
- Characteristics: {', '.join(domain_analysis.get('characteristics', []))}
- Key Terminology: {', '.join(domain_analysis.get('key_terminology', []))}

ìœ„ ìš”ì²­ì´ ê²½ì œ/ê¸ˆìœµ/íˆ¬ì ê´€ë ¨ ìš”ì²­ì¸ì§€ íŒë‹¨í•˜ì„¸ìš”.

ê²½ì œ/ê¸ˆìœµ/íˆ¬ì ê´€ë ¨ ìš”ì²­ì˜ ì˜ˆ:
- ì£¼ì‹ ì‹œì¥ ë¶„ì„, íˆ¬ì ì „ëµ, ê²½ì œ ì§€í‘œ ë¶„ì„
- ê¸°ì—… ì¬ë¬´ ë¶„ì„, ì£¼ê°€ ì˜ˆì¸¡, í¬íŠ¸í´ë¦¬ì˜¤ ê´€ë¦¬
- ê²½ì œ ì „ë§, ê¸ˆìœµ ì •ì±…, í™˜ìœ¨ ë¶„ì„
- ë¶€ë™ì‚° íˆ¬ì, ì±„ê¶Œ ë¶„ì„, íŒŒìƒìƒí’ˆ

ë¹„ê²½ì œ ìš”ì²­ì˜ ì˜ˆ:
- ê¸°ìˆ  ë™í–¥, ê³¼í•™ ì—°êµ¬, ì˜í•™ ì—°êµ¬
- ì—­ì‚¬, ë¬¸í™”, ì˜ˆìˆ 
- êµìœ¡, ë²•ë¥ , ì •ì¹˜ (ê²½ì œì™€ ë¬´ê´€í•œ ê²½ìš°)

ì¶œë ¥ í˜•ì‹ (JSON only, ì¶”ê°€ í…ìŠ¤íŠ¸ ê¸ˆì§€):
{{
    "is_economic": true/false,
    "confidence": 0.0-1.0,
    "reason": "íŒë‹¨ ê·¼ê±°ë¥¼ í•œêµ­ì–´ë¡œ 1ë¬¸ì¥"
}}
"""
            
            result = await execute_llm_task(
                prompt=prompt,
                task_type=TaskType.ANALYSIS,
                model_name=None,
                system_message="You are an expert at classifying research requests. Determine if a request is related to economics, finance, or investment."
            )
            
            result_text = result.content or "{}"
            
            # JSON íŒŒì‹± (jsonì€ ì´ë¯¸ íŒŒì¼ ìƒë‹¨ì—ì„œ importë¨)
            json_match = re.search(r'\{[\s\S]*\}', result_text)
            if json_match:
                try:
                    analysis_result = json.loads(json_match.group())
                    is_economic = analysis_result.get('is_economic', False)
                    confidence = analysis_result.get('confidence', 0.5)
                    reason = analysis_result.get('reason', '')
                    
                    # confidenceê°€ 0.7 ì´ìƒì´ë©´ ê²½ì œ ê´€ë ¨ìœ¼ë¡œ íŒë‹¨
                    if is_economic and confidence >= 0.7:
                        logger.info(f"[{self.name}] Economic request detected (confidence: {confidence:.2f}): {reason}")
                        return True
                    else:
                        logger.info(f"[{self.name}] Not an economic request (confidence: {confidence:.2f}): {reason}")
                        return False
                except json.JSONDecodeError:
                    logger.warning(f"[{self.name}] Failed to parse economic detection JSON, using keyword-based result")
                    return has_economic_keyword
            else:
                logger.warning(f"[{self.name}] No JSON found in economic detection result, using keyword-based result")
                return has_economic_keyword
                
        except Exception as e:
            logger.error(f"[{self.name}] Economic request detection failed: {e}")
            # ì—ëŸ¬ ë°œìƒ ì‹œ í‚¤ì›Œë“œ ê¸°ë°˜ ê²°ê³¼ ì‚¬ìš©
            return has_economic_keyword if 'has_economic_keyword' in locals() else False
    
    async def _call_financial_agent(self, user_query: str) -> Dict[str, Any]:
        """
        Financial Agent MCP ë„êµ¬ë¥¼ í˜¸ì¶œí•˜ì—¬ ê²½ì œ ì§€í‘œ ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
        
        Args:
            user_query: ì‚¬ìš©ì ìš”ì²­
            
        Returns:
            Dict: Financial Agent ë¶„ì„ ê²°ê³¼
        """
        try:
            from src.core.mcp_integration import execute_tool
            
            logger.info(f"[{self.name}] Calling financial_agent::run_financial_analysis...")
            
            # MCP ë„êµ¬ í˜¸ì¶œ
            result = await execute_tool(
                "financial_agent::run_financial_analysis",
                {"user_query": user_query}
            )
            
            if result.get("success", False):
                # execute_toolì˜ data í•„ë“œì— financial_agentì˜ ì „ì²´ ê²°ê³¼ê°€ ë“¤ì–´ìˆìŒ
                financial_result = result.get("data", {})
                # financial_agentì˜ ê²°ê³¼ë„ success í‚¤ë¥¼ í¬í•¨í•˜ë¯€ë¡œ í™•ì¸
                if isinstance(financial_result, dict) and financial_result.get("success", False):
                    logger.info(f"[{self.name}] Financial agent returned successful result")
                    return financial_result
                else:
                    logger.warning(f"[{self.name}] Financial agent result format unexpected: {type(financial_result)}")
                    return financial_result if isinstance(financial_result, dict) else None
            else:
                error_msg = result.get("error", "Unknown error")
                logger.warning(f"[{self.name}] Financial agent returned error: {error_msg}")
                return None
                
        except Exception as e:
            logger.error(f"[{self.name}] Failed to call financial agent: {e}")
            return None
    
    async def execute(self, state: AgentState) -> AgentState:
        """Execute planning task with Skills-based instruction and detailed logging."""
        logger.info(f"=" * 80)
        logger.info(f"[{self.name.upper()}] Starting research planning")
        logger.info(f"Query: {state['user_query']}")
        logger.info(f"Session: {state['session_id']}")
        logger.info(f"=" * 80)
        
        # ì‚¬ìš©ì ì‘ë‹µ ëŒ€ê¸° ì¤‘ì´ë©´ ì‘ë‹µ ì²˜ë¦¬
        if state.get("waiting_for_user", False):
            user_responses = state.get("user_responses", {})
            if user_responses:
                # ì‘ë‹µì´ ìˆìœ¼ë©´ ëª…í™•í™” ì •ë³´ ì ìš©
                from src.core.human_clarification_handler import get_clarification_handler
                clarification_handler = get_clarification_handler()
                
                for question_id, response_data in user_responses.items():
                    clarification = response_data.get("clarification", {})
                    # ëª…í™•í™” ì •ë³´ë¥¼ stateì— ì €ì¥
                    state["clarification_context"] = state.get("clarification_context", {})
                    state["clarification_context"][question_id] = clarification
                    
                    # ì‘ë‹µì— ë”°ë¼ ì‘ì—… ë°©í–¥ ì¡°ì •
                    response = response_data.get("response", "")
                    if response == "top_5":
                        # ìƒìœ„ 5ê°œ ê²°ê³¼ë§Œ ì‚¬ìš©í•˜ë„ë¡ ì„¤ì •
                        state["max_results"] = 5
                    elif response == "top_10":
                        # ìƒìœ„ 10ê°œ ê²°ê³¼ë§Œ ì‚¬ìš©í•˜ë„ë¡ ì„¤ì •
                        state["max_results"] = 10
                    elif response == "expand":
                        # ê²€ìƒ‰ ë²”ìœ„ í™•ëŒ€
                        state["expand_search"] = True
                    elif response == "modify":
                        # ê²€ìƒ‰ì–´ ìˆ˜ì • í•„ìš”
                        state["modify_query"] = True
                
                # ëŒ€ê¸° ìƒíƒœ í•´ì œ
                state["waiting_for_user"] = False
                state["pending_questions"] = []
                logger.info("âœ… User responses processed, continuing execution")
        
        # Read from shared memory - ONLY search within current session to prevent cross-task contamination
        memory = self.context.shared_memory
        current_session_id = state['session_id']
        
        # Search only within current session to prevent mixing previous task memories
        previous_plans = memory.search(
            state['user_query'], 
            limit=3,
            scope=MemoryScope.SESSION,
            session_id=current_session_id  # Critical: filter by current session only
        )
        
        logger.info(f"[{self.name}] Previous plans found in current session ({current_session_id}): {len(previous_plans) if previous_plans else 0}")
        
        # If no plans found in current session, explicitly set to empty to avoid confusion
        if not previous_plans:
            previous_plans = []
            logger.info(f"[{self.name}] No previous plans in current session - starting fresh task")
        
        # Skills-based instruction ì‚¬ìš©
        instruction = self.instruction if self.skill else "You are a research planning agent."
        
        logger.info(f"[{self.name}] Using skill: {self.skill is not None}")
        
        # LLM í˜¸ì¶œì€ llm_managerë¥¼ í†µí•´ Gemini ì§ê²° ì‚¬ìš©
        from src.core.llm_manager import execute_llm_task, TaskType
        
        # Use YAML-based prompt
        from src.core.skills.agent_loader import get_prompt
        
        # Phase 1: Domain Analysis and Exploration
        logger.info(f"[{self.name}] ğŸ” Starting domain analysis and exploration...")
        domain_analysis_result = await self.domain_exploration(state['user_query'])
        state['domain_analysis'] = domain_analysis_result
        logger.info(f"[{self.name}] âœ… Domain analysis completed: {domain_analysis_result.get('domain', 'unknown')}")
        
        # Phase 1.5: ê²½ì œ ê´€ë ¨ ìš”ì²­ ê°ì§€ (LLM ê¸°ë°˜)
        logger.info(f"[{self.name}] ğŸ” Checking if request is related to economics/finance...")
        is_economic_request = await self._detect_economic_request(state['user_query'], domain_analysis_result)
        state['is_economic_request'] = is_economic_request
        
        # Phase 1.6: ê²½ì œ ê´€ë ¨ ìš”ì²­ì´ë©´ financial_agent í˜¸ì¶œ
        financial_analysis_result = None
        if is_economic_request:
            logger.info(f"[{self.name}] âœ… Economic/finance related request detected")
            logger.info(f"[{self.name}] ğŸ“Š Calling financial_agent for economic indicator analysis...")
            try:
                financial_analysis_result = await self._call_financial_agent(state['user_query'])
                if financial_analysis_result and financial_analysis_result.get('success'):
                    logger.info(f"[{self.name}] âœ… Financial agent analysis completed successfully")
                    state['financial_analysis_result'] = financial_analysis_result
                else:
                    logger.warning(f"[{self.name}] âš ï¸ Financial agent analysis failed or returned no results")
                    financial_analysis_result = None
            except Exception as e:
                logger.warning(f"[{self.name}] âš ï¸ Financial agent call failed: {e}. Continuing with normal planning.")
                financial_analysis_result = None
        else:
            logger.info(f"[{self.name}] â„¹ï¸ Not an economic/finance related request")
        
        # Format previous_plans for prompt - only include if from current session
        # CRITICAL: Previous context is for REFERENCE ONLY - current task must be planned independently
        if previous_plans:
            # Filter to ensure only current session plans are included
            current_session_plans = [
                p for p in previous_plans 
                if p.get("session_id") == current_session_id
            ]
            if current_session_plans:
                # Format previous plans with STRONG warning that they are REFERENCE ONLY
                previous_plans_text = f"""
âš ï¸ REFERENCE ONLY - DO NOT REUSE âš ï¸
Previous research context (for domain understanding ONLY - NOT for task execution):
{chr(10).join([f"- {p.get('key', 'plan')}: {str(p.get('value', ''))[:200]}" for p in current_session_plans])}

CRITICAL: The above is for CONTEXT REFERENCE ONLY. You MUST create a NEW plan specifically for the CURRENT task: "{state['user_query']}".
DO NOT reuse previous task queries, search terms, or plan structures.
"""
            else:
                previous_plans_text = "No previous research found in current session. This is a COMPLETELY NEW task - create a fresh plan for the current query only."
        else:
            previous_plans_text = "No previous research found in current session. This is a COMPLETELY NEW task - create a fresh plan for the current query only."
        
        # ë„ë©”ì¸ ë¶„ì„ ê²°ê³¼ë¥¼ í”„ë¡¬í”„íŠ¸ì— í¬í•¨
        domain_context = ""
        if domain_analysis_result:
            domain_context = f"""
Domain Analysis Results:
- Domain: {domain_analysis_result.get('domain', 'general')}
- Subdomains: {', '.join(domain_analysis_result.get('subdomains', []))}
- Characteristics: {', '.join(domain_analysis_result.get('characteristics', []))}
- Key Terminology: {', '.join(domain_analysis_result.get('key_terminology', []))}
- Reliable Source Types: {', '.join(domain_analysis_result.get('reliable_source_types', []))}
- Verification Criteria: {', '.join(domain_analysis_result.get('verification_criteria', []))}
"""
        
        # Financial Agent ë¶„ì„ ê²°ê³¼ë¥¼ í”„ë¡¬í”„íŠ¸ì— í¬í•¨
        financial_context = ""
        if financial_analysis_result:
            try:
                # Financial analysis ê²°ê³¼ë¥¼ JSON ë¬¸ìì—´ë¡œ ë³€í™˜ (ìš”ì•½ í¬í•¨)
                financial_summary = {
                    "extracted_info": financial_analysis_result.get("extracted_info", {}),
                    "market_outlook": financial_analysis_result.get("market_outlook"),
                    "investment_plan": financial_analysis_result.get("investment_plan"),
                    "technical_analysis_summary": {
                        ticker: {
                            "price": data.get("price"),
                            "rsi": data.get("rsi"),
                            "macd": data.get("macd")
                        }
                        for ticker, data in financial_analysis_result.get("technical_analysis", {}).items()
                    } if financial_analysis_result.get("technical_analysis") else {},
                    "daily_pnl": financial_analysis_result.get("daily_pnl"),
                    "sentiment_analysis": financial_analysis_result.get("sentiment_analysis")
                }
                
                financial_context = f"""
Financial Agent Analysis Results (ê²½ì œ ì§€í‘œ ë¶„ì„):
{json.dumps(financial_summary, ensure_ascii=False, indent=2)}

ì´ ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë” êµ¬ì²´ì ì´ê³  ì •í™•í•œ ì—°êµ¬ ê³„íšì„ ìˆ˜ë¦½í•˜ì„¸ìš”.
ê²½ì œ ì§€í‘œ, ì‹œì¥ ì „ë§, íˆ¬ì ê³„íš ë“±ì„ ê³ ë ¤í•˜ì—¬ ì—°êµ¬ ë°©í–¥ì„ ì„¤ì •í•˜ì„¸ìš”.
"""
            except Exception as e:
                logger.warning(f"[{self.name}] Failed to format financial analysis context: {e}")
                financial_context = ""
        
        # promptëŠ” execute_llm_taskì˜ decoratorì—ì„œ ìë™ìœ¼ë¡œ ìµœì í™”ë¨
        logger.info(f"[{self.name}] Calling LLM for planning...")
        
        # Current time calculation for prompt
        current_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S (%A)')
        
        prompt = get_prompt("planner", "planning",
                           instruction=self.instruction,
                           user_query=state['user_query'],
                           previous_plans=previous_plans_text,
                           current_time=current_time)
        
        # ë„ë©”ì¸ ë¶„ì„ ê²°ê³¼ì™€ Financial Agent ê²°ê³¼ë¥¼ í”„ë¡¬í”„íŠ¸ì— ì¶”ê°€
        context_parts = []
        if domain_context:
            context_parts.append(domain_context)
        if financial_context:
            context_parts.append(financial_context)
        
        if context_parts:
            prompt = "\n\n".join(context_parts) + "\n\n" + prompt

        # promptëŠ” execute_llm_taskì˜ decoratorì—ì„œ ìë™ìœ¼ë¡œ ìµœì í™”ë¨
        logger.info(f"[{self.name}] Calling LLM for planning...")
        # Gemini ì‹¤í–‰
        model_result = await execute_llm_task(
            prompt=prompt,
            task_type=TaskType.PLANNING,
            model_name=None,
            system_message=None
        )
        plan = model_result.content or 'No plan generated'
        
        logger.info(f"[{self.name}] âœ… Plan generated: {len(plan)} characters")
        logger.info(f"[{self.name}] Plan preview: {plan[:200]}...")
        
        # Council í™œì„±í™” í™•ì¸ ë° ì ìš©
        use_council = state.get('use_council', None)  # ìˆ˜ë™ í™œì„±í™” ì˜µì…˜
        if use_council is None:
            # ìë™ í™œì„±í™” íŒë‹¨
            from src.core.council_activator import get_council_activator
            activator = get_council_activator()
            activation_decision = activator.should_activate(
                process_type='planning',
                query=state['user_query'],
                context={'domains': [], 'steps': []}  # ì»¨í…ìŠ¤íŠ¸ëŠ” í–¥í›„ í™•ì¥ ê°€ëŠ¥
            )
            use_council = activation_decision.should_activate
            if use_council:
                logger.info(f"[{self.name}] ğŸ›ï¸ Council auto-activated: {activation_decision.reason}")
        
        # Council ì ìš© (í™œì„±í™”ëœ ê²½ìš°)
        if use_council:
            try:
                from src.core.llm_council import run_full_council
                logger.info(f"[{self.name}] ğŸ›ï¸ Running Council review for research plan...")
                
                # Councilì— ê³„íš ê²€í†  ìš”ì²­
                council_query = f"""Review and improve the following research plan. Provide feedback on completeness, feasibility, and quality.

Research Query: {state['user_query']}

Research Plan:
{plan}

Provide an improved version of the plan that addresses any gaps or issues you identify."""
                
                stage1_results, stage2_results, stage3_result, metadata = await run_full_council(
                    council_query
                )
                
                # Council ê²°ê³¼ë¥¼ ê³„íšì— ë°˜ì˜
                council_improved_plan = stage3_result.get('response', plan)
                plan = council_improved_plan
                
                logger.info(f"[{self.name}] âœ… Council review completed. Plan improved with consensus.")
                logger.info(f"[{self.name}] Council aggregate rankings: {metadata.get('aggregate_rankings', [])}")
                
                # Council ë©”íƒ€ë°ì´í„°ë¥¼ stateì— ì €ì¥
                state['council_metadata'] = {
                    'planning': {
                        'stage1_results': stage1_results,
                        'stage2_results': stage2_results,
                        'stage3_result': stage3_result,
                        'metadata': metadata
                    }
                }
            except Exception as e:
                logger.warning(f"[{self.name}] Council review failed: {e}. Using original plan.")
                # Council ì‹¤íŒ¨ ì‹œ ì›ë³¸ ê³„íš ì‚¬ìš© (fallback ì œê±° - ëª…í™•í•œ ë¡œê¹…ë§Œ)
        
        state['research_plan'] = plan
        
        # ì‘ì—… ë¶„í• : ì—°êµ¬ ê³„íšì„ ì—¬ëŸ¬ ë…ë¦½ì ì¸ ì‘ì—…ìœ¼ë¡œ ë¶„í• 
        logger.info(f"[{self.name}] Splitting research plan into parallel tasks...")
        
        # Use YAML-based prompt template for task decomposition
        from src.core.skills.agent_loader import get_prompt
        
        # ë„ë©”ì¸ ë¶„ì„ ê²°ê³¼ë¥¼ JSON ë¬¸ìì—´ë¡œ ë³€í™˜
        domain_analysis_text = json.dumps(domain_analysis_result, ensure_ascii=False, indent=2) if domain_analysis_result else "{}"
        
        task_split_prompt = get_prompt(
            "planner",
            "task_decomposition",
            plan=plan,
            query=state['user_query'],
            domain_analysis=domain_analysis_text,
            current_time=current_time
        )

        try:
            task_split_result = await execute_llm_task(
                prompt=task_split_prompt,
                task_type=TaskType.PLANNING,
                model_name=None,
                system_message="You are a task decomposition agent. Split research plans into independent parallel tasks."
            )
            
            task_split_text = task_split_result.content or ""
            
            # JSON íŒŒì‹± ì‹œë„
            
            # JSON ë¸”ë¡ ì¶”ì¶œ
            json_match = re.search(r'\{[\s\S]*\}', task_split_text)
            if json_match:
                task_split_json = json.loads(json_match.group())
                tasks = task_split_json.get('tasks', [])
            else:
                # JSONì´ ì—†ìœ¼ë©´ í…ìŠ¤íŠ¸ì—ì„œ ì‘ì—… ì¶”ì¶œ ì‹œë„
                tasks = []
                lines = task_split_text.split('\n')
                current_task = None
                for line in lines:
                    line = line.strip()
                    if 'task_id' in line.lower() or 'task' in line.lower() and ':' in line:
                        if current_task:
                            tasks.append(current_task)
                        task_id_match = re.search(r'task[_\s]*(\d+)', line, re.IGNORECASE)
                        task_id = f"task_{task_id_match.group(1) if task_id_match else len(tasks) + 1}"
                        current_task = {
                            "task_id": task_id,
                            "description": "",
                            "search_queries": [],
                            "priority": len(tasks) + 1,
                            "estimated_time": "medium",
                            "dependencies": []
                        }
                    elif current_task:
                        if 'description' in line.lower() or 'ì„¤ëª…' in line:
                            desc_match = re.search(r':\s*(.+)', line)
                            if desc_match:
                                current_task["description"] = desc_match.group(1).strip()
                        elif 'query' in line.lower() or 'ì¿¼ë¦¬' in line:
                            query_match = re.search(r':\s*(.+)', line)
                            if query_match:
                                current_task["search_queries"].append(query_match.group(1).strip())
                
                if current_task:
                    tasks.append(current_task)
            
            # ì‘ì—…ì´ ì—†ìœ¼ë©´ ê¸°ë³¸ ì‘ì—… ìƒì„±
            if not tasks:
                logger.warning(f"[{self.name}] Failed to parse tasks, creating default task")
                tasks = [{
                    "task_id": "task_1",
                    "description": state['user_query'],
                    "search_queries": [state['user_query']],
                    "priority": 1,
                    "estimated_time": "medium",
                    "dependencies": []
                }]
            
            # ê° ì‘ì—…ì— ë©”íƒ€ë°ì´í„° ì¶”ê°€ ë° ê²€ìƒ‰ ì¿¼ë¦¬ ê²€ì¦
            user_query_lower = state['user_query'].lower()
            # ì˜ëª»ëœ ê²€ìƒ‰ ì¿¼ë¦¬ í‚¤ì›Œë“œ (ë©”íƒ€ ì •ë³´ ê´€ë ¨)
            invalid_keywords = [
                'ì‘ì—… ë¶„í• ', 'íƒœìŠ¤í¬ ë¶„í• ', 'ë³‘ë ¬í™”', 'ë³‘ë ¬ ì‹¤í–‰', 'task decomposition',
                'task split', 'parallel', 'parallelization', 'ì—°êµ¬ ë°©ë²•ë¡ ', 'ì—°êµ¬ ì „ëµ',
                'ì—°êµ¬ ê³„íš', 'research methodology', 'research strategy', 'research plan',
                'í•˜ìœ„ ì—°êµ¬ ì£¼ì œ ë¶„í•´', 'ë…ë¦½ì  ì—°êµ¬ íƒœìŠ¤í¬', 'ì—°êµ¬ ì‘ì—… ë³‘ë ¬í™”'
            ]
            
            for i, task in enumerate(tasks):
                if 'task_id' not in task:
                    task['task_id'] = f"task_{i + 1}"
                if 'name' not in task:
                    task['name'] = task.get('description', state['user_query'])[:100]
                if 'description' not in task:
                    task['description'] = state['user_query']
                
                # Task êµ¬ì¡° í™•ì¥ í•„ë“œ ê¸°ë³¸ê°’ ì„¤ì •
                if 'objectives' not in task:
                    task['objectives'] = [task.get('description', state['user_query'])]
                
                if 'required_information' not in task:
                    task['required_information'] = {
                        'data_types': ['quantitative', 'qualitative'],
                        'key_entities': [],
                        'sources': {
                            'min_count': 3,
                            'reliability_threshold': 0.7,
                            'preferred_types': ['academic', 'news', 'government']
                        }
                    }
                
                if 'verification_strategy' not in task:
                    task['verification_strategy'] = {
                        'cross_verify': True,
                        'fact_check': True,
                        'source_validation': True,
                        'min_consensus_sources': 2
                    }
                
                if 'success_criteria' not in task:
                    task['success_criteria'] = [
                        f"Task {task.get('task_id')} completed with valid results",
                        "Sources meet reliability threshold"
                    ]
                
                # ê²€ìƒ‰ ì¿¼ë¦¬ ê²€ì¦ ë° í•„í„°ë§
                if 'search_queries' in task and task['search_queries']:
                    # ì˜ëª»ëœ ê²€ìƒ‰ ì¿¼ë¦¬ í•„í„°ë§
                    valid_queries = []
                    for query in task['search_queries']:
                        query_str = str(query).strip()
                        query_lower = query_str.lower()
                        
                        # {query} í”Œë ˆì´ìŠ¤í™€ë”ê°€ í¬í•¨ëœ ì¿¼ë¦¬ ì™„ì „ ì œì™¸
                        if "{query}" in query_str or "{query}" in query_lower:
                            logger.warning(f"[{self.name}] Task {task.get('task_id')}: Filtered out query with placeholder: '{query_str[:50]}...'")
                            continue
                        
                        # ë©”íƒ€ ì •ë³´ ê´€ë ¨ í‚¤ì›Œë“œê°€ í¬í•¨ëœ ì¿¼ë¦¬ ì œì™¸
                        is_invalid = any(keyword in query_lower for keyword in invalid_keywords)
                        # ì‚¬ìš©ì ì¿¼ë¦¬ì™€ ê´€ë ¨ì´ ì—†ëŠ” ì¿¼ë¦¬ ì œì™¸ (ë„ˆë¬´ ì§§ê±°ë‚˜ ì¼ë°˜ì ì¸ ê²½ìš°)
                        is_too_generic = len(query_str) < 10
                        
                        if not is_invalid and not is_too_generic:
                            valid_queries.append(query_str)
                        else:
                            logger.warning(f"[{self.name}] Task {task.get('task_id')}: Filtered out invalid query: '{query_str[:50]}...' (invalid={is_invalid}, generic={is_too_generic})")
                    
                    # ìœ íš¨í•œ ì¿¼ë¦¬ê°€ ì—†ìœ¼ë©´ ì‚¬ìš©ì ì¿¼ë¦¬ ì‚¬ìš©
                    if not valid_queries:
                        logger.warning(f"[{self.name}] Task {task.get('task_id')} has no valid search queries, using user query: '{state['user_query']}'")
                        valid_queries = [state['user_query']]
                    
                    task['search_queries'] = valid_queries
                    logger.info(f"[{self.name}] Task {task.get('task_id')}: Final search queries: {valid_queries}")
                else:
                    # search_queriesê°€ ì—†ìœ¼ë©´ ì‚¬ìš©ì ì¿¼ë¦¬ ì‚¬ìš©
                    task['search_queries'] = [state['user_query']]
                
                if 'priority' not in task:
                    task['priority'] = i + 1
                if 'estimated_time' not in task:
                    task['estimated_time'] = "medium"
                if 'dependencies' not in task:
                    task['dependencies'] = []
            
            state['research_tasks'] = tasks
            logger.info(f"[{self.name}] âœ… Split research plan into {len(tasks)} parallel tasks")
            for task in tasks:
                queries = task.get('search_queries', [])
                queries_preview = [q[:40] + '...' if len(q) > 40 else q for q in queries[:3]]
                logger.info(f"[{self.name}]   - {task.get('task_id')}: {task.get('description', '')[:50]}... ({len(queries)} queries: {queries_preview})")
                
        except Exception as e:
            logger.error(f"[{self.name}] âŒ Failed to split tasks: {e}")
            # ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ì‘ì—… ìƒì„±
            state['research_tasks'] = [{
                "task_id": "task_1",
                "description": state['user_query'],
                "search_queries": [state['user_query']],
                "priority": 1,
                "estimated_time": "medium",
                "dependencies": []
            }]
            logger.warning(f"[{self.name}] Using default single task")
        
        state['current_agent'] = self.name
        
        # Write to shared memory
        memory.write(
            key=f"plan_{state['session_id']}",
            value=plan,
            scope=MemoryScope.SESSION,
            session_id=state['session_id'],
            agent_id=self.name
        )
        
        memory.write(
            key=f"tasks_{state['session_id']}",
            value=state['research_tasks'],
            scope=MemoryScope.SESSION,
            session_id=state['session_id'],
            agent_id=self.name
        )
        
        logger.info(f"[{self.name}] Plan and tasks saved to shared memory")
        logger.info(f"=" * 80)
        
        return state


class ExecutorAgent:
    """Executor agent - executes research tasks using tools (Skills-based)."""
    
    def __init__(self, context: AgentContext, skill: Optional[Skill] = None):
        self.context = context
        self.name = "executor"
        self.available_tools: list = []  # MCP ìë™ í• ë‹¹ ë„êµ¬
        self.tool_infos: list = []  # ë„êµ¬ ë©”íƒ€ë°ì´í„°
        self.skill = skill
        
        # Skillì´ ì—†ìœ¼ë©´ ë¡œë“œ ì‹œë„
        if self.skill is None:
            skill_manager = get_skill_manager()
            self.skill = skill_manager.load_skill("research_executor")
        
        # Skill instruction ì‚¬ìš©
        if self.skill:
            self.instruction = self.skill.instructions
        else:
            self.instruction = "You are a research execution agent."
    
    async def _filter_results_by_relevance(
        self,
        search_results: List[Dict[str, Any]],
        user_query: str,
        search_queries: List[str],
        current_time: str = ""
    ) -> List[Dict[str, Any]]:
        """
        ê²€ìƒ‰ ê²°ê³¼ë¥¼ ê´€ë ¨ì„± ê¸°ì¤€ìœ¼ë¡œ ì‚¬ì „ í•„í„°ë§í•©ë‹ˆë‹¤.
        
        Args:
            search_results: í•„í„°ë§í•  ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
            user_query: ì›ë˜ ì‚¬ìš©ì ì¿¼ë¦¬
            search_queries: ê²€ìƒ‰ì— ì‚¬ìš©ëœ ì¿¼ë¦¬ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            ê´€ë ¨ì„± ì ìˆ˜ 3ì  ì´ìƒì¸ ê²°ê³¼ë§Œ í¬í•¨ëœ ë¦¬ìŠ¤íŠ¸
        """
        from src.core.llm_manager import execute_llm_task, TaskType
        
        MIN_REQUIRED_RESULTS = 30
        RELEVANCE_THRESHOLD = 3  # 1-10 ì ìˆ˜ ê¸°ì¤€
        
        if len(search_results) <= MIN_REQUIRED_RESULTS:
            # ê²°ê³¼ê°€ ì´ë¯¸ ì¶©ë¶„í•˜ë©´ í•„í„°ë§ ìŠ¤í‚µ (ë„ˆë¬´ ê³µê²©ì ìœ¼ë¡œ í•„í„°ë§í•˜ì§€ ì•ŠìŒ)
            logger.info(f"[{self.name}] Results count ({len(search_results)}) is acceptable, skipping aggressive filtering")
            return search_results
        
        logger.info(f"[{self.name}] ğŸ” Filtering {len(search_results)} results by relevance (threshold: {RELEVANCE_THRESHOLD}/10)")
        
        # ë°°ì¹˜ë¡œ ê´€ë ¨ì„± í‰ê°€ (ì„±ëŠ¥ ìµœì í™”)
        batch_size = 10
        filtered_results = []
        
        for i in range(0, len(search_results), batch_size):
            batch = search_results[i:i+batch_size]
            
            # ë°°ì¹˜ í‰ê°€ í”„ë¡¬í”„íŠ¸
            batch_evaluation_prompt = f"""ë‹¤ìŒ ê²€ìƒ‰ ê²°ê³¼ë“¤ì„ ì›ë˜ ì¿¼ë¦¬ì™€ì˜ ê´€ë ¨ì„±ì— ë”°ë¼ í‰ê°€í•˜ì„¸ìš”.

í˜„ì¬ ì‹œê°: {current_time}
ì›ë˜ ì¿¼ë¦¬: {user_query}
ê²€ìƒ‰ ì¿¼ë¦¬: {', '.join(search_queries[:3])}

ê²€ìƒ‰ ê²°ê³¼:
{chr(10).join([f"{j+1}. ì œëª©: {r.get('title', 'N/A')[:100]}{chr(10)}   ë‚´ìš©: {r.get('snippet', r.get('content', ''))[:200]}{chr(10)}   URL: {r.get('url', 'N/A')}" for j, r in enumerate(batch)])}

ê° ê²°ê³¼ì— ëŒ€í•´ ë‹¤ìŒì„ í‰ê°€í•˜ì„¸ìš”:
1. ì§ì ‘ì  ê´€ë ¨ì„± (1-10): ì¿¼ë¦¬ì™€ ì§ì ‘ì ìœ¼ë¡œ ê´€ë ¨ì´ ìˆëŠ”ê°€?
2. ê°„ì ‘ì  ê´€ë ¨ì„± (1-10): ë°°ê²½ ì •ë³´ë‚˜ ë§¥ë½ ì œê³µì— ë„ì›€ì´ ë˜ëŠ”ê°€?
3. ì™„ì „íˆ ë¬´ê´€í•œì§€ ì—¬ë¶€ (YES/NO)

ì‘ë‹µ í˜•ì‹ (JSON):
{{
  "evaluations": [
    {{
      "index": 1,
      "direct_relevance": 8,
      "indirect_relevance": 5,
      "is_irrelevant": false,
      "overall_score": 7,
      "reason": "ì—”ë¹„ë””ì•„ GPU ì‹œì¥ ì ìœ ìœ¨ì— ëŒ€í•œ ì§ì ‘ì  ì •ë³´"
    }},
    ...
  ]
}}

âš ï¸ ì¤‘ìš”:
- ì™„ì „íˆ ë¬´ê´€í•œ ê²°ê³¼ë§Œ ì œì™¸ (ì˜ˆ: ì—”ë¹„ë””ì•„ ì¿¼ë¦¬ì¸ë° ë¶€ë™ì‚° ê´€ë ¨ ê²°ê³¼)
- ê´€ë ¨ì„±ì´ ì•½ê°„ ë‚®ì•„ë„ ë°°ê²½ ì •ë³´ë¡œ ìœ ìš©í•˜ë©´ í¬í•¨
- overall_scoreëŠ” (direct_relevance * 0.7 + indirect_relevance * 0.3)ë¡œ ê³„ì‚°"""
            
            try:
                evaluation_result = await execute_llm_task(
                    prompt=batch_evaluation_prompt,
                    task_type=TaskType.ANALYSIS,
                    model_name=None,
                    system_message="You are an expert information relevance evaluator. Evaluate search results for relevance to the query."
                )
                
                # JSON íŒŒì‹±                
                evaluation_text = evaluation_result.content or "{}"
                json_match = re.search(r'\{[\s\S]*\}', evaluation_text)
                if json_match:
                    try:
                        evaluation_data = json.loads(json_match.group())
                        evaluations = evaluation_data.get('evaluations', [])
                        
                        for eval_item in evaluations:
                            idx = eval_item.get('index', 0) - 1  # 1-based to 0-based
                            if 0 <= idx < len(batch):
                                overall_score = eval_item.get('overall_score', 0)
                                is_irrelevant = eval_item.get('is_irrelevant', False)
                                
                                # ê´€ë ¨ì„± ì ìˆ˜ê°€ threshold ì´ìƒì´ê³  ë¬´ê´€í•˜ì§€ ì•Šìœ¼ë©´ í¬í•¨
                                if overall_score >= RELEVANCE_THRESHOLD and not is_irrelevant:
                                    result = batch[idx].copy()
                                    result['relevance_score'] = overall_score
                                    result['relevance_reason'] = eval_item.get('reason', '')
                                    filtered_results.append(result)
                                else:
                                    logger.debug(f"[{self.name}] Filtered out result {i+idx+1}: score={overall_score}, irrelevant={is_irrelevant}")
                    except json.JSONDecodeError:
                        logger.warning(f"[{self.name}] Failed to parse relevance evaluation JSON, including all results in batch")
                        filtered_results.extend(batch)
                else:
                    logger.warning(f"[{self.name}] No JSON found in relevance evaluation, including all results in batch")
                    filtered_results.extend(batch)
                    
            except Exception as e:
                logger.warning(f"[{self.name}] Relevance evaluation failed for batch {i//batch_size + 1}: {e}. Including all results in batch.")
                filtered_results.extend(batch)
        
        # í•„í„°ë§ í›„ì—ë„ ìµœì†Œ 30ê°œ ì´ìƒ ë³´ì¥
        if len(filtered_results) < MIN_REQUIRED_RESULTS:
            logger.warning(f"[{self.name}] âš ï¸ Filtered results ({len(filtered_results)}) < minimum ({MIN_REQUIRED_RESULTS}), including lower relevance results")
            # ê´€ë ¨ì„± ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ìƒìœ„ ê²°ê³¼ í¬í•¨
            scored_results = []
            for result in search_results:
                score = result.get('relevance_score', 5)  # ê¸°ë³¸ê°’ 5
                scored_results.append((score, result))
            
            scored_results.sort(reverse=True, key=lambda x: x[0])
            filtered_results = [r for _, r in scored_results[:MIN_REQUIRED_RESULTS]]
            logger.info(f"[{self.name}] âœ… Included top {len(filtered_results)} results to meet minimum requirement")
        
        return filtered_results
    
    async def execute(self, state: AgentState, assigned_task: Optional[Dict[str, Any]] = None) -> AgentState:
        """Execute research tasks with detailed logging."""
        logger.info(f"=" * 80)
        logger.info(f"[{self.name.upper()}] Starting research execution")
        logger.info(f"Agent ID: {self.context.agent_id}")
        logger.info(f"Query: {state['user_query']}")
        logger.info(f"Session: {state['session_id']}")
        logger.info(f"=" * 80)
        
        # ì‚¬ìš©ì ì‘ë‹µ ëŒ€ê¸° ì¤‘ì´ë©´ ì‘ë‹µ ì²˜ë¦¬
        if state.get("waiting_for_user", False):
            user_responses = state.get("user_responses", {})
            if user_responses:
                # ì‘ë‹µì´ ìˆìœ¼ë©´ ëª…í™•í™” ì •ë³´ ì ìš©
                from src.core.human_clarification_handler import get_clarification_handler
                clarification_handler = get_clarification_handler()
                
                for question_id, response_data in user_responses.items():
                    clarification = response_data.get("clarification", {})
                    # ëª…í™•í™” ì •ë³´ë¥¼ stateì— ì €ì¥
                    state["clarification_context"] = state.get("clarification_context", {})
                    state["clarification_context"][question_id] = clarification
                    
                    # ì‘ë‹µì— ë”°ë¼ ì‘ì—… ë°©í–¥ ì¡°ì •
                    response = response_data.get("response", "")
                    if response == "top_5":
                        # ìƒìœ„ 5ê°œ ê²°ê³¼ë§Œ ì‚¬ìš©í•˜ë„ë¡ ì„¤ì •
                        state["max_results"] = 5
                    elif response == "top_10":
                        # ìƒìœ„ 10ê°œ ê²°ê³¼ë§Œ ì‚¬ìš©í•˜ë„ë¡ ì„¤ì •
                        state["max_results"] = 10
                    elif response == "expand":
                        # ê²€ìƒ‰ ë²”ìœ„ í™•ëŒ€
                        state["expand_search"] = True
                    elif response == "modify":
                        # ê²€ìƒ‰ì–´ ìˆ˜ì • í•„ìš”
                        state["modify_query"] = True
                
                # ëŒ€ê¸° ìƒíƒœ í•´ì œ
                state["waiting_for_user"] = False
                state["pending_questions"] = []
                logger.info("âœ… User responses processed, continuing execution")
        
        # ì‘ì—… í• ë‹¹: assigned_taskê°€ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ stateì—ì„œ ì°¾ê¸°
        if assigned_task is None:
            # state['research_tasks']ì—ì„œ ì´ ì—ì´ì „íŠ¸ì—ê²Œ í• ë‹¹ëœ ì‘ì—… ì°¾ê¸°
            tasks = state.get('research_tasks', [])
            if tasks:
                # agent_idë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‘ì—… í• ë‹¹ (ë¼ìš´ë“œë¡œë¹ˆ)
                agent_id = self.context.agent_id
                if agent_id.startswith("executor_"):
                    try:
                        agent_index = int(agent_id.split("_")[1])
                        if agent_index < len(tasks):
                            assigned_task = tasks[agent_index]
                            logger.info(f"[{self.name}] Assigned task {assigned_task.get('task_id', 'unknown')} to {agent_id}")
                        else:
                            # ì¸ë±ìŠ¤ê°€ ë²”ìœ„ë¥¼ ë²—ì–´ë‚˜ë©´ ì²« ë²ˆì§¸ ì‘ì—… í• ë‹¹
                            assigned_task = tasks[0]
                            logger.info(f"[{self.name}] Agent index out of range, using first task")
                    except (ValueError, IndexError):
                        assigned_task = tasks[0] if tasks else None
                        logger.info(f"[{self.name}] Using first task (fallback)")
                else:
                    # agent_idê°€ executor_ í˜•ì‹ì´ ì•„ë‹ˆë©´ ì²« ë²ˆì§¸ ì‘ì—… ì‚¬ìš©
                    assigned_task = tasks[0] if tasks else None
            else:
                # ì‘ì—…ì´ ì—†ìœ¼ë©´ ë©”ëª¨ë¦¬ì—ì„œ ì½ê¸°
                memory = self.context.shared_memory
                tasks = memory.read(
                    key=f"tasks_{state['session_id']}",
                    scope=MemoryScope.SESSION,
                    session_id=state['session_id']
                ) or []
                if tasks:
                    assigned_task = tasks[0] if tasks else None
        
        # Read plan from shared memory
        memory = self.context.shared_memory
        plan = memory.read(
            key=f"plan_{state['session_id']}",
            scope=MemoryScope.SESSION,
            session_id=state['session_id']
        )
        
        logger.info(f"[{self.name}] Research plan loaded: {plan is not None}")
        if plan:
            logger.info(f"[{self.name}] Plan preview: {plan[:200]}...")
        
        # ì‹¤ì œ ì—°êµ¬ ì‹¤í–‰ - MCP Hubë¥¼ í†µí•œ ë³‘ë ¬ ê²€ìƒ‰ ìˆ˜í–‰
        query = state['user_query']
        
        # Current time calculation for prompt and context
        current_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S (%A)')
        
        results = []
        
        try:
            # MCP Hub ì´ˆê¸°í™” í™•ì¸
            from src.core.mcp_integration import get_mcp_hub, execute_tool, ToolCategory
            
            hub = get_mcp_hub()
            logger.info(f"[{self.name}] MCP Hub status: {len(hub.mcp_sessions) if hub.mcp_sessions else 0} servers connected")
            
            if not hub.mcp_sessions:
                logger.info(f"[{self.name}] Initializing MCP Hub...")
                await hub.initialize_mcp()
                logger.info(f"[{self.name}] MCP Hub initialized: {len(hub.mcp_sessions)} servers")
            
            # ì‘ì—… í• ë‹¹ì´ ìˆìœ¼ë©´ í•´ë‹¹ ì‘ì—…ì˜ ê²€ìƒ‰ ì¿¼ë¦¬ ì‚¬ìš©
            # CRITICAL: Verify that assigned_task is for the CURRENT query
            search_queries = []
            if assigned_task:
                task_id = assigned_task.get('task_id', 'unknown')
                task_description = assigned_task.get('description', '')
                raw_queries = assigned_task.get('search_queries', [])
                
                logger.info(f"[{self.name}] âš ï¸ TASK VERIFICATION:")
                logger.info(f"[{self.name}]   - Current User Query: '{query}'")
                logger.info(f"[{self.name}]   - Assigned Task ID: {task_id}")
                logger.info(f"[{self.name}]   - Task Description: {task_description[:100]}...")
                logger.info(f"[{self.name}]   - Raw queries count: {len(raw_queries)}")
                
                # Verify that assigned_task queries are related to current query
                # Extract key terms from current query for validation
                current_query_terms = set(query.lower().split())
                query_relevance_check = False
                
                # ì˜ëª»ëœ ê²€ìƒ‰ ì¿¼ë¦¬ í•„í„°ë§ (ë©”íƒ€ ì •ë³´ ê´€ë ¨)
                invalid_keywords = [
                    'ì‘ì—… ë¶„í• ', 'íƒœìŠ¤í¬ ë¶„í• ', 'ë³‘ë ¬í™”', 'ë³‘ë ¬ ì‹¤í–‰', 'task decomposition',
                    'task split', 'parallel', 'parallelization', 'ì—°êµ¬ ë°©ë²•ë¡ ', 'ì—°êµ¬ ì „ëµ',
                    'ì—°êµ¬ ê³„íš', 'research methodology', 'research strategy', 'research plan',
                    'í•˜ìœ„ ì—°êµ¬ ì£¼ì œ ë¶„í•´', 'ë…ë¦½ì  ì—°êµ¬ íƒœìŠ¤í¬', 'ì—°êµ¬ ì‘ì—… ë³‘ë ¬í™”'
                ]
                
                for q in raw_queries:
                    q_str = str(q).strip()
                    q_lower = q_str.lower()
                    
                    # {query} í”Œë ˆì´ìŠ¤í™€ë”ê°€ í¬í•¨ëœ ì¿¼ë¦¬ ì™„ì „ ì œì™¸
                    if "{query}" in q_str or "{query}" in q_lower:
                        logger.warning(f"[{self.name}] âŒ Filtered out query with placeholder: '{q_str[:50]}...'")
                        continue
                    
                    # Verify query relevance to current user query
                    query_terms = set(q_lower.split())
                    # Check if query contains at least one key term from current query (for relevance)
                    if len(current_query_terms) > 0:
                        common_terms = query_terms.intersection(current_query_terms)
                        if len(common_terms) > 0:
                            query_relevance_check = True
                    
                    is_invalid = any(keyword in q_lower for keyword in invalid_keywords)
                    is_too_generic = len(q_str) < 10
                    
                    if not is_invalid and not is_too_generic:
                        search_queries.append(q_str)
                        logger.info(f"[{self.name}] âœ… Valid query added: '{q_str[:80]}...'")
                    else:
                        logger.warning(f"[{self.name}] âŒ Filtered out invalid query: '{q_str[:50]}...' (invalid={is_invalid}, generic={is_too_generic})")
                
                # Verify task relevance
                if not query_relevance_check and len(current_query_terms) > 0:
                    logger.warning(f"[{self.name}] âš ï¸ WARNING: Assigned task queries may not be related to current query: '{query}'")
                    logger.warning(f"[{self.name}] âš ï¸ This might be a previous task's queries. Verifying task assignment...")
                
                # ìœ íš¨í•œ ì¿¼ë¦¬ê°€ ì—†ìœ¼ë©´ ì‚¬ìš©ì ì¿¼ë¦¬ ì‚¬ìš©
                if not search_queries:
                    logger.warning(f"[{self.name}] âš ï¸ No valid queries in assigned task, using CURRENT user query: '{query}'")
                    search_queries = [query]
                else:
                    logger.info(f"[{self.name}] âœ… Using {len(search_queries)} valid queries from task {task_id} (for current query: '{query}')")
            
            # ì‘ì—… í• ë‹¹ì´ ì—†ê±°ë‚˜ ì¿¼ë¦¬ê°€ ì—†ìœ¼ë©´ ê¸°ì¡´ ë¡œì§ ì‚¬ìš©
            if not search_queries:
                search_queries = [query]  # ê¸°ë³¸ ì¿¼ë¦¬
                if plan:
                    # LLMìœ¼ë¡œ ì—°êµ¬ ê³„íšì—ì„œ ê²€ìƒ‰ ì¿¼ë¦¬ ì¶”ì¶œ
                    from src.core.llm_manager import execute_llm_task, TaskType

                    # Use YAML-based prompt for query generation
                    from src.core.skills.agent_loader import get_prompt
                    query_generation_prompt = get_prompt("planner", "query_generation",
                                                        plan=plan,
                                                        query=query,
                                                        current_time=current_time)

                    try:
                        system_message = self.config.prompts["query_generation"]["system_message"]
                        # query_generation_promptì™€ system_messageëŠ” execute_llm_taskì˜ decoratorì—ì„œ ìë™ìœ¼ë¡œ ìµœì í™”ë¨
                        
                        query_result = await execute_llm_task(
                            prompt=query_generation_prompt,
                            task_type=TaskType.PLANNING,
                            model_name=None,
                            system_message=system_message
                        )

                        generated_queries = query_result.content or ""
                        # ê° ì¤„ì„ ì¿¼ë¦¬ë¡œ íŒŒì‹±
                        for line in generated_queries.split('\n'):
                            line = line.strip()
                            if line and not line.startswith('#') and len(line) > 5:
                                search_queries.append(line)

                        # ì¤‘ë³µ ì œê±°
                        search_queries = list(dict.fromkeys(search_queries))[:5]  # ìµœëŒ€ 5ê°œ
                        logger.info(f"[{self.name}] Generated {len(search_queries)} search queries from plan")
                    except Exception as e:
                        logger.warning(f"[{self.name}] Failed to generate search queries from plan: {e}, using original query only")
            
            # ìµœì†Œ 3-5ê°œì˜ ë‹¤ì–‘í•œ ê²€ìƒ‰ ì¿¼ë¦¬ ë³´ì¥
            MIN_QUERIES = 3
            MAX_QUERIES = 8
            if len(search_queries) < MIN_QUERIES:
                logger.info(f"[{self.name}] Only {len(search_queries)} queries available, generating additional queries to ensure diversity...")
                # ì‚¬ìš©ì ì¿¼ë¦¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹¤ì–‘í•œ ê´€ì ì˜ ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„±
                base_query = query
                additional_queries = []
                
                # ë‹¤ì–‘í•œ ê´€ì ì˜ ì¿¼ë¦¬ ìƒì„±
                query_variations = [
                    f"{base_query} ë¶„ì„",
                    f"{base_query} ì „ë§ {datetime.now().year}",
                    f"{base_query} ë™í–¥",
                    f"{base_query} í˜„í™©",
                    f"{base_query} ì „ë¬¸ê°€ ì˜ê²¬"
                ]
                
                for variation in query_variations:
                    if variation not in search_queries and len(search_queries) < MAX_QUERIES:
                        search_queries.append(variation)
                        additional_queries.append(variation)
                
                if additional_queries:
                    logger.info(f"[{self.name}] Added {len(additional_queries)} additional query variations: {additional_queries}")
            
            # ë³‘ë ¬ ê²€ìƒ‰ ì‹¤í–‰
            logger.info(f"[{self.name}] Executing {len(search_queries)} searches in parallel...")
            
            # ê²€ìƒ‰ ì¿¼ë¦¬ ì¤‘ë³µ ì œê±° (Strict Deduplication)
            unique_search_queries = []
            seen_queries = set()
            normalized_base_query = re.sub(r'\s+', ' ', query.lower().strip())
            
            for q in search_queries:
                q_normalized = re.sub(r'\s+', ' ', q.lower().strip())
                
                # 1. ì´ë¯¸ ì²˜ë¦¬ëœ ì¿¼ë¦¬ì¸ì§€ í™•ì¸
                if q_normalized in seen_queries:
                    logger.warning(f"[{self.name}] âš ï¸ Duplicate query removed: '{q}'")
                    continue
                
                # 2. Base ì¿¼ë¦¬ì™€ ì™„ì „íˆ ë™ì¼í•œ ê²½ìš° (ì´ë¯¸ í¬í•¨ë˜ì–´ ìˆì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ)
                # ë‹¨, ì²« ë²ˆì§¸ ì¿¼ë¦¬ê°€ Base ì¿¼ë¦¬ì¸ ê²½ìš°ëŠ” í—ˆìš©
                if q_normalized == normalized_base_query and seen_queries:
                    logger.warning(f"[{self.name}] âš ï¸ Query identical to user query removed (redundant): '{q}'")
                    continue
                    
                seen_queries.add(q_normalized)
                unique_search_queries.append(q)
            
            search_queries = unique_search_queries
            logger.info(f"[{self.name}] Unique search queries ({len(search_queries)}): {search_queries}")
            
            async def execute_single_search(search_query: str, query_index: int) -> Dict[str, Any]:
                """ë‹¨ì¼ ê²€ìƒ‰ ì‹¤í–‰ (ì—¬ëŸ¬ ê²€ìƒ‰ ë„êµ¬ fallback ì§€ì›)."""
                # ì‹¤ì œ ê²€ìƒ‰ ì¿¼ë¦¬ ê°’ ë¡œê·¸ ì¶œë ¥
                logger.info(f"[{self.name}] Search {query_index + 1}/{len(search_queries)}: '{search_query}'")
                
                # ê° ê²€ìƒ‰ë§ˆë‹¤ ë” ë§ì€ ê²°ê³¼ ìˆ˜ì§‘ (ìµœì†Œ 30ê°œ ì¶œì²˜ ë³´ì¥ì„ ìœ„í•´)
                # ì—¬ëŸ¬ ê²€ìƒ‰ ì¿¼ë¦¬ ì‚¬ìš© ì‹œ ê° ì¿¼ë¦¬ë‹¹ ìµœì†Œ 10-15ê°œì”© ìˆ˜ì§‘í•˜ì—¬ ì´ 30ê°œ ì´ìƒ ë³´ì¥
                num_queries = len(search_queries)
                results_per_query = max(10, min(15, 30 // max(1, num_queries)))  # ìµœì†Œ 10ê°œ, ìµœëŒ€ 15ê°œ, ì´ 30ê°œ ì´ìƒ ë³´ì¥
                
                # ì—¬ëŸ¬ ê²€ìƒ‰ ë„êµ¬ ì‹œë„ (fallback ì§€ì›)
                search_tools = ["g-search", "mcp_search", "ddg_search"]  # ìš°ì„ ìˆœìœ„ ìˆœì„œ
                
                for tool_name in search_tools:
                    try:
                        logger.info(f"[{self.name}] Trying search tool: {tool_name}")
                        search_result = await execute_tool(
                            tool_name,
                            {"query": search_query, "max_results": results_per_query}
                        )
                        
                        # ì„±ê³µí•œ ê²½ìš°
                        if search_result.get('success', False):
                            logger.info(f"[{self.name}] âœ… Search succeeded with {tool_name}")
                            return {
                                "query": search_query,
                                "index": query_index,
                                "result": search_result,
                                "success": True,
                                "tool_used": tool_name
                            }
                        else:
                            # ì‹¤íŒ¨í–ˆì§€ë§Œ ì—ëŸ¬ê°€ ì—†ëŠ” ê²½ìš° (ë‹¤ìŒ ë„êµ¬ ì‹œë„)
                            error_msg = search_result.get('error', 'Unknown error')
                            logger.warning(f"[{self.name}] âš ï¸ {tool_name} returned success=False: {error_msg}")
                            continue
                            
                    except Exception as e:
                        error_str = str(e)
                        # DuckDuckGo MCP ì„œë²„ ë²„ê·¸ ë“± íŠ¹ì • ì—ëŸ¬ ì²˜ë¦¬
                        if "AttributeError" in error_str or "TimeoutError" in error_str or "HTTPStatusError" in error_str:
                            logger.warning(f"[{self.name}] âš ï¸ {tool_name} failed with known issue: {error_str[:100]}... (trying next tool)")
                        else:
                            logger.warning(f"[{self.name}] âš ï¸ {tool_name} failed: {error_str[:100]}... (trying next tool)")
                        continue
                
                # ëª¨ë“  ê²€ìƒ‰ ë„êµ¬ ì‹¤íŒ¨
                logger.error(f"[{self.name}] âŒ All search tools failed for query: '{search_query}'")
                return {
                    "query": search_query,
                    "index": query_index,
                    "result": {"success": False, "error": "All search tools failed"},
                    "success": False
                }
            
            # ëª¨ë“  ê²€ìƒ‰ì„ ë³‘ë ¬ë¡œ ì‹¤í–‰
            search_tasks = [execute_single_search(q, i) for i, q in enumerate(search_queries)]
            search_results_list = await asyncio.gather(*search_tasks)
            
            logger.info(f"[{self.name}] âœ… Completed {len(search_results_list)} parallel searches")
            
            # ëª¨ë“  ì„±ê³µí•œ ê²€ìƒ‰ ê²°ê³¼ í†µí•©
            successful_results = [sr for sr in search_results_list if sr.get('success') and sr.get('result', {}).get('data')]
            
            # ìµœì†Œ 30ê°œ ê²°ê³¼ ë³´ì¥ì„ ìœ„í•œ ì¶”ê°€ ê²€ìƒ‰ ë¡œì§
            MIN_REQUIRED_RESULTS = 30
            total_results_count = 0
            for sr in successful_results:
                result_data = sr.get('result', {}).get('data', {})
                if isinstance(result_data, dict):
                    total_results_count += len(result_data.get('results', result_data.get('items', [])))
                elif isinstance(result_data, list):
                    total_results_count += len(result_data)
            
            logger.info(f"[{self.name}] ğŸ“Š Total results collected so far: {total_results_count}")
            
            # ê²°ê³¼ê°€ ë¶€ì¡±í•˜ë©´ ì¶”ê°€ ê²€ìƒ‰ ìˆ˜í–‰
            if total_results_count < MIN_REQUIRED_RESULTS and len(search_queries) > 0:
                additional_queries_needed = (MIN_REQUIRED_RESULTS - total_results_count) // 10 + 1
                logger.info(f"[{self.name}] ğŸ” Results insufficient ({total_results_count} < {MIN_REQUIRED_RESULTS}), generating {additional_queries_needed} additional search queries...")
                
                # ì¶”ê°€ ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„± (ë™ì˜ì–´, ê´€ë ¨ ìš©ì–´ ê¸°ë°˜)
                from src.core.llm_manager import execute_llm_task, TaskType
                query_expansion_prompt = f"""Generate {additional_queries_needed} additional search queries related to the following research topic. Use synonyms, related terms, and different perspectives.

Original queries: {', '.join(search_queries[:3])}
Research topic: {state['user_query']}

Generate diverse search queries that will help find more relevant documents. Each query should be specific and different from the original queries.

Return only the queries, one per line, without numbering or bullets."""
                
                try:
                    expansion_result = await execute_llm_task(
                        prompt=query_expansion_prompt,
                        task_type=TaskType.PLANNING,
                        model_name=None,
                        system_message="You are a search query expansion expert. Generate diverse, specific search queries."
                    )
                    
                    additional_queries = [q.strip() for q in expansion_result.content.split('\n') if q.strip() and len(q.strip()) > 10]
                    additional_queries = additional_queries[:additional_queries_needed]
                    
                    logger.info(f"[{self.name}] âœ… Generated {len(additional_queries)} additional search queries")
                    
                    # ì¶”ê°€ ê²€ìƒ‰ ìˆ˜í–‰
                    additional_search_tasks = [execute_single_search(q, len(search_queries) + i) for i, q in enumerate(additional_queries)]
                    additional_search_results = await asyncio.gather(*additional_search_tasks)
                    search_results_list.extend(additional_search_results)
                    
                    # ì„±ê³µí•œ ê²°ê³¼ ì—…ë°ì´íŠ¸
                    successful_results = [sr for sr in search_results_list if sr.get('success') and sr.get('result', {}).get('data')]
                    logger.info(f"[{self.name}] âœ… Additional searches completed: {len([sr for sr in additional_search_results if sr.get('success')])} successful")
                except Exception as e:
                    logger.warning(f"[{self.name}] âš ï¸ Failed to generate additional queries: {e}")
            
            if not successful_results:
                # ì‹¤íŒ¨í•œ ê²€ìƒ‰ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘
                failed_searches = [sr for sr in search_results_list if not sr.get('success')]
                error_details = []
                for fs in failed_searches:
                    query = fs.get('query', 'unknown')
                    result = fs.get('result', {})
                    error = result.get('error', 'Unknown error')
                    error_details.append(f"  - Query: '{query[:60]}...' â†’ Error: {str(error)[:100]}")
                
                logger.error(f"[{self.name}] âŒ ëª¨ë“  ê²€ìƒ‰ ì¿¼ë¦¬ ì‹¤í–‰ ì‹¤íŒ¨ ({len(failed_searches)}/{len(search_results_list)} ì‹¤íŒ¨)")
                logger.error(f"[{self.name}] ğŸ“‹ ì‹¤íŒ¨ ìƒì„¸:")
                for detail in error_details:
                    logger.error(f"[{self.name}] {detail}")
                
                # MCP ì„œë²„ ì—°ê²° ìƒíƒœ í™•ì¸
                try:
                    from src.core.mcp_integration import get_mcp_hub
                    mcp_hub = get_mcp_hub()
                    connected_servers = list(mcp_hub.mcp_sessions.keys()) if mcp_hub.mcp_sessions else []
                    logger.error(f"[{self.name}] ğŸ”Œ í˜„ì¬ ì—°ê²°ëœ MCP ì„œë²„: {connected_servers if connected_servers else 'ì—†ìŒ'}")
                    logger.error(f"[{self.name}] ğŸ“ Fallback (duckduckgo_search ë¼ì´ë¸ŒëŸ¬ë¦¬)ê°€ ì‘ë™í–ˆëŠ”ì§€ í™•ì¸ í•„ìš”")
                except Exception as e:
                    logger.debug(f"[{self.name}] MCP Hub ìƒíƒœ í™•ì¸ ì‹¤íŒ¨: {e}")
                
                error_msg = f"ì—°êµ¬ ì‹¤í–‰ ì‹¤íŒ¨: ëª¨ë“  ê²€ìƒ‰ ì¿¼ë¦¬ ì‹¤í–‰ì´ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ({len(failed_searches)}/{len(search_results_list)} ì‹¤íŒ¨)"
                raise RuntimeError(error_msg)
            
            # ëª¨ë“  ê²€ìƒ‰ ê²°ê³¼ë¥¼ í†µí•© (í•˜ë“œì½”ë”© ì œê±°, ë™ì  í†µí•©)
            all_search_data = []
            for sr in successful_results:
                result_data = sr['result'].get('data', {})
                if isinstance(result_data, dict):
                    items = result_data.get('results', result_data.get('items', []))
                    if isinstance(items, list):
                        all_search_data.extend(items)
                elif isinstance(result_data, list):
                    all_search_data.extend(result_data)
            
            # í†µí•©ëœ ê²°ê³¼ë¥¼ í•˜ë‚˜ì˜ ê²€ìƒ‰ ê²°ê³¼ í˜•ì‹ìœ¼ë¡œ êµ¬ì„±
            search_result = {
                'success': True,
                'data': {
                    'results': all_search_data,
                    'total_results': len(all_search_data),
                    'source': 'parallel_search'
                }
            }
            
            logger.info(f"[{self.name}] âœ… Integrated {len(all_search_data)} results from {len(successful_results)} successful searches")
            
            # ëª¨ë“  ê²€ìƒ‰ ê²°ê³¼ë¥¼ SharedResultsManagerì— ê³µìœ 
            if self.context.shared_results_manager:
                shared_count = 0
                for sr in search_results_list:
                    if sr.get('success'):
                        task_id = f"search_{sr['index']}"
                        result_id = await self.context.shared_results_manager.share_result(
                            task_id=task_id,
                            agent_id=self.context.agent_id,  # ê³ ìœ í•œ agent_id ì‚¬ìš©
                            result=sr['result'],
                            metadata={"query": sr['query'], "index": sr['index']},
                            confidence=1.0 if sr.get('success') else 0.0
                        )
                        shared_count += 1
                        logger.info(f"[{self.name}] ğŸ”— Shared search result for query: '{sr['query'][:50]}...' (result_id: {result_id[:8]}..., agent_id: {self.context.agent_id})")

                # ê³µìœ  í†µê³„ ë¡œê¹…
                total_results = len([sr for sr in search_results_list if sr.get('success')])
                logger.info(f"[{self.name}] ğŸ“¤ Shared {shared_count}/{total_results} successful search results with other agents")
                logger.info(f"[{self.name}] ğŸ¤ Agent communication: {shared_count} results shared via SharedResultsManager")
            
            logger.info(f"[{self.name}] Search completed: success={search_result.get('success')}, total_results={search_result.get('data', {}).get('total_results', 0)}")
            logger.info(f"[{self.name}] Search result type: {type(search_result)}, keys: {list(search_result.keys()) if isinstance(search_result, dict) else 'N/A'}")
            
            if search_result.get('success') and search_result.get('data'):
                data = search_result.get('data', {})
                logger.info(f"[{self.name}] Data type: {type(data)}, keys: {list(data.keys()) if isinstance(data, dict) else 'N/A'}")
                
                # ê²€ìƒ‰ ê²°ê³¼ íŒŒì‹± - ë‹¤ì–‘í•œ í˜•ì‹ ì§€ì›
                search_results = []
                if isinstance(data, dict):
                    # í‘œì¤€ í˜•ì‹: {"query": "...", "results": [...], "total_results": N, "source": "..."}
                    search_results = data.get('results', [])
                    logger.info(f"[{self.name}] Found 'results' key: {len(search_results)} items")
                    
                    if not search_results:
                        # ë‹¤ë¥¸ í‚¤ ì‹œë„
                        search_results = data.get('items', data.get('data', []))
                        logger.info(f"[{self.name}] Tried 'items' or 'data' keys: {len(search_results)} items")
                    
                    # data ìì²´ê°€ ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° (ì¤‘ì²©ëœ ê²½ìš°)
                    if not search_results and isinstance(data, dict):
                        # dataì˜ ê°’ ì¤‘ ë¦¬ìŠ¤íŠ¸ ì°¾ê¸°
                        for key, value in data.items():
                            if isinstance(value, list) and len(value) > 0:
                                # ì²« ë²ˆì§¸ í•­ëª©ì´ dictì¸ì§€ í™•ì¸
                                if value and isinstance(value[0], dict):
                                    search_results = value
                                    logger.info(f"[{self.name}] Found list in key '{key}': {len(search_results)} items")
                                    break
                elif isinstance(data, list):
                    search_results = data
                    logger.info(f"[{self.name}] Data is directly a list: {len(search_results)} items")
                
                logger.info(f"[{self.name}] âœ… Parsed {len(search_results)} search results")
                
                # ë””ë²„ê¹…: ì²« ë²ˆì§¸ ê²°ê³¼ ìƒ˜í”Œ ì¶œë ¥
                if search_results and len(search_results) > 0:
                    first_result = search_results[0]
                    logger.info(f"[{self.name}] First result type: {type(first_result)}, sample: {str(first_result)[:200]}")
                
                # Phase 2: ê²€ìƒ‰ ê²°ê³¼ ê´€ë ¨ì„± ì‚¬ì „ í•„í„°ë§
                if search_results and len(search_results) > 0:
                    logger.info(f"[{self.name}] ğŸ” Starting relevance pre-filtering for {len(search_results)} results...")
                    filtered_results = await self._filter_results_by_relevance(
                        search_results, 
                        state['user_query'],
                        assigned_task.get('search_queries', [state['user_query']]) if assigned_task else [state['user_query']],
                        current_time
                    )
                    search_results = filtered_results
                    logger.info(f"[{self.name}] âœ… Relevance filtering completed: {len(search_results)} relevant results (from {len(search_results) + (len(search_results) - len(filtered_results)) if len(filtered_results) < len(search_results) else 0} total)")
                    
                    # ì˜ë¬¸ì  ê°ì§€ (ê²€ìƒ‰ ê²°ê³¼ê°€ ëª¨í˜¸í•˜ê±°ë‚˜ ì‚¬ìš©ì ì„ í˜¸ë„ê°€ í•„ìš”í•œ ê²½ìš°)
                    if len(filtered_results) > 10 or len(filtered_results) == 0:
                        # ê²°ê³¼ê°€ ë„ˆë¬´ ë§ê±°ë‚˜ ì—†ìœ¼ë©´ ì‚¬ìš©ìì—ê²Œ ì§ˆë¬¸
                        from src.core.human_clarification_handler import get_clarification_handler
                        clarification_handler = get_clarification_handler()
                        
                        # ì˜ë¬¸ì  ìƒì„±
                        ambiguity = {
                            "type": "resource_constraint" if len(filtered_results) > 10 else "scope_depth",
                            "field": "result_count",
                            "description": f"Found {len(filtered_results)} results. Need to clarify scope or priority.",
                            "suggested_question": "ê²€ìƒ‰ ê²°ê³¼ê°€ ë§ìŠµë‹ˆë‹¤. ì–´ë–¤ ë°©í–¥ìœ¼ë¡œ ì§„í–‰í• ê¹Œìš”?" if len(filtered_results) > 10 else "ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ê²€ìƒ‰ ë²”ìœ„ë¥¼ ì¡°ì •í• ê¹Œìš”?",
                            "suggested_options": [
                                {"label": "ìƒìœ„ 5ê°œ ê²°ê³¼ë§Œ ì‚¬ìš©", "value": "top_5"},
                                {"label": "ìƒìœ„ 10ê°œ ê²°ê³¼ ì‚¬ìš©", "value": "top_10"},
                                {"label": "ëª¨ë“  ê²°ê³¼ ì‚¬ìš©", "value": "all"}
                            ] if len(filtered_results) > 10 else [
                                {"label": "ê²€ìƒ‰ ë²”ìœ„ í™•ëŒ€", "value": "expand"},
                                {"label": "ê²€ìƒ‰ì–´ ìˆ˜ì •", "value": "modify"},
                                {"label": "ê³„ì† ì§„í–‰", "value": "continue"}
                            ]
                        }
                        
                        question = await clarification_handler.generate_question(
                            ambiguity,
                            {'user_request': state['user_query'], 'result_count': len(filtered_results)}
                        )
                        
                        # CLI ëª¨ë“œ ê°ì§€ (ë” ì •í™•í•œ ë°©ë²•)
                        import sys
                        is_cli_mode = (
                            not hasattr(sys, 'ps1') and  # Interactive shellì´ ì•„ë‹˜
                            'streamlit' not in sys.modules and  # Streamlitì´ ë¡œë“œë˜ì§€ ì•ŠìŒ
                            not any('streamlit' in str(arg) for arg in sys.argv)  # Streamlit ì‹¤í–‰ ì¸ìê°€ ì—†ìŒ
                        )
                        
                        # CLI ëª¨ë“œì´ê±°ë‚˜ autopilot ëª¨ë“œì¸ ê²½ìš° ìë™ ì„ íƒ
                        if is_cli_mode or state.get("autopilot_mode", False):
                            logger.info("ğŸ¤– CLI/Autopilot mode - auto-selecting response")
                            
                            # History ê¸°ë°˜ ìë™ ì„ íƒ
                            shared_memory = self.context.shared_memory
                            auto_response = await clarification_handler.auto_select_response(
                                question,
                                {'user_request': state['user_query'], 'result_count': len(filtered_results)},
                                shared_memory
                            )
                            
                            # ì‘ë‹µ ì²˜ë¦¬
                            processed = await clarification_handler.process_user_response(
                                question['id'],
                                auto_response,
                                {'question': question}
                            )
                            
                            if processed.get('validated', False):
                                # ëª…í™•í™” ì •ë³´ ì ìš©
                                clarification = processed.get('clarification', {})
                                
                                # ì‘ë‹µì— ë”°ë¼ ê²°ê³¼ í•„í„°ë§
                                if auto_response == "top_5":
                                    filtered_results = filtered_results[:5]
                                elif auto_response == "top_10":
                                    filtered_results = filtered_results[:10]
                                # "all"ì´ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
                                
                                logger.info(f"âœ… Auto-selected: {auto_response}, using {len(filtered_results)} results")
                                # ê³„ì† ì§„í–‰ (return í•˜ì§€ ì•ŠìŒ)
                        else:
                            # ì›¹ ëª¨ë“œ: ì‚¬ìš©ìì—ê²Œ ì§ˆë¬¸
                            state['pending_questions'] = state.get('pending_questions', []) + [question]
                            state['waiting_for_user'] = True
                            state['user_responses'] = state.get('user_responses', {})
                            
                            logger.info(f"â“ Generated question during execution: {question['id']}")
                            logger.info("â¸ï¸ Waiting for user response...")
                            
                            return state
                
                if search_results and len(search_results) > 0:
                    # ì‹¤ì œ ê²€ìƒ‰ ê²°ê³¼ë¥¼ êµ¬ì¡°í™”ëœ í˜•ì‹ìœ¼ë¡œ ì €ì¥
                    unique_results = []
                    seen_urls = set()
                    filtered_count = 0
                    filtered_reasons = []
                    
                    # ì‹¤ì œ ê²€ìƒ‰ ì¿¼ë¦¬ ê°’ ë¡œê·¸ ì¶œë ¥ (query ë³€ìˆ˜ëŠ” ì‹¤ì œ ê²€ìƒ‰ ì¿¼ë¦¬)
                    actual_query = query if isinstance(query, str) else str(query)
                    logger.info(f"[{self.name}] Processing {len(search_results)} results for query: '{actual_query}'")
                    
                    for i, result in enumerate(search_results, 1):
                        # ë‹¤ì–‘í•œ í˜•ì‹ ì§€ì›
                        if isinstance(result, dict):
                            title = result.get('title', result.get('name', result.get('Title', 'No title')))
                            snippet = result.get('snippet', result.get('content', result.get('summary', result.get('description', result.get('abstract', '')))))
                            url = result.get('url', result.get('link', result.get('href', result.get('URL', ''))))
                            
                            # snippetì— ë§ˆí¬ë‹¤ìš´ í˜•ì‹ì˜ ì—¬ëŸ¬ ê²°ê³¼ê°€ ë“¤ì–´ìˆëŠ” ê²½ìš° íŒŒì‹±
                            if snippet and ("Found" in snippet or "search results" in snippet.lower() or "\n1." in snippet):
                                logger.info(f"[{self.name}] Detected markdown format in snippet, parsing...")
                                parsed_results = []
                                lines = snippet.split('\n')
                                current_result = None
                                
                                for line in lines:
                                    original_line = line
                                    line = line.strip()
                                    if not line:
                                        continue
                                    
                                    # íŒ¨í„´ 1: ë§ˆí¬ë‹¤ìš´ ë§í¬ "1. [Title](URL)"
                                    link_match = re.match(r'^\d+\.\s*\[([^\]]+)\]\(([^\)]+)\)', line)
                                    # íŒ¨í„´ 2: ë²ˆí˜¸ì™€ ì œëª©ë§Œ "1. [Title]" ë˜ëŠ” "1. Title"
                                    title_match = re.match(r'^\d+\.\s*(?:\[([^\]]+)\]|(.+?))(?:\s*$|:)', line)
                                    # íŒ¨í„´ 3: URL ì¤„ "   URL: https://..."
                                    url_match = re.search(r'URL:\s*(https?://[^\s]+)', line, re.IGNORECASE)
                                    # íŒ¨í„´ 4: Summary ì¤„ "   Summary: ..."
                                    summary_match = re.search(r'Summary:\s*(.+)$', line, re.IGNORECASE)
                                    
                                    if link_match:
                                        # ì´ì „ ê²°ê³¼ ì €ì¥
                                        if current_result and current_result.get('title'):
                                            parsed_results.append(current_result)
                                        
                                        title_parsed = link_match.group(1)
                                        url_parsed = link_match.group(2)
                                        current_result = {
                                            "title": title_parsed,
                                            "url": url_parsed,
                                            "snippet": ""
                                        }
                                    elif title_match and not current_result:
                                        # ë²ˆí˜¸ì™€ ì œëª©ë§Œ ìˆëŠ” ê²½ìš° (ë‹¤ìŒ ì¤„ì— URLì´ ì˜¬ ê²ƒìœ¼ë¡œ ì˜ˆìƒ)
                                        title_parsed = title_match.group(1) or title_match.group(2)
                                        if title_parsed:
                                            current_result = {
                                                "title": title_parsed.strip(),
                                                "url": "",
                                                "snippet": ""
                                            }
                                    elif url_match:
                                        # URLì´ ë³„ë„ ì¤„ì— ìˆëŠ” ê²½ìš°
                                        if current_result:
                                            current_result["url"] = url_match.group(1)
                                        else:
                                            # URLë§Œ ìˆê³  ì œëª©ì´ ì—†ëŠ” ê²½ìš° (ì´ì „ ê²°ê³¼ì— ì¶”ê°€)
                                            if parsed_results:
                                                parsed_results[-1]["url"] = url_match.group(1)
                                    elif summary_match and current_result:
                                        # Summary ì¤„
                                        current_result["snippet"] = summary_match.group(1).strip()
                                    elif current_result and line and not any([
                                        line.startswith('URL:'), 
                                        line.startswith('Summary:'),
                                        line.startswith('Found'),
                                        'search results' in line.lower()
                                    ]):
                                        # ì„¤ëª… í…ìŠ¤íŠ¸ (ë“¤ì—¬ì“°ê¸°ëœ ê²½ìš°)
                                        if original_line.startswith('   ') or original_line.startswith('\t'):
                                            if current_result["snippet"]:
                                                current_result["snippet"] += " " + line
                                            else:
                                                current_result["snippet"] = line
                                
                                # ë§ˆì§€ë§‰ ê²°ê³¼ ì¶”ê°€
                                if current_result and current_result.get('title'):
                                    parsed_results.append(current_result)
                                
                                if parsed_results:
                                    logger.info(f"[{self.name}] Parsed {len(parsed_results)} results from markdown snippet")
                                    # íŒŒì‹±ëœ ê²°ê³¼ë“¤ì„ unique_resultsì— ì¶”ê°€
                                    for parsed_result in parsed_results:
                                        parsed_url = parsed_result.get('url', '')
                                        parsed_title = parsed_result.get('title', '')
                                        parsed_snippet = parsed_result.get('snippet', '')
                                        
                                        if parsed_url and parsed_url in seen_urls:
                                            logger.debug(f"[{self.name}] Duplicate URL skipped in parsed results: {parsed_url[:50]}")
                                            continue
                                        if parsed_url:
                                            seen_urls.add(parsed_url)
                                        
                                        # ë§ˆí¬ë‹¤ìš´ íŒŒì‹± ê²°ê³¼ë„ í•„í„°ë§ ì ìš©
                                        invalid_indicators = [
                                            "no results were found", "bot detection",
                                            "no results", "not found", "try again",
                                            "unable to", "error occurred", "no matches"
                                        ]
                                        parsed_snippet_lower = parsed_snippet.lower() if parsed_snippet else ""
                                        matched_indicators = [ind for ind in invalid_indicators if ind in parsed_snippet_lower]
                                        
                                        if matched_indicators:
                                            filtered_count += 1
                                            reason = f"Matched indicators: {', '.join(matched_indicators)}"
                                            filtered_reasons.append({
                                                "result_index": f"{i}(parsed)",
                                                "title": parsed_title[:80],
                                                "reason": reason,
                                                "snippet_preview": parsed_snippet[:200] if parsed_snippet else "(empty)"
                                            })
                                            logger.warning(f"[{self.name}] âš ï¸ Filtering invalid parsed result: '{parsed_title[:60]}...' - Reason: {reason}")
                                            continue
                                        
                                        unique_results.append({
                                            "index": len(unique_results) + 1,
                                            "title": parsed_title,
                                            "snippet": parsed_snippet[:500],
                                            "url": parsed_url,
                                            "source": "search"
                                        })
                                        logger.info(f"[{self.name}] Parsed result: {parsed_title[:50]}... (URL: {parsed_url[:50] if parsed_url else 'N/A'}...)")
                                    
                                    # ì›ë³¸ ê²°ê³¼ëŠ” ê±´ë„ˆë›°ê¸°
                                    continue
                            
                            logger.debug(f"[{self.name}] Result {i}: title={title[:50] if title else 'N/A'}, url={url[:50] if url else 'N/A'}")
                        elif isinstance(result, str):
                            # ë¬¸ìì—´ í˜•ì‹ì¸ ê²½ìš° íŒŒì‹± ì‹œë„ (ë§ˆí¬ë‹¤ìš´ ë§í¬ í˜•ì‹)
                            link_match = re.match(r'^\d+\.\s*\[([^\]]+)\]\(([^\)]+)\)', result.strip())
                            if link_match:
                                title = link_match.group(1)
                                url = link_match.group(2)
                                snippet = ""
                                logger.info(f"[{self.name}] Parsed string result {i} as markdown: {title[:50]}")
                            else:
                                logger.warning(f"[{self.name}] Result {i} is string but not markdown format, skipping: {result[:100]}")
                                continue
                        else:
                            logger.warning(f"[{self.name}] Unknown result format for result {i}: {type(result)}, value: {str(result)[:100]}")
                            continue
                        
                        # URL ì¤‘ë³µ ì œê±°
                        if url and url in seen_urls:
                            logger.debug(f"[{self.name}] Duplicate URL skipped: {url}")
                            continue
                        if url:
                            seen_urls.add(url)
                        
                        # ë””ë²„ê¹…: ì›ë³¸ ë°ì´í„° ë¡œê¹…
                        logger.debug(f"[{self.name}] Result {i} ì›ë³¸ ë°ì´í„° - title: '{title[:80]}', snippet: '{snippet[:150] if snippet else '(empty)'}', url: '{url[:80] if url else '(empty)'}'")
                        
                        # snippet ë‚´ìš©ìœ¼ë¡œ ìœ íš¨í•˜ì§€ ì•Šì€ ê²€ìƒ‰ ê²°ê³¼ í•„í„°ë§
                        invalid_indicators = [
                            "no results were found", "bot detection",
                            "no results", "not found", "try again",
                            "unable to", "error occurred", "no matches"
                        ]
                        snippet_lower = snippet.lower() if snippet else ""
                        matched_indicators = [ind for ind in invalid_indicators if ind in snippet_lower]
                        
                        if matched_indicators:
                            filtered_count += 1
                            reason = f"Matched indicators: {', '.join(matched_indicators)}"
                            filtered_reasons.append({
                                "result_index": i,
                                "title": title[:80],
                                "reason": reason,
                                "snippet_preview": snippet[:200] if snippet else "(empty)"
                            })
                            logger.warning(f"[{self.name}] âš ï¸ Filtering invalid search result {i}: '{title[:60]}...' - Reason: {reason}")
                            logger.debug(f"[{self.name}]   Filtered snippet preview: '{snippet[:200] if snippet else '(empty)'}'")
                            continue
                        
                        # êµ¬ì¡°í™”ëœ ê²°ê³¼ ì €ì¥
                        result_dict = {
                            "index": len(unique_results) + 1,
                            "title": title,
                            "snippet": snippet[:500] if snippet else "",
                            "url": url,
                            "source": "search"
                        }
                        unique_results.append(result_dict)
                        
                        logger.info(f"[{self.name}] Result {i}: {title[:50]}... (URL: {url[:50] if url else 'N/A'}...)")
                    
                    # í•„í„°ë§ í†µê³„ ë¡œê¹…
                    total_processed = len(search_results)
                    valid_results = len(unique_results)
                    logger.info(f"[{self.name}] ğŸ“Š í•„í„°ë§ í†µê³„: ì´ {total_processed}ê°œ ì¤‘ {filtered_count}ê°œ í•„í„°ë§ë¨, {valid_results}ê°œ ìœ íš¨í•œ ê²°ê³¼")
                    
                    if filtered_count > 0:
                        logger.warning(f"[{self.name}] âš ï¸ í•„í„°ë§ëœ ê²°ê³¼ ìƒì„¸:")
                        for fr in filtered_reasons[:5]:  # ìµœëŒ€ 5ê°œë§Œ ìƒì„¸ ë¡œê¹…
                            logger.warning(f"[{self.name}]   - ê²°ê³¼ {fr['result_index']}: '{fr['title']}' - {fr['reason']}")
                            logger.warning(f"[{self.name}]     Snippet: '{fr['snippet_preview']}'")
                        if len(filtered_reasons) > 5:
                            logger.warning(f"[{self.name}]   ... ì™¸ {len(filtered_reasons) - 5}ê°œ ê²°ê³¼ë„ í•„í„°ë§ë¨")
                    
                    # ê²°ê³¼ë¥¼ êµ¬ì¡°í™”ëœ í˜•ì‹ìœ¼ë¡œ ì €ì¥
                    if unique_results:
                        results = unique_results
                        logger.info(f"[{self.name}] âœ… Collected {len(results)} unique results")
                        
                        # ìµœì†Œ 5ê°œ ì´ìƒì˜ ê³ ìœ í•œ ì¶œì²˜ ë³´ì¥
                        MIN_UNIQUE_SOURCES = 5
                        unique_urls = set()
                        for result in results:
                            url = result.get('url', '')
                            if url:
                                # URLì—ì„œ ë„ë©”ì¸ ì¶”ì¶œ
                                try:
                                    from urllib.parse import urlparse
                                    parsed = urlparse(url)
                                    domain = f"{parsed.scheme}://{parsed.netloc}"
                                    unique_urls.add(domain)
                                except:
                                    unique_urls.add(url)
                        
                        logger.info(f"[{self.name}] ğŸ“Š Unique sources found: {len(unique_urls)} (minimum required: {MIN_UNIQUE_SOURCES})")
                        
                        # ì¶œì²˜ê°€ ë¶€ì¡±í•˜ë©´ ì¶”ê°€ ê²€ìƒ‰ ìˆ˜í–‰
                        if len(unique_urls) < MIN_UNIQUE_SOURCES:
                            logger.warning(f"[{self.name}] âš ï¸ Only {len(unique_urls)} unique sources found, need at least {MIN_UNIQUE_SOURCES}. Performing additional searches...")
                            
                            # ì¶”ê°€ ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„± (ë‹¤ì–‘í•œ ê´€ì )
                            additional_queries = []
                            base_query = query
                            
                            # ë‹¤ì–‘í•œ ê²€ìƒ‰ì–´ íŒ¨í„´ ì‹œë„
                            additional_patterns = [
                                f"{base_query} ë‰´ìŠ¤",
                                f"{base_query} ë¦¬í¬íŠ¸",
                                f"{base_query} ì¡°ì‚¬",
                                f"{base_query} í†µê³„",
                                f"{base_query} ìë£Œ"
                            ]
                            
                            # ì´ë¯¸ ì‚¬ìš©í•œ ì¿¼ë¦¬ ì œì™¸
                            used_queries = set(search_queries)
                            for pattern in additional_patterns:
                                if pattern not in used_queries and len(additional_queries) < 3:
                                    additional_queries.append(pattern)
                            
                            if additional_queries:
                                logger.info(f"[{self.name}] ğŸ” Executing {len(additional_queries)} additional searches for more sources...")
                                
                                # ì¶”ê°€ ê²€ìƒ‰ ì‹¤í–‰
                                additional_search_tasks = [execute_single_search(q, len(search_queries) + i) for i, q in enumerate(additional_queries)]
                                additional_results_list = await asyncio.gather(*additional_search_tasks)
                                
                                # ì¶”ê°€ ê²€ìƒ‰ ê²°ê³¼ í†µí•©
                                additional_unique_results = []
                                additional_seen_urls = seen_urls.copy()
                                
                                for sr in additional_results_list:
                                    if sr.get('success') and sr.get('result', {}).get('data'):
                                        result_data = sr['result'].get('data', {})
                                        if isinstance(result_data, dict):
                                            items = result_data.get('results', result_data.get('items', []))
                                            if isinstance(items, list):
                                                for item in items:
                                                    if isinstance(item, dict):
                                                        url = item.get('url', item.get('link', ''))
                                                        if url and url not in additional_seen_urls:
                                                            title = item.get('title', item.get('name', ''))
                                                            snippet = item.get('snippet', item.get('content', ''))
                                                            if title and len(title.strip()) >= 3:
                                                                additional_unique_results.append({
                                                                    "index": len(results) + len(additional_unique_results) + 1,
                                                                    "title": title,
                                                                    "snippet": snippet[:500] if snippet else '',
                                                                    "url": url,
                                                                    "source": "additional_search"
                                                                })
                                                                additional_seen_urls.add(url)
                                        
                                        # ë„ë©”ì¸ ì¶”ì¶œí•˜ì—¬ ê³ ìœ  ì¶œì²˜ í™•ì¸
                                        for item in additional_unique_results:
                                            url = item.get('url', '')
                                            if url:
                                                try:
                                                    from urllib.parse import urlparse
                                                    parsed = urlparse(url)
                                                    domain = f"{parsed.scheme}://{parsed.netloc}"
                                                    unique_urls.add(domain)
                                                except:
                                                    unique_urls.add(url)
                                        
                                        # ì¶©ë¶„í•œ ì¶œì²˜ë¥¼ ì–»ìœ¼ë©´ ì¤‘ë‹¨
                                        if len(unique_urls) >= MIN_UNIQUE_SOURCES:
                                            break
                                
                                if additional_unique_results:
                                    results.extend(additional_unique_results)
                                    logger.info(f"[{self.name}] âœ… Added {len(additional_unique_results)} additional results from {len(additional_queries)} searches")
                                    logger.info(f"[{self.name}] ğŸ“Š Total unique sources: {len(unique_urls)} (target: {MIN_UNIQUE_SOURCES})")
                                else:
                                    logger.warning(f"[{self.name}] âš ï¸ Additional searches did not yield new unique sources")
                            else:
                                logger.warning(f"[{self.name}] âš ï¸ No additional query patterns available")
                        else:
                            logger.info(f"[{self.name}] âœ… Sufficient unique sources found: {len(unique_urls)} >= {MIN_UNIQUE_SOURCES}")
                        
                        # ìµœì¢… ê²°ê³¼ ìš”ì•½
                        final_unique_sources = set()
                        for result in results:
                            url = result.get('url', '')
                            if url:
                                try:
                                    from urllib.parse import urlparse
                                    parsed = urlparse(url)
                                    domain = f"{parsed.scheme}://{parsed.netloc}"
                                    final_unique_sources.add(domain)
                                except:
                                    final_unique_sources.add(url)
                        
                        logger.info(f"[{self.name}] ğŸ“Š Final collection: {len(results)} results from {len(final_unique_sources)} unique sources")
                        if len(final_unique_sources) < MIN_UNIQUE_SOURCES:
                            logger.warning(f"[{self.name}] âš ï¸ Warning: Only {len(final_unique_sources)} unique sources collected (target: {MIN_UNIQUE_SOURCES})")
                        
                        # ê²€ìƒ‰ ê²°ê³¼ ê²€í†  ë° ì‹¤ì œ ì›¹ í˜ì´ì§€ ë‚´ìš© í¬ë¡¤ë§
                        logger.info(f"[{self.name}] ğŸ” Reviewing search results and fetching full web content...")
                        
                        # ê²€ìƒ‰ ê²°ê³¼ ê²€í†  ë° ì‹¤ì œ ì›¹ í˜ì´ì§€ í¬ë¡¤ë§
                        enriched_results = []
                        for result in results:
                            url = result.get('url', '')
                            if not url:
                                enriched_results.append(result)
                                continue
                            
                            try:
                                # ì‹¤ì œ ì›¹ í˜ì´ì§€ ë‚´ìš© ê°€ì ¸ì˜¤ê¸°
                                logger.info(f"[{self.name}] ğŸ“¥ Fetching full content from: {url[:80]}...")
                                fetch_result = await execute_tool("fetch", {"url": url})
                                
                                if fetch_result.get('success') and fetch_result.get('data'):
                                    content = fetch_result.get('data', {}).get('content', '')
                                    if content:
                                        # HTML íƒœê·¸ ì œê±° ë° í…ìŠ¤íŠ¸ ì •ë¦¬
                                        from bs4 import BeautifulSoup
                                        
                                        try:
                                            soup = BeautifulSoup(content, 'html.parser')
                                            # ìŠ¤í¬ë¦½íŠ¸, ìŠ¤íƒ€ì¼, í—¤ë”, í‘¸í„° ì œê±°
                                            for element in soup(['script', 'style', 'header', 'footer', 'nav', 'aside']):
                                                element.decompose()
                                            
                                            # ë©”ì¸ ì½˜í…ì¸  ì¶”ì¶œ
                                            main_content = soup.find('main') or soup.find('article') or soup.find('div', class_=re.compile(r'content|article|post|main', re.I))
                                            if main_content:
                                                full_text = main_content.get_text(separator='\n', strip=True)
                                            else:
                                                full_text = soup.get_text(separator='\n', strip=True)
                                            
                                            # í…ìŠ¤íŠ¸ ì •ë¦¬ (ë„ˆë¬´ ê¸´ ê³µë°± ì œê±°)
                                            full_text = re.sub(r'\n{3,}', '\n\n', full_text)
                                            full_text = re.sub(r' {3,}', ' ', full_text)
                                            
                                            # ìµœëŒ€ ê¸¸ì´ ì œí•œ (50000ì)
                                            if len(full_text) > 50000:
                                                full_text = full_text[:50000] + "... [truncated]"
                                            
                                            result['full_content'] = full_text
                                            result['content_length'] = len(full_text)
                                            
                                            # ë‚ ì§œ ì •ë³´ ì¶”ì¶œ ì‹œë„
                                            date_patterns = [
                                                r'(\d{4})[.\-/](\d{1,2})[.\-/](\d{1,2})',  # YYYY-MM-DD
                                                r'(\d{1,2})[.\-/](\d{1,2})[.\-/](\d{4})',  # MM-DD-YYYY
                                                r'(\d{4})ë…„\s*(\d{1,2})ì›”\s*(\d{1,2})ì¼',  # í•œêµ­ì–´ í˜•ì‹
                                            ]
                                            
                                            date_found = None
                                            for pattern in date_patterns:
                                                matches = re.findall(pattern, full_text[:5000])  # ì²˜ìŒ 5000ìë§Œ ê²€ìƒ‰
                                                if matches:
                                                    try:
                                                        match = matches[-1]  # ê°€ì¥ ìµœê·¼ ë‚ ì§œ
                                                        if len(match) == 3:
                                                            if 'ë…„' in full_text[:5000]:
                                                                # í•œêµ­ì–´ í˜•ì‹
                                                                date_str = f"{match[0]}-{match[1].zfill(2)}-{match[2].zfill(2)}"
                                                            elif len(match[0]) == 4:
                                                                # YYYY-MM-DD
                                                                date_str = f"{match[0]}-{match[1].zfill(2)}-{match[2].zfill(2)}"
                                                            else:
                                                                # MM-DD-YYYY
                                                                date_str = f"{match[2]}-{match[0].zfill(2)}-{match[1].zfill(2)}"
                                                            date_found = datetime.strptime(date_str, "%Y-%m-%d")
                                                            break
                                                    except:
                                                        continue
                                            
                                            if date_found:
                                                result['published_date'] = date_found.isoformat()
                                                logger.info(f"[{self.name}] ğŸ“… Found date: {date_found.strftime('%Y-%m-%d')} for {url[:50]}...")
                                            else:
                                                # ë‚ ì§œë¥¼ ì°¾ì§€ ëª»í•œ ê²½ìš° í˜„ì¬ ì‹œê°„ìœ¼ë¡œ ì„¤ì • (ìµœì‹  ì •ë³´ ìš°ì„ )
                                                result['published_date'] = datetime.now().isoformat()
                                                logger.info(f"[{self.name}] âš ï¸ No date found, using current time for {url[:50]}...")
                                            
                                            logger.info(f"[{self.name}] âœ… Fetched {len(full_text)} characters from {url[:50]}...")
                                        except Exception as e:
                                            logger.warning(f"[{self.name}] âš ï¸ Failed to parse HTML from {url[:50]}...: {e}")
                                            # íŒŒì‹± ì‹¤íŒ¨í•´ë„ ì›ë³¸ ê²°ê³¼ëŠ” ìœ ì§€
                                            result['full_content'] = content[:50000] if len(content) > 50000 else content
                                            result['content_length'] = len(result['full_content'])
                                    else:
                                        logger.warning(f"[{self.name}] âš ï¸ No content fetched from {url[:50]}...")
                                else:
                                    logger.warning(f"[{self.name}] âš ï¸ Failed to fetch content from {url[:50]}...: {fetch_result.get('error', 'Unknown error')}")
                            except Exception as e:
                                logger.error(f"[{self.name}] âŒ Error fetching content from {url[:50]}...: {e}")
                            
                            enriched_results.append(result)
                        
                        # ìµœì‹  ì •ë³´ ìš°ì„ ìˆœìœ„ë¡œ ì •ë ¬
                        enriched_results.sort(key=lambda x: (
                            datetime.fromisoformat(x.get('published_date', datetime.now().isoformat())) if x.get('published_date') else datetime.min,
                            x.get('content_length', 0)
                        ), reverse=True)
                        
                        logger.info(f"[{self.name}] âœ… Enriched {len(enriched_results)} results with full web content")
                        results = enriched_results
                        
                        # ê²€ìƒ‰ ê²°ê³¼ ê²€í†  (LLMìœ¼ë¡œ ê²€ìƒ‰ ê²°ê³¼ í‰ê°€)
                        logger.info(f"[{self.name}] ğŸ” Reviewing search results for relevance and recency...")
                        try:
                            from src.core.llm_manager import execute_llm_task, TaskType
                            
                            # ê²€ìƒ‰ ê²°ê³¼ ìš”ì•½ ë° í‰ê°€
                            review_prompt = f"""ë‹¤ìŒì€ '{query}'ì— ëŒ€í•œ ê²€ìƒ‰ ê²°ê³¼ì…ë‹ˆë‹¤. ê° ê²°ê³¼ë¥¼ ê²€í† í•˜ì—¬:
1. ì‚¬ìš©ì ì¿¼ë¦¬ì™€ì˜ ê´€ë ¨ì„± í‰ê°€
2. ì •ë³´ì˜ ìµœì‹ ì„± í™•ì¸ (ë‚ ì§œ ì •ë³´ í¬í•¨)
3. ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì¶œì²˜ì¸ì§€ í™•ì¸
4. ì‹¤ì œ ì›¹ í˜ì´ì§€ ë‚´ìš©ì´ ì¿¼ë¦¬ì™€ ê´€ë ¨ì´ ìˆëŠ”ì§€ í™•ì¸

ê²€ìƒ‰ ê²°ê³¼:
{chr(10).join([f"{i+1}. {r.get('title', 'N/A')} - {r.get('url', 'N/A')} - ë‚ ì§œ: {r.get('published_date', 'N/A')} - ë‚´ìš© ê¸¸ì´: {r.get('content_length', 0)}ì" for i, r in enumerate(results[:10])])}

ê° ê²°ê³¼ì— ëŒ€í•´:
- ê´€ë ¨ì„± ì ìˆ˜ (0-10)
- ìµœì‹ ì„± í‰ê°€ (ìµœì‹ /ë³´í†µ/ì˜¤ë˜ë¨)
- ì‹ ë¢°ë„ í‰ê°€ (ë†’ìŒ/ë³´í†µ/ë‚®ìŒ)
- ì¶”ì²œ ì—¬ë¶€ (ì¶”ì²œ/ë³´í†µ/ë¹„ì¶”ì²œ)

í˜•ì‹: JSON ë°°ì—´ë¡œ ë°˜í™˜
[
  {{
    "index": 1,
    "relevance_score": 8,
    "recency": "ìµœì‹ ",
    "reliability": "ë†’ìŒ",
    "recommend": "ì¶”ì²œ",
    "reason": "ìµœì‹  ì •ë³´ì´ë©° ì¿¼ë¦¬ì™€ ì§ì ‘ ê´€ë ¨"
  }},
  ...
]
"""
                            
                            review_result = await execute_llm_task(
                                prompt=review_prompt,
                                task_type=TaskType.ANALYSIS,
                                model_name=None,
                                system_message="You are an expert information analyst who evaluates search results for relevance, recency, and reliability."
                            )
                            
                            # LLM ê²°ê³¼ íŒŒì‹±
                            review_text = review_result.content or ""
                            try:
                                # JSON ì¶”ì¶œ
                                json_match = re.search(r'\[.*\]', review_text, re.DOTALL)
                                if json_match:
                                    json_str = json_match.group().strip()
                                    if not json_str or json_str == "[]":
                                        logger.warning(f"[{self.name}] âš ï¸ Empty JSON array in review result")
                                    else:
                                        review_data = json.loads(json_str)
                                        
                                        # ê²€í†  ê²°ê³¼ë¥¼ ê²°ê³¼ì— ì¶”ê°€
                                        for review_item in review_data:
                                            idx = review_item.get('index', 0) - 1
                                            if 0 <= idx < len(results):
                                                results[idx]['review'] = {
                                                    'relevance_score': review_item.get('relevance_score', 5),
                                                    'recency': review_item.get('recency', 'ë³´í†µ'),
                                                    'reliability': review_item.get('reliability', 'ë³´í†µ'),
                                                    'recommend': review_item.get('recommend', 'ë³´í†µ'),
                                                    'reason': review_item.get('reason', '')
                                                }
                                        
                                        # ì¶”ì²œ ê²°ê³¼ë§Œ í•„í„°ë§ (ì„ íƒì )
                                        recommended_results = [r for r in results if r.get('review', {}).get('recommend') == 'ì¶”ì²œ']
                                        if recommended_results:
                                            logger.info(f"[{self.name}] âœ… Found {len(recommended_results)} highly recommended results")
                                            # ì¶”ì²œ ê²°ê³¼ë¥¼ ìš°ì„ ì ìœ¼ë¡œ ì‚¬ìš©í•˜ë˜, ìµœì†Œ 5ê°œëŠ” ìœ ì§€
                                            if len(recommended_results) >= 5:
                                                results = recommended_results
                                            else:
                                                # ì¶”ì²œ ê²°ê³¼ + ì¼ë°˜ ê²°ê³¼ í˜¼í•©
                                                results = recommended_results + [r for r in results if r not in recommended_results][:5-len(recommended_results)]
                                        
                                        logger.info(f"[{self.name}] âœ… Reviewed {len(review_data)} search results")
                            except Exception as e:
                                logger.warning(f"[{self.name}] âš ï¸ Failed to parse review result: {e}")
                        except Exception as e:
                            logger.warning(f"[{self.name}] âš ï¸ Failed to review search results: {e}")
                    else:
                        # ëª¨ë“  ê²°ê³¼ê°€ í•„í„°ë§ëœ ê²½ìš° ìƒì„¸í•œ ì—ëŸ¬ ë©”ì‹œì§€
                        error_details = []
                        error_details.append(f"ê²€ìƒ‰ ì¿¼ë¦¬: '{query[:100]}'")
                        error_details.append(f"ì´ ê²€ìƒ‰ ê²°ê³¼: {total_processed}ê°œ")
                        error_details.append(f"í•„í„°ë§ëœ ê²°ê³¼: {filtered_count}ê°œ")
                        error_details.append(f"ìœ íš¨í•œ ê²°ê³¼: 0ê°œ")
                        
                        if filtered_reasons:
                            error_details.append("\ní•„í„°ë§ëœ ê²°ê³¼ ìƒì„¸:")
                            for fr in filtered_reasons[:3]:  # ìµœëŒ€ 3ê°œë§Œ ì—ëŸ¬ ë©”ì‹œì§€ì— í¬í•¨
                                error_details.append(f"  - ê²°ê³¼ {fr['result_index']}: '{fr['title']}' - {fr['reason']}")
                        
                        error_msg = f"ì—°êµ¬ ì‹¤í–‰ ì‹¤íŒ¨: ëª¨ë“  ê²€ìƒ‰ ê²°ê³¼ê°€ í•„í„°ë§ë˜ì—ˆìŠµë‹ˆë‹¤.\n" + "\n".join(error_details)
                        logger.error(f"[{self.name}] âŒ {error_msg}")
                        raise RuntimeError(error_msg)
                else:
                    # ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŒ - ì‹¤íŒ¨ ì²˜ë¦¬
                    logger.error(f"[{self.name}] âŒ ê²€ìƒ‰ ê²°ê³¼ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
                    logger.error(f"[{self.name}]   ê²€ìƒ‰ ì¿¼ë¦¬: '{query[:100]}'")
                    logger.error(f"[{self.name}]   ê²€ìƒ‰ ë„êµ¬: {search_result.get('source', 'unknown')}")
                    logger.error(f"[{self.name}]   ê²€ìƒ‰ ì„±ê³µ ì—¬ë¶€: {search_result.get('success', False)}")
                    if search_result.get('error'):
                        logger.error(f"[{self.name}]   ê²€ìƒ‰ ì—ëŸ¬: {search_result.get('error')}")
                    error_msg = f"ì—°êµ¬ ì‹¤í–‰ ì‹¤íŒ¨: '{query[:100]}'ì— ëŒ€í•œ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                    logger.error(f"[{self.name}] âŒ {error_msg}")
                    raise RuntimeError(error_msg)
            else:
                # ê²€ìƒ‰ ì‹¤íŒ¨ - ì—ëŸ¬ ë°˜í™˜
                logger.error(f"[{self.name}] âŒ ê²€ìƒ‰ ë„êµ¬ ì‹¤í–‰ ì‹¤íŒ¨")
                logger.error(f"[{self.name}]   ê²€ìƒ‰ ì¿¼ë¦¬: '{query[:100]}'")
                logger.error(f"[{self.name}]   ê²€ìƒ‰ ë„êµ¬: {search_result.get('source', 'unknown')}")
                logger.error(f"[{self.name}]   ê²€ìƒ‰ ì„±ê³µ ì—¬ë¶€: {search_result.get('success', False)}")
                logger.error(f"[{self.name}]   ì—ëŸ¬ ë©”ì‹œì§€: {search_result.get('error', 'Unknown error')}")
                if search_result.get('data'):
                    logger.debug(f"[{self.name}]   ì‘ë‹µ ë°ì´í„° íƒ€ì…: {type(search_result.get('data'))}")
                    logger.debug(f"[{self.name}]   ì‘ë‹µ ë°ì´í„° ìƒ˜í”Œ: {str(search_result.get('data'))[:200]}")
                error_msg = f"ì—°êµ¬ ì‹¤í–‰ ì‹¤íŒ¨: ê²€ìƒ‰ ë„êµ¬ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. {search_result.get('error', 'Unknown error')}"
                logger.error(f"[{self.name}] âŒ {error_msg}")
                raise RuntimeError(error_msg)
                
        except Exception as e:
            # ì‹¤ì œ ì˜¤ë¥˜ ë°œìƒ - ì‹¤íŒ¨ ì²˜ë¦¬
            import traceback
            error_type = type(e).__name__
            error_msg = f"ì—°êµ¬ ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}"
            logger.error(f"[{self.name}] âŒ ì˜ˆì™¸ ë°œìƒ: {error_type}")
            logger.error(f"[{self.name}]   ì—ëŸ¬ ë©”ì‹œì§€: {error_msg}")
            logger.error(f"[{self.name}]   ê²€ìƒ‰ ì¿¼ë¦¬: '{query[:100] if 'query' in locals() else 'N/A'}'")
            logger.debug(f"[{self.name}]   Traceback:\n{traceback.format_exc()}")
            
            # ì‹¤íŒ¨ ìƒíƒœ ê¸°ë¡
            state['research_results'] = []
            state['current_agent'] = self.name
            state['error'] = error_msg
            state['research_failed'] = True
            
            # ë©”ëª¨ë¦¬ì— ì‹¤íŒ¨ ì •ë³´ ê¸°ë¡
            memory.write(
                key=f"execution_error_{state['session_id']}",
                value=error_msg,
                scope=MemoryScope.SESSION,
                session_id=state['session_id'],
                agent_id=self.name
            )
            
            # ì‹¤íŒ¨ ìƒíƒœ ë°˜í™˜ (ë”ë¯¸ ë°ì´í„° ì—†ì´)
            return state
        
        # Council í™œì„±í™” í™•ì¸ ë° ì ìš© (ì¤‘ìš”í•œ ì •ë³´ ìˆ˜ì§‘ ì‹œ)
        use_council = state.get('use_council', None)  # ìˆ˜ë™ í™œì„±í™” ì˜µì…˜
        if use_council is None:
            # ìë™ í™œì„±í™” íŒë‹¨
            from src.core.council_activator import get_council_activator
            activator = get_council_activator()
            
            # ì¤‘ìš”í•œ ì‚¬ì‹¤ í™•ì¸ì´ í•„ìš”í•œì§€ íŒë‹¨
            context = {
                'results_count': len(results),
                'has_controversial_topic': any(
                    keyword in state['user_query'].lower() 
                    for keyword in ['debate', 'controversy', 'disagreement', 'ë…¼ìŸ', 'ì˜ê²¬']
                ),
                'high_stakes': any(
                    keyword in state['user_query'].lower()
                    for keyword in ['critical', 'important', 'decision', 'ì¤‘ìš”í•œ', 'ê²°ì •']
                )
            }
            
            activation_decision = activator.should_activate(
                process_type='execution',
                query=state['user_query'],
                context=context
            )
            use_council = activation_decision.should_activate
            if use_council:
                logger.info(f"[{self.name}] ğŸ›ï¸ Council auto-activated: {activation_decision.reason}")
        
        # Council ì ìš© (í™œì„±í™”ëœ ê²½ìš°)
        if use_council and results:
            try:
                from src.core.llm_council import run_full_council
                logger.info(f"[{self.name}] ğŸ›ï¸ Running Council verification for research results...")
                
                # ê²°ê³¼ ìš”ì•½ ìƒì„±
                results_summary = "\n\n".join([
                    f"Result {i+1}:\nTitle: {r.get('title', 'N/A')}\nURL: {r.get('url', 'N/A')}\nSnippet: {r.get('snippet', 'N/A')[:200]}"
                    for i, r in enumerate(results[:10])  # ìµœëŒ€ 10ê°œë§Œ ê²€í† 
                ])
                
                council_query = f"""Verify the accuracy and reliability of the following research results. Identify any inconsistencies, missing information, or potential issues.

Research Query: {state['user_query']}

Research Results:
{results_summary}

Provide a verification report with:
1. Accuracy assessment
2. Missing information
3. Recommendations for improvement"""
                
                stage1_results, stage2_results, stage3_result, metadata = await run_full_council(
                    council_query
                )
                
                # Council ê²€ì¦ ê²°ê³¼ë¥¼ ê²°ê³¼ì— ì¶”ê°€
                verification_report = stage3_result.get('response', '')
                logger.info(f"[{self.name}] âœ… Council verification completed.")
                logger.info(f"[{self.name}] Council aggregate rankings: {metadata.get('aggregate_rankings', [])}")
                
                # Council ë©”íƒ€ë°ì´í„°ë¥¼ stateì— ì €ì¥
                if 'council_metadata' not in state:
                    state['council_metadata'] = {}
                state['council_metadata']['execution'] = {
                    'stage1_results': stage1_results,
                    'stage2_results': stage2_results,
                    'stage3_result': stage3_result,
                    'metadata': metadata,
                    'verification_report': verification_report
                }
                
                # ê²€ì¦ ë¦¬í¬íŠ¸ë¥¼ ê²°ê³¼ì— ì¶”ê°€
                results.append({
                    'title': 'Council Verification Report',
                    'url': '',
                    'snippet': verification_report,
                    'source': 'council',
                    'council_verified': True
                })
            except Exception as e:
                logger.warning(f"[{self.name}] Council verification failed: {e}. Using original results.")
                # Council ì‹¤íŒ¨ ì‹œ ì›ë³¸ ê²°ê³¼ ì‚¬ìš© (fallback ì œê±° - ëª…í™•í•œ ë¡œê¹…ë§Œ)
        
        # Executor ê²°ê³¼ë¥¼ SharedResultsManagerì— ê³µìœ  (ë…¼ë°•ì„ ìœ„í•´)
        executor_discussions = []
        if self.context.shared_results_manager and results:
            for i, result in enumerate(results[:10]):  # ìµœëŒ€ 10ê°œ ê²°ê³¼ì— ëŒ€í•´ ë…¼ë°•
                result_id = await self.context.shared_results_manager.share_result(
                    task_id=f"executor_result_{i}",
                    agent_id=self.context.agent_id,
                    result=result,
                    metadata={"executor_result_index": i, "query": state['user_query']},
                    confidence=result.get('confidence', 0.8) if isinstance(result, dict) else 0.8
                )
                logger.info(f"[{self.name}] ğŸ”— Shared executor result {i} for debate (result_id: {result_id[:8]}...)")
            
            # ë‹¤ë¥¸ Executorë“¤ì˜ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸° (ë…¼ë°•ì„ ìœ„í•´)
            other_executor_results = await self.context.shared_results_manager.get_shared_results(
                exclude_agent_id=self.context.agent_id
            )
            
            if other_executor_results:
                logger.info(f"[{self.name}] ğŸ’¬ Found {len(other_executor_results)} other executor results for debate")
                # ë…¼ë°•ì€ Verifierì™€ Evaluatorì—ì„œ ìˆ˜í–‰í•˜ë„ë¡ í•¨ (ì—¬ê¸°ì„œëŠ” ê²°ê³¼ë§Œ ê³µìœ )
        
        # ì„±ê³µì ìœ¼ë¡œ ê²°ê³¼ ìˆ˜ì§‘ëœ ê²½ìš°
        state['research_results'] = results  # ë¦¬ìŠ¤íŠ¸ë¡œ ì €ì¥ (ë®ì–´ì“°ê¸°)
        state['current_agent'] = self.name
        state['research_failed'] = False
        
        # ë…¼ë°• ê²°ê³¼ ì´ˆê¸°í™” (Verifierì™€ Evaluatorê°€ ì±„ìš¸ ê²ƒ)
        if 'agent_debates' not in state:
            state['agent_debates'] = {}
        
        logger.info(f"[{self.name}] âœ… Research execution completed: {len(results)} results")
        
        # Write to shared memory (êµ¬ì¡°í™”ëœ í˜•ì‹)
        memory.write(
            key=f"research_results_{state['session_id']}",
            value=results,
            scope=MemoryScope.SESSION,
            session_id=state['session_id'],
            agent_id=self.name
        )
        
        logger.info(f"[{self.name}] Results saved to shared memory")
        logger.info(f"=" * 80)
        
        return state


class VerifierAgent:
    """Verifier agent - verifies research results (Skills-based)."""
    
    def __init__(self, context: AgentContext, skill: Optional[Skill] = None):
        self.context = context
        self.name = "verifier"
        self.available_tools: list = []  # MCP ìë™ í• ë‹¹ ë„êµ¬
        self.tool_infos: list = []  # ë„êµ¬ ë©”íƒ€ë°ì´í„°
        self.skill = skill
        
        # Skillì´ ì—†ìœ¼ë©´ ë¡œë“œ ì‹œë„
        if self.skill is None:
            skill_manager = get_skill_manager()
            self.skill = skill_manager.load_skill("evaluator")
        
        # Skill instruction ì‚¬ìš©
        if self.skill:
            self.instruction = self.skill.instructions
        else:
            self.instruction = "You are a verification agent."
    
    async def execute(self, state: AgentState) -> AgentState:
        """Verify research results with LLM-based verification."""
        logger.info(f"=" * 80)
        logger.info(f"[{self.name.upper()}] Starting verification")
        logger.info(f"=" * 80)
        
        # ì—°êµ¬ ì‹¤íŒ¨ í™•ì¸
        if state.get('research_failed'):
            logger.error(f"[{self.name}] âŒ Research execution failed, skipping verification")
            state['verified_results'] = []
            state['verification_failed'] = True
            state['current_agent'] = self.name
            return state
        
        memory = self.context.shared_memory
        
        # Read results from state or shared memory
        results = state.get('research_results', [])
        if not results:
            results = memory.read(
                key=f"research_results_{state['session_id']}",
                scope=MemoryScope.SESSION,
                session_id=state['session_id']
            ) or []
        
        # SharedResultsManagerì—ì„œ ë‹¤ë¥¸ Executorì˜ ê²°ê³¼ë„ ê°€ì ¸ì˜¤ê¸°
        if self.context.shared_results_manager:
            shared_results = await self.context.shared_results_manager.get_shared_results(
                exclude_agent_id=self.name
            )
            logger.info(f"[{self.name}] ğŸ” Found {len(shared_results)} shared results from other agents")

            # ê³µìœ ëœ ê²°ê³¼ë¥¼ resultsì— ì¶”ê°€
            shared_data_count = 0
            for shared_result in shared_results:
                if isinstance(shared_result.result, dict) and shared_result.result.get('data'):
                    # ê²€ìƒ‰ ê²°ê³¼ì—ì„œ êµ¬ì¡°í™”ëœ ë°ì´í„° ì¶”ì¶œ
                    data = shared_result.result.get('data', {})
                    if isinstance(data, dict):
                        shared_search_results = data.get('results', data.get('items', []))
                        if isinstance(shared_search_results, list):
                            results.extend(shared_search_results)
                            shared_data_count += len(shared_search_results)
                    elif isinstance(data, list):
                        results.extend(data)
                        shared_data_count += len(data)

            logger.info(f"[{self.name}] ğŸ“¥ Retrieved {shared_data_count} additional results from {len(shared_results)} shared agent results")
            logger.info(f"[{self.name}] ğŸ¤ Agent communication: Retrieved results from agents: {[r.agent_id for r in shared_results]}")
        
        logger.info(f"[{self.name}] Found {len(results)} results to verify (including shared results)")
        
        if not results or len(results) == 0:
            # ê²€ì¦í•  ê²°ê³¼ê°€ ì—†ëŠ” ì´ìœ  ìƒì„¸ ë¶„ì„
            logger.error(f"[{self.name}] âŒ ê²€ì¦í•  ì—°êµ¬ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            
            # stateì—ì„œ ê²°ê³¼ ì¶”ì 
            execution_results = state.get('execution_results', [])
            compression_results = state.get('compression_results', [])
            shared_results = state.get('shared_results', [])
            
            logger.error(f"[{self.name}] ğŸ“‹ ê²°ê³¼ ì¶”ì :")
            logger.error(f"[{self.name}]   - execution_results: {len(execution_results) if isinstance(execution_results, list) else 0}ê°œ")
            logger.error(f"[{self.name}]   - compression_results: {len(compression_results) if isinstance(compression_results, list) else 0}ê°œ")
            logger.error(f"[{self.name}]   - shared_results: {len(shared_results) if isinstance(shared_results, list) else 0}ê°œ")
            logger.error(f"[{self.name}]   - ê²€ì¦ì— ì „ë‹¬ëœ results: {len(results) if isinstance(results, list) else 0}ê°œ")
            
            # execution_results ìƒì„¸ ë¶„ì„
            if execution_results:
                successful_executions = [er for er in execution_results if er.get('success', False)]
                failed_executions = [er for er in execution_results if not er.get('success', False)]
                logger.error(f"[{self.name}]   - ì„±ê³µí•œ ì‹¤í–‰: {len(successful_executions)}ê°œ")
                logger.error(f"[{self.name}]   - ì‹¤íŒ¨í•œ ì‹¤í–‰: {len(failed_executions)}ê°œ")
                
                if failed_executions:
                    logger.error(f"[{self.name}]   ğŸ“ ì‹¤íŒ¨í•œ ì‹¤í–‰ ìƒì„¸:")
                    for i, fe in enumerate(failed_executions[:3], 1):  # ìµœëŒ€ 3ê°œë§Œ í‘œì‹œ
                        error = fe.get('error', 'Unknown error')
                        logger.error(f"[{self.name}]     {i}. {str(error)[:100]}")
            
            # ê²€ìƒ‰ ê²°ê³¼ê°€ ìˆëŠ”ì§€ í™•ì¸
            search_results_found = False
            for er in execution_results if isinstance(execution_results, list) else []:
                if isinstance(er, dict):
                    data = er.get('data', {})
                    if isinstance(data, dict):
                        results_data = data.get('results', data.get('items', []))
                        if results_data and len(results_data) > 0:
                            search_results_found = True
                            logger.error(f"[{self.name}]   âš ï¸ ê²€ìƒ‰ ê²°ê³¼ëŠ” ìˆì§€ë§Œ ê²€ì¦ ë‹¨ê³„ì— ì „ë‹¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!")
                            break
            
            if not search_results_found:
                logger.error(f"[{self.name}]   âš ï¸ ê²€ìƒ‰ ë‹¨ê³„ì—ì„œ ê²°ê³¼ë¥¼ ì–»ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ExecutorAgentì˜ ê²€ìƒ‰ ì‹¤íŒ¨ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
            
            error_msg = "ê²€ì¦ ì‹¤íŒ¨: ê²€ì¦í•  ì—°êµ¬ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."
            logger.error(f"[{self.name}] âŒ {error_msg}")
            state['verified_results'] = []
            state['verification_failed'] = True
            state['error'] = error_msg
            state['current_agent'] = self.name
            return state
        
        # LLMì„ ì‚¬ìš©í•œ ì‹¤ì œ ê²€ì¦
        from src.core.llm_manager import execute_llm_task, TaskType
        
        verified = []
        rejected_reasons = []  # ê²€ì¦ ì‹¤íŒ¨ ì›ì¸ ì¶”ì 
        skipped_count = 0
        verification_errors = []
        
        user_query = state.get('user_query', '')
        logger.info(f"[{self.name}] ğŸ” Starting verification of {len(results)} results for query: '{user_query}'")
        
        for i, result in enumerate(results, 1):
            if isinstance(result, dict):
                # ë‹¤ì–‘í•œ í‚¤ì—ì„œ title, snippet, url ì¶”ì¶œ ì‹œë„
                title = result.get('title') or result.get('name') or result.get('Title') or result.get('headline') or ''
                snippet = result.get('snippet') or result.get('content') or result.get('summary') or result.get('description') or result.get('abstract') or ''
                url = result.get('url') or result.get('link') or result.get('href') or result.get('URL') or ''
                
                # titleì´ ë¹„ì–´ìˆê±°ë‚˜ "Search Results" ê°™ì€ ë©”íƒ€ë°ì´í„°ì¸ ê²½ìš° ìŠ¤í‚µ
                if not title or len(title.strip()) < 3:
                    skipped_count += 1
                    logger.debug(f"[{self.name}] â­ï¸ Skipping result {i}: empty or invalid title")
                    continue
                
                # "Search Results", "Results", "Error" ê°™ì€ ë©”íƒ€ë°ì´í„° ì œì™¸
                title_lower = title.lower().strip()
                if title_lower in ['search results', 'results', 'error', 'no results', 'no title']:
                    skipped_count += 1
                    logger.debug(f"[{self.name}] â­ï¸ Skipping result {i}: metadata title '{title}'")
                    continue
                
                # snippetì´ ë¹„ì–´ìˆê³  urlë„ ì—†ëŠ” ê²½ìš° ìŠ¤í‚µ
                if not snippet and not url:
                    skipped_count += 1
                    logger.debug(f"[{self.name}] â­ï¸ Skipping result {i}: no content or URL")
                    continue

                # snippet ë‚´ìš©ìœ¼ë¡œ ìœ íš¨í•˜ì§€ ì•Šì€ ê²€ìƒ‰ ê²°ê³¼ í•„í„°ë§
                invalid_indicators = [
                    "no results were found", "bot detection",
                    "no results", "not found", "try again",
                    "unable to", "error occurred", "no matches"
                ]
                snippet_lower = snippet.lower() if snippet else ""
                if any(indicator in snippet_lower for indicator in invalid_indicators):
                    skipped_count += 1
                    logger.debug(f"[{self.name}] â­ï¸ Skipping result {i}: invalid snippet content (contains error message)")
                    continue
                
                # full_content ìš°ì„  ì‚¬ìš©, ì—†ìœ¼ë©´ snippet ì‚¬ìš©
                full_content = result.get('full_content', '')
                verification_content = full_content[:2000] if full_content else (snippet[:800] if snippet else 'ë‚´ìš© ì—†ìŒ')
                
                # ë‚ ì§œ ì •ë³´ ì¶”ê°€
                published_date = result.get('published_date', '')
                date_info = ""
                if published_date:
                    try:
                        date_obj = datetime.fromisoformat(published_date.replace('Z', '+00:00'))
                        date_info = f"\n- ë°œí–‰ì¼: {date_obj.strftime('%Y-%m-%d')}"
                    except:
                        date_info = f"\n- ë°œí–‰ì¼: {published_date[:10]}"
                
                # LLMìœ¼ë¡œ ê²€ì¦ (ì ê²€ ë° ì œì–¸ ì¤‘ì‹¬) - ê°•í™”ëœ ë²„ì „
                # Current time for verification context
                current_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S (%A)')
                
                verification_prompt = f"""ë‹¤ìŒ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì—„ê²©í•˜ê²Œ ì ê²€í•˜ê³  ì œì–¸í•˜ì„¸ìš”:
                
í˜„ì¬ ì‹œê°: {current_time}

**ê²€ìƒ‰ ê²°ê³¼ ì •ë³´:**
- ì œëª©: {title}
- ë‚´ìš©: {verification_content}
- URL: {url if url else 'URL ì—†ìŒ'}{date_info}

**ì›ë˜ ì¿¼ë¦¬:** {user_query}

**ì ê²€ ë° ì œì–¸ ì‘ì—…:**

ë‹¹ì‹ ì˜ ì—­í• ì€ ìë£Œë¥¼ "ì–µì œ"í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, ìë£Œë¥¼ **ì—„ê²©í•˜ê²Œ ì ê²€í•˜ê³  ì œì–¸**í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

1. **ê´€ë ¨ì„± ì ê²€** (ì—„ê²©):
   - ì´ ìë£Œê°€ ì¿¼ë¦¬ì™€ ì§ì ‘ì ìœ¼ë¡œ ê´€ë ¨ì´ ìˆëŠ”ê°€?
   - ê´€ë ¨ì„±ì´ ë‚®ë‹¤ë©´ ì–´ë–¤ ë¶€ë¶„ì´ ê´€ë ¨ì´ ìˆëŠ”ê°€? (êµ¬ì²´ì ìœ¼ë¡œ ëª…ì‹œ)
   - ë°°ê²½ ì •ë³´ë¡œë§Œ ìœ ìš©í•œê°€? (ê·¸ë ‡ë‹¤ë©´ ë‚®ì€ ê´€ë ¨ì„± ì ìˆ˜ ë¶€ì—¬)

2. **í’ˆì§ˆ ì ê²€** (ì—„ê²©):
   - ìë£Œì˜ ì‹ ë¢°ì„±ì€ ì–´ë–¤ê°€? (ì¶œì²˜, ì‘ì„±ì, ë°œí–‰ê¸°ê´€ ê³ ë ¤)
   - ì •ë³´ì˜ ì •í™•ì„±ì— ì˜¤ë¥˜ê°€ ìˆëŠ”ê°€? (êµ¬ì²´ì ì¸ ì˜¤ë¥˜ ëª…ì‹œ)
   - ì¶œì²˜ê°€ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ”ê°€? (ë„ë©”ì¸, ê¸°ê´€, ì‘ì„±ì ê²€ì¦)
   - í†µê³„ë‚˜ ìˆ«ìê°€ ìˆë‹¤ë©´ ì¶œì²˜ê°€ ëª…ì‹œë˜ì–´ ìˆëŠ”ê°€?

3. **ê·¼ê±° ë° ì¦ê±° ì ê²€** (ìƒˆë¡œ ì¶”ê°€):
   - ì£¼ì¥ì— ëŒ€í•œ ê·¼ê±°ê°€ ì œì‹œë˜ì–´ ìˆëŠ”ê°€?
   - í†µê³„ë‚˜ ìˆ«ìëŠ” ì¶œì²˜ê°€ ìˆëŠ”ê°€?
   - ë‚ ì§œë‚˜ ì‚¬ì‹¤ì€ ê²€ì¦ ê°€ëŠ¥í•œê°€?
   - ë¶ˆí™•ì‹¤í•œ ì •ë³´ëŠ” ëª…ì‹œë˜ì–´ ìˆëŠ”ê°€?

4. **ì œì–¸**:
   - ì´ ìë£Œë¥¼ ì‚¬ìš©í•  ë•Œ ì£¼ì˜í•  ì ì€? (êµ¬ì²´ì ìœ¼ë¡œ)
   - ê°œì„ ì´ í•„ìš”í•œ ë¶€ë¶„ì€? (êµ¬ì²´ì ìœ¼ë¡œ)
   - ë‹¤ë¥¸ ìë£Œì™€ í•¨ê»˜ ì‚¬ìš©í•˜ë©´ ë” ì¢‹ì„ ì •ë³´ì¸ê°€?
   - ì¶”ê°€ ì¡°ì‚¬ê°€ í•„ìš”í•œ ë¶€ë¶„ì€? (êµ¬ì²´ì ìœ¼ë¡œ)

**ì¤‘ìš” ì›ì¹™:**
- **í° ì˜¤ë¥˜ë§Œ ì¡°ì •**: ëª…ë°±í•œ ì˜¤ë¥˜ë‚˜ ì™„ì „íˆ ë¬´ê´€í•œ ìë£Œë§Œ ê±°ë¶€
- **ì‘ì€ ë¬¸ì œëŠ” ì œì–¸ê³¼ í•¨ê»˜ í†µê³¼**: ê´€ë ¨ì„±ì´ ì•½ê°„ ë‚®ê±°ë‚˜ í’ˆì§ˆì´ ì•½ê°„ ë‚®ì•„ë„ ì œì–¸ê³¼ í•¨ê»˜ í¬í•¨
- **ì–µì œë³´ë‹¤ëŠ” ìœ ë„**: ìë£Œë¥¼ ê±°ë¶€í•˜ê¸°ë³´ë‹¤ëŠ” ì˜¬ë°”ë¥¸ ë°©í–¥ìœ¼ë¡œ ì‚¬ìš©í•˜ë„ë¡ ì œì–¸
- **ê²€ìƒ‰ ê²°ê³¼ì˜ íŠ¹ì„± ì´í•´**: LLMì´ ëª¨ë¥´ëŠ” ìƒíƒœì—ì„œ ì°¾ì•„ë³¸ ê²°ê³¼ì´ë¯€ë¡œ, ì™„ë²½í•˜ì§€ ì•Šì•„ë„ ê´€ë ¨ ì •ë³´ëŠ” í¬í•¨
- **ê·¼ê±° ì—†ëŠ” í™•ì‹  ê¸ˆì§€**: ë¶ˆí™•ì‹¤í•œ ì •ë³´ëŠ” ë°˜ë“œì‹œ ë¶ˆí™•ì‹¤ì„± ëª…ì‹œ, ê·¼ê±° ì—†ëŠ” ì£¼ì¥ì€ ë‚®ì€ ì‹ ë¢°ë„ ë¶€ì—¬

**ì‘ë‹µ í˜•ì‹ (ë°˜ë“œì‹œ ì´ í˜•ì‹ìœ¼ë¡œ ì‘ì„±):**
```
STATUS: VERIFIED ë˜ëŠ” REJECTED
RELEVANCE_SCORE: 1-10 (ê´€ë ¨ì„± ì ìˆ˜, ì—„ê²©í•˜ê²Œ í‰ê°€)
QUALITY_SCORE: 1-10 (í’ˆì§ˆ ì ìˆ˜, ì—„ê²©í•˜ê²Œ í‰ê°€)
EVIDENCE_SCORE: 1-10 (ê·¼ê±°/ì¦ê±° ì ìˆ˜, ìƒˆë¡œ ì¶”ê°€)
CONFIDENCE_LEVEL: HIGH/MEDIUM/LOW (ì‹ ë¢°ë„ ìˆ˜ì¤€)
UNCERTAINTY_ISSUES: ë¶ˆí™•ì‹¤í•œ ë¶€ë¶„ ëª…ì‹œ (ì—†ìœ¼ë©´ "ì—†ìŒ")
ISSUES: ë°œê²¬ëœ ë¬¸ì œì  (êµ¬ì²´ì ìœ¼ë¡œ, ì—†ìœ¼ë©´ "ì—†ìŒ")
RECOMMENDATIONS: ì‚¬ìš© ì‹œ ì œì–¸ì‚¬í•­ (êµ¬ì²´ì ìœ¼ë¡œ, ì—†ìœ¼ë©´ "ì—†ìŒ")
ADDITIONAL_RESEARCH_NEEDED: ì¶”ê°€ ì¡°ì‚¬ í•„ìš”í•œ ë¶€ë¶„ (êµ¬ì²´ì ìœ¼ë¡œ, ì—†ìœ¼ë©´ "ì—†ìŒ")
REASON: ìµœì¢… íŒë‹¨ ì´ìœ  (í•œ ì¤„, êµ¬ì²´ì ìœ¼ë¡œ)
```

âš ï¸ **ì ˆëŒ€ í•˜ì§€ ë§ ê²ƒ:**
- "y y y y..." ê°™ì€ ë°˜ë³µ ë¬¸ì ì‚¬ìš© ê¸ˆì§€
- ë‹¨ìˆœíˆ "REJECTED"ë§Œ ì‘ì„±í•˜ì§€ ë§ê³  ë°˜ë“œì‹œ ìœ„ í˜•ì‹ ì¤€ìˆ˜
- ë„ˆë¬´ ì—„ê²©í•˜ê²Œ íŒë‹¨í•˜ì§€ ë§ ê²ƒ
- **ê·¼ê±° ì—†ëŠ” í™•ì‹  í‘œí˜„ ê¸ˆì§€**: ë¶ˆí™•ì‹¤í•œ ì •ë³´ëŠ” ë°˜ë“œì‹œ ë¶ˆí™•ì‹¤ì„± ëª…ì‹œ
- **ëª¨í˜¸í•œ í‘œí˜„ ê¸ˆì§€**: ëª¨ë“  ì ìˆ˜ì™€ íŒë‹¨ì€ êµ¬ì²´ì ì¸ ê·¼ê±°ì™€ í•¨ê»˜ ì œê³µ"""
                
                try:
                    logger.info(f"[{self.name}] ğŸ” Verifying result {i}/{len(results)}: '{title[:60]}...'")
                    
                    # Source Validation ìˆ˜í–‰
                    source_validation_result = None
                    if url:
                        try:
                            from src.verification.source_validator import SourceValidator
                            source_validator = SourceValidator()
                            source_validation_result = await source_validator.validate_source(url, verification_content)
                            logger.info(f"[{self.name}] ğŸ“Š Source validation: {source_validation_result.overall_score:.2f} (domain: {source_validation_result.domain_type.value})")
                        except Exception as e:
                            logger.warning(f"[{self.name}] Source validation failed: {e}")
                    
                    # Fact-checking ìˆ˜í–‰ (ì£¼ìš” ì£¼ì¥ì´ ìˆëŠ” ê²½ìš°)
                    fact_check_result = None
                    if verification_content and len(verification_content) > 100:
                        try:
                            from src.verification.fact_checker import FactChecker
                            fact_checker = FactChecker()
                            # ì£¼ìš” ì£¼ì¥ ì¶”ì¶œ (ìˆ«ì, ë‚ ì§œ, í†µê³„ ë“±)
                            claims = []
                            # ìˆ«ì íŒ¨í„´ ì°¾ê¸°
                            numbers = re.findall(r'\d+[.,]\d+[ì¡°ì–µë§Œì›%]|\d+[ì¡°ì–µë§Œì›%]', verification_content)
                            if numbers:
                                claims.extend([f"ìˆ«ì/í†µê³„: {num}" for num in numbers[:3]])
                            # ë‚ ì§œ íŒ¨í„´ ì°¾ê¸°
                            dates = re.findall(r'\d{4}ë…„|\d{4}-\d{2}-\d{2}', verification_content)
                            if dates:
                                claims.extend([f"ë‚ ì§œ: {date}" for date in dates[:2]])
                            
                            if claims:
                                fact_check_result = await fact_checker.verify_fact(
                                    fact_text=verification_content[:500],
                                    sources=[result]
                                )
                                logger.info(f"[{self.name}] âœ… Fact-checking: {fact_check_result.fact_status.value} (confidence: {fact_check_result.confidence_score:.2f})")
                        except Exception as e:
                            logger.warning(f"[{self.name}] Fact-checking failed: {e}")
                    
                    # Cross-verification ìˆ˜í–‰ (ë‹¤ë¥¸ ê²°ê³¼ì™€ ë¹„êµ)
                    cross_verification_score = None
                    if len(results) > 1:
                        try:
                            # ê°™ì€ ì •ë³´ê°€ ë‹¤ë¥¸ ì¶œì²˜ì—ì„œë„ í™•ì¸ë˜ëŠ”ì§€ ì²´í¬
                            similar_results = []
                            for other_result in results:
                                if other_result != result and isinstance(other_result, dict):
                                    other_title = other_result.get('title', '')
                                    other_snippet = other_result.get('snippet', '')
                                    # ìœ ì‚¬ë„ ì²´í¬ (ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜)
                                    common_keywords = set(title.lower().split()) & set(other_title.lower().split())
                                    if len(common_keywords) >= 2:
                                        similar_results.append(other_result)
                            
                            if similar_results:
                                cross_verification_score = len(similar_results) / len(results)
                                logger.info(f"[{self.name}] ğŸ”„ Cross-verification: {len(similar_results)} similar results found (score: {cross_verification_score:.2f})")
                        except Exception as e:
                            logger.warning(f"[{self.name}] Cross-verification failed: {e}")
                    
                    # ê²€ì¦ í”„ë¡¬í”„íŠ¸ì— ì¶”ê°€ ì •ë³´ í¬í•¨
                    enhanced_prompt = verification_prompt
                    if source_validation_result:
                        enhanced_prompt += f"\n\n**ì¶œì²˜ ì‹ ë¢°ë„ ì •ë³´:**\n- ë„ë©”ì¸ ì‹ ë¢°ë„: {source_validation_result.domain_trust:.2f}\n- ì „ì²´ ì‹ ë¢°ë„ ì ìˆ˜: {source_validation_result.overall_score:.2f}\n- ë„ë©”ì¸ íƒ€ì…: {source_validation_result.domain_type.value}"
                    if fact_check_result:
                        enhanced_prompt += f"\n\n**Fact-checking ê²°ê³¼:**\n- ìƒíƒœ: {fact_check_result.fact_status.value}\n- ì‹ ë¢°ë„: {fact_check_result.confidence_score:.2f}"
                    if cross_verification_score is not None:
                        enhanced_prompt += f"\n\n**Cross-verification:**\n- ìœ ì‚¬ ê²°ê³¼ ë°œê²¬: {cross_verification_score:.2f}"
                    
                    verification_result = await execute_llm_task(
                        prompt=enhanced_prompt,
                        task_type=TaskType.VERIFICATION,
                        model_name=None,
                        system_message="You are a verification agent that checks and provides recommendations for research materials. Your role is to guide proper use of materials, not to overly suppress them. Only reject materials with major errors or complete irrelevance. For minor issues, provide recommendations and include the material."
                    )
                    
                    verification_text = verification_result.content or "UNKNOWN"
                    
                    # ì´ìƒí•œ ë°˜ë³µ íŒ¨í„´ ê°ì§€ ë° í•„í„°ë§
                    if len(set(verification_text.strip().split())) < 3 or verification_text.count('y') > 10 or verification_text.count('Y') > 10:
                        logger.warning(f"[{self.name}] âš ï¸ Detected abnormal response pattern, using fallback verification")
                        # ì´ìƒí•œ ì‘ë‹µì´ë©´ ê´€ë ¨ì„± ê¸°ë°˜ìœ¼ë¡œ íŒë‹¨
                        verification_text = f"STATUS: VERIFIED\nREASON: Abnormal LLM response detected, using content-based verification"
                    
                    # êµ¬ì¡°í™”ëœ ì‘ë‹µ íŒŒì‹±
                    verification_upper = verification_text.upper().strip()
                    
                    # STATUS í•„ë“œ ì¶”ì¶œ
                    status_match = None
                    if "STATUS:" in verification_upper:
                        status_line = [line for line in verification_upper.split('\n') if 'STATUS:' in line]
                        if status_line:
                            status_match = status_line[0]
                    elif "VERIFIED" in verification_upper:
                        status_match = "VERIFIED"
                    elif "REJECTED" in verification_upper:
                        status_match = "REJECTED"
                    
                    # RELEVANCE_SCORE ì¶”ì¶œ (ê´€ë ¨ì„± ì ìˆ˜)
                    relevance_score = 5  # ê¸°ë³¸ê°’
                    if "RELEVANCE_SCORE:" in verification_upper:
                        score_lines = [line for line in verification_upper.split('\n') if 'RELEVANCE_SCORE:' in line]
                        if score_lines:
                            try:
                                score_str = score_lines[0].split('RELEVANCE_SCORE:')[1].strip().split()[0]
                                relevance_score = int(float(score_str))
                            except:
                                pass
                    
                    # QUALITY_SCORE ì¶”ì¶œ (í’ˆì§ˆ ì ìˆ˜)
                    quality_score = 5  # ê¸°ë³¸ê°’
                    if "QUALITY_SCORE:" in verification_upper:
                        score_lines = [line for line in verification_upper.split('\n') if 'QUALITY_SCORE:' in line]
                        if score_lines:
                            try:
                                score_str = score_lines[0].split('QUALITY_SCORE:')[1].strip().split()[0]
                                quality_score = int(float(score_str))
                            except:
                                pass
                    
                    # EVIDENCE_SCORE ì¶”ì¶œ (ê·¼ê±°/ì¦ê±° ì ìˆ˜) - ìƒˆë¡œ ì¶”ê°€
                    evidence_score = 5  # ê¸°ë³¸ê°’
                    if "EVIDENCE_SCORE:" in verification_upper:
                        score_lines = [line for line in verification_upper.split('\n') if 'EVIDENCE_SCORE:' in line]
                        if score_lines:
                            try:
                                score_str = score_lines[0].split('EVIDENCE_SCORE:')[1].strip().split()[0]
                                evidence_score = int(float(score_str))
                            except:
                                pass
                    
                    # CONFIDENCE_LEVEL ì¶”ì¶œ (ì‹ ë¢°ë„ ìˆ˜ì¤€) - ìƒˆë¡œ ì¶”ê°€
                    confidence_level = "MEDIUM"  # ê¸°ë³¸ê°’
                    if "CONFIDENCE_LEVEL:" in verification_upper:
                        level_lines = [line for line in verification_upper.split('\n') if 'CONFIDENCE_LEVEL:' in line]
                        if level_lines:
                            level_str = level_lines[0].split('CONFIDENCE_LEVEL:')[1].strip().split()[0]
                            if level_str in ["HIGH", "MEDIUM", "LOW"]:
                                confidence_level = level_str
                    
                    # UNCERTAINTY_ISSUES ì¶”ì¶œ (ë¶ˆí™•ì‹¤ì„± ì´ìŠˆ) - ìƒˆë¡œ ì¶”ê°€
                    uncertainty_issues = "ì—†ìŒ"
                    if "UNCERTAINTY_ISSUES:" in verification_text:
                        issue_lines = [line for line in verification_text.split('\n') if 'UNCERTAINTY_ISSUES:' in line]
                        if issue_lines:
                            issue_text = issue_lines[0].split('UNCERTAINTY_ISSUES:')[1].strip()
                            if issue_text and issue_text != "ì—†ìŒ":
                                uncertainty_issues = issue_text[:300]
                    
                    # ADDITIONAL_RESEARCH_NEEDED ì¶”ì¶œ (ì¶”ê°€ ì¡°ì‚¬ í•„ìš”) - ìƒˆë¡œ ì¶”ê°€
                    additional_research_needed = "ì—†ìŒ"
                    if "ADDITIONAL_RESEARCH_NEEDED:" in verification_text:
                        research_lines = [line for line in verification_text.split('\n') if 'ADDITIONAL_RESEARCH_NEEDED:' in line]
                        if research_lines:
                            research_text = research_lines[0].split('ADDITIONAL_RESEARCH_NEEDED:')[1].strip()
                            if research_text and research_text != "ì—†ìŒ":
                                additional_research_needed = research_text[:300]
                    
                    # ì¢…í•© ì‹ ë¢°ë„ ê³„ì‚° (ë‹¤ë‹¨ê³„ ê²€ì¦)
                    # Self-verification: evidence_scoreì™€ quality_scoreì˜ í‰ê· 
                    self_verification_score = (evidence_score + quality_score) / 2.0 / 10.0
                    # Cross-verification: cross_verification_score ì‚¬ìš© (ì´ë¯¸ ê³„ì‚°ë¨)
                    cross_verification_score_normalized = cross_verification_score if cross_verification_score is not None else 0.5
                    # External verification: source_validationê³¼ fact_check ì‚¬ìš©
                    external_verification_score = 0.5  # ê¸°ë³¸ê°’
                    if source_validation_result:
                        external_verification_score = source_validation_result.overall_score
                    elif fact_check_result:
                        external_verification_score = fact_check_result.confidence_score
                    
                    # ìµœì¢… ì‹ ë¢°ë„ ì ìˆ˜ (ê°€ì¤‘ í‰ê· )
                    final_confidence = (
                        self_verification_score * 0.3 +
                        cross_verification_score_normalized * 0.4 +
                        external_verification_score * 0.3
                    )
                    
                    # ì‹ ë¢°ë„ ìˆ˜ì¤€ì— ë”°ë¥¸ ì ìˆ˜ ì¡°ì •
                    if confidence_level == "LOW":
                        final_confidence = min(final_confidence, 0.5)
                    elif confidence_level == "HIGH":
                        final_confidence = max(final_confidence, 0.7)
                    
                    # ê²€ì¦ íŒë‹¨: REJECTEDê°€ ëª…ì‹œì ìœ¼ë¡œ ìˆê³  ê´€ë ¨ì„± ì ìˆ˜ê°€ ë§¤ìš° ë‚®ì€ ê²½ìš°ë§Œ ê±°ë¶€
                    is_verified = True  # ê¸°ë³¸ê°’ì€ í†µê³¼
                    if status_match and "REJECTED" in status_match:
                        # REJECTEDì´ì§€ë§Œ ê´€ë ¨ì„± ì ìˆ˜ê°€ 3 ì´ìƒì´ë©´ í†µê³¼ (í° ì˜¤ë¥˜ë§Œ ê±°ë¶€)
                        if relevance_score >= 3:
                            logger.info(f"[{self.name}] âš ï¸ Result marked as REJECTED but relevance_score={relevance_score} >= 3, verifying anyway")
                            is_verified = True
                        else:
                            is_verified = False
                    elif status_match and "VERIFIED" in status_match:
                        is_verified = True
                    elif "REJECTED" in verification_upper and relevance_score < 2:
                        # ëª…ì‹œì  REJECTEDê°€ ì—†ì–´ë„ ê´€ë ¨ì„± ì ìˆ˜ê°€ ë§¤ìš° ë‚®ìœ¼ë©´ ê±°ë¶€
                        is_verified = False
                    else:
                        # ëª…ì‹œì  íŒë‹¨ì´ ì—†ìœ¼ë©´ ê´€ë ¨ì„± ê¸°ë°˜ìœ¼ë¡œ íŒë‹¨
                        is_verified = relevance_score >= 3
                    
                    logger.info(f"[{self.name}] ğŸ“‹ Verification result {i}: '{verification_text[:150]}' -> is_verified={is_verified}")
                    
                    if is_verified:
                        # ì œì–¸ì‚¬í•­ ì¶”ì¶œ
                        recommendations = "ì—†ìŒ"
                        if "RECOMMENDATIONS:" in verification_text:
                            rec_lines = [line for line in verification_text.split('\n') if 'RECOMMENDATIONS:' in line]
                            if rec_lines:
                                rec_text = rec_lines[0].split('RECOMMENDATIONS:')[1].strip()
                                if rec_text and rec_text != "ì—†ìŒ":
                                    recommendations = rec_text[:300]
                        
                        # ì´ìŠˆ ì¶”ì¶œ
                        issues = "ì—†ìŒ"
                        if "ISSUES:" in verification_text:
                            issue_lines = [line for line in verification_text.split('\n') if 'ISSUES:' in line]
                            if issue_lines:
                                issue_text = issue_lines[0].split('ISSUES:')[1].strip()
                                if issue_text and issue_text != "ì—†ìŒ":
                                    issues = issue_text[:300]
                        
                        verified_result = {
                            "index": i,
                            "title": title,
                            "snippet": snippet,
                            "url": url,
                            "status": "verified",
                            "verification_note": verification_text[:500],  # ë” ê¸´ ì œì–¸ í¬í•¨
                            "relevance_score": relevance_score,
                            "quality_score": quality_score,
                            "evidence_score": evidence_score,
                            "confidence_level": confidence_level,
                            "final_confidence": final_confidence,
                            "uncertainty_issues": uncertainty_issues,
                            "recommendations": recommendations,
                            "issues": issues,
                            "additional_research_needed": additional_research_needed,
                            # ë‹¤ë‹¨ê³„ ê²€ì¦ ì ìˆ˜
                            "verification_stages": {
                                "self_verification_score": self_verification_score,
                                "cross_verification_score": cross_verification_score_normalized,
                                "external_verification_score": external_verification_score,
                                "final_confidence": final_confidence
                            },
                            "source_validation": {
                                "overall_score": source_validation_result.overall_score if source_validation_result else None,
                                "domain_type": source_validation_result.domain_type.value if source_validation_result else None,
                                "domain_trust": source_validation_result.domain_trust if source_validation_result else None
                            } if source_validation_result else None,
                            "fact_check": {
                                "status": fact_check_result.fact_status.value if fact_check_result else None,
                                "confidence": fact_check_result.confidence_score if fact_check_result else None
                            } if fact_check_result else None,
                            "cross_verification_score": cross_verification_score
                        }
                        # full_contentì™€ published_date í¬í•¨
                        if full_content:
                            verified_result['full_content'] = full_content
                        if published_date:
                            verified_result['published_date'] = published_date
                        verified.append(verified_result)
                        logger.info(f"[{self.name}] âœ… Result {i} verified: '{title[:50]}...' (relevance: {relevance_score}, issues: {issues[:50] if issues != 'ì—†ìŒ' else 'ì—†ìŒ'})")
                    else:
                        rejected_reasons.append({
                            "index": i,
                            "title": title[:80],
                            "reason": verification_text[:200],
                            "url": url[:100] if url else "N/A"
                        })
                        logger.info(f"[{self.name}] âš ï¸ Result {i} rejected: '{title[:50]}...' (reason: {verification_text[:100]})")
                        continue
                except Exception as e:
                    error_str = str(e).lower()
                    verification_errors.append({
                        "index": i,
                        "title": title[:80],
                        "error": str(e)[:200]
                    })
                    # Rate limitì´ë‚˜ ëª¨ë“  ëª¨ë¸ ì‹¤íŒ¨ ì‹œì—ëŠ” í¬í•¨í•˜ì§€ ì•ŠìŒ (í’ˆì§ˆ ì €í•˜ ë°©ì§€)
                    if "rate limit" in error_str or "429" in error_str or "all fallback models failed" in error_str or "no available models" in error_str:
                        logger.warning(f"[{self.name}] âš ï¸ Verification failed for result {i}: {e} (rate limit/all models failed), excluding from results")
                        continue  # í’ˆì§ˆ ì €í•˜ ë°©ì§€ë¥¼ ìœ„í•´ ì œì™¸
                    else:
                        logger.warning(f"[{self.name}] âš ï¸ Verification failed for result {i}: {e}, including anyway")
                        # ê²€ì¦ ì‹¤íŒ¨í•´ë„ ê¸°ë³¸ ì •ë³´ê°€ ìˆìœ¼ë©´ í¬í•¨ (ë‹¨, rate limitì´ ì•„ë‹Œ ê²½ìš°ë§Œ)
                        if title and (snippet or url):
                            verified.append({
                                "index": i,
                                "title": title,
                                "snippet": snippet,
                                "url": url,
                                "status": "partial",
                                "verification_note": f"Verification failed: {str(e)[:100]}"
                            })
            else:
                skipped_count += 1
                logger.warning(f"[{self.name}] âš ï¸ Unknown result format: {type(result)}, value: {str(result)[:100]}")
                continue
        
        # ê²€ì¦ í†µê³„ ë° ë””ë²„ê¹… ì •ë³´ ì¶œë ¥
        logger.info(f"[{self.name}] ğŸ“Š Verification Statistics:")
        logger.info(f"[{self.name}]   - Total results: {len(results)}")
        logger.info(f"[{self.name}]   - Verified: {len(verified)}")
        logger.info(f"[{self.name}]   - Rejected: {len(rejected_reasons)}")
        logger.info(f"[{self.name}]   - Skipped: {skipped_count}")
        logger.info(f"[{self.name}]   - Verification errors: {len(verification_errors)}")
        
        if rejected_reasons:
            logger.warning(f"[{self.name}] ğŸ” Rejected Results Analysis:")
            for rejected in rejected_reasons[:5]:  # ìµœëŒ€ 5ê°œë§Œ í‘œì‹œ
                logger.warning(f"[{self.name}]   - Result {rejected['index']}: '{rejected['title']}'")
                logger.warning(f"[{self.name}]     Reason: {rejected['reason']}")
                logger.warning(f"[{self.name}]     URL: {rejected['url']}")
        
        if verification_errors:
            logger.error(f"[{self.name}] âŒ Verification Errors:")
            for error_info in verification_errors[:3]:  # ìµœëŒ€ 3ê°œë§Œ í‘œì‹œ
                logger.error(f"[{self.name}]   - Result {error_info['index']}: '{error_info['title']}'")
                logger.error(f"[{self.name}]     Error: {error_info['error']}")
        
        # ê²€ì¦ëœ ê²°ê³¼ê°€ ì—†ì„ ë•Œ ì›ë³¸ ê²°ê³¼ë¥¼ ì‚¬ìš©í•˜ëŠ” fallback
        if not verified and len(results) > 0:
            logger.warning(f"[{self.name}] âš ï¸ No results verified! Using original results as fallback...")
            logger.warning(f"[{self.name}] ğŸ” This may indicate:")
            logger.warning(f"[{self.name}]   1. Search queries are not matching the user query")
            logger.warning(f"[{self.name}]   2. Verification criteria are too strict")
            logger.warning(f"[{self.name}]   3. Search results are genuinely irrelevant")
            
            # ì›ë³¸ ê²°ê³¼ë¥¼ ê²€ì¦ëœ ê²°ê³¼ë¡œ ì‚¬ìš© (ì‹ ë¢°ë„ ë‚®ê²Œ)
            for i, result in enumerate(results[:5], 1):  # ìµœëŒ€ 5ê°œë§Œ
                if isinstance(result, dict):
                    title = result.get('title') or result.get('name') or ''
                    snippet = result.get('snippet') or result.get('content') or ''
                    url = result.get('url') or result.get('link') or ''
                    
                    if title and len(title.strip()) >= 3:
                        verified.append({
                            "index": i,
                            "title": title,
                            "snippet": snippet[:500] if snippet else '',
                            "url": url,
                            "status": "fallback",
                            "verification_note": "No verified results found, using original search results as fallback"
                        })
                        logger.warning(f"[{self.name}] âš ï¸ Added fallback result {i}: '{title[:50]}...'")
            
            logger.warning(f"[{self.name}] âš ï¸ Using {len(verified)} fallback results (low confidence)")
        
        logger.info(f"[{self.name}] âœ… Verification completed: {len(verified)}/{len(results)} results verified (including fallback)")
        
        # ê²€ì¦ ê²°ê³¼ë¥¼ SharedResultsManagerì— ê³µìœ 
        if self.context.shared_results_manager:
            shared_verification_count = 0
            for verified_result in verified:
                task_id = f"verification_{verified_result.get('index', 0)}"
                result_id = await self.context.shared_results_manager.share_result(
                    task_id=task_id,
                    agent_id=self.context.agent_id,  # ê³ ìœ í•œ agent_id ì‚¬ìš©
                    result=verified_result,
                    metadata={"status": verified_result.get('status', 'unknown')},
                    confidence=1.0 if verified_result.get('status') == 'verified' else 0.5
                )
                shared_verification_count += 1
                # ê°œë³„ ë¡œê·¸ëŠ” debug ë ˆë²¨ë¡œ ë³€ê²½ (ë„ˆë¬´ ë§ì€ ë¡œê·¸ ë°©ì§€)
                logger.debug(f"[{self.name}] ğŸ”— Shared verification result {verified_result.get('index', 0)} (result_id: {result_id[:8]}..., status: {verified_result.get('status', 'unknown')})")

            logger.info(f"[{self.name}] ğŸ“¤ Shared {shared_verification_count} verification results with other agents")

            # Executor ê²°ê³¼ì— ëŒ€í•œ ë…¼ë°• (Debate) ìˆ˜í–‰
            if self.context.discussion_manager and self.context.shared_results_manager and len(verified) > 0:
                # Executor ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
                executor_results = await self.context.shared_results_manager.get_shared_results(
                    task_id=None  # ëª¨ë“  Executor ê²°ê³¼
                )
                
                # Executor ê²°ê³¼ í•„í„°ë§ (executorë¡œ ì‹œì‘í•˜ëŠ” agent_id)
                executor_shared_results = [r for r in executor_results if r.agent_id.startswith('executor')]
                
                if executor_shared_results:
                    logger.info(f"[{self.name}] ğŸ’¬ Found {len(executor_shared_results)} executor results to debate")
                    
                    # ê° Executor ê²°ê³¼ì— ëŒ€í•´ ë…¼ë°• ìˆ˜í–‰
                    debate_results = []
                    for executor_result in executor_shared_results[:5]:  # ìµœëŒ€ 5ê°œ ê²°ê³¼ì— ëŒ€í•´ ë…¼ë°•
                        # ë‹¤ë¥¸ Verifierë“¤ì˜ ê²€ì¦ ê²°ê³¼ë„ ê°€ì ¸ì˜¤ê¸°
                        other_verifiers = await self.context.shared_results_manager.get_shared_results(
                            agent_id=None,
                            exclude_agent_id=self.context.agent_id
                        )
                        other_verifier_results = [r for r in other_verifiers if r.agent_id.startswith('verifier')]
                        
                        # ë…¼ë°• ìˆ˜í–‰
                        debate_result = await self.context.discussion_manager.agent_discuss_result(
                            result_id=executor_result.task_id,
                            agent_id=self.context.agent_id,
                            other_agent_results=other_verifier_results[:3] + [executor_result],  # ë‹¤ë¥¸ Verifier + Executor ê²°ê³¼
                            discussion_type="verification"
                        )
                        
                        if debate_result:
                            debate_results.append(debate_result)
                            logger.info(f"[{self.name}] ğŸ’¬ Debate completed for executor result: consistency={debate_result.get('consistency_check', 'unknown')}, validity={debate_result.get('logical_validity', 'unknown')}")
                    
                    # ë…¼ë°• ê²°ê³¼ë¥¼ stateì— ì €ì¥
                    if 'agent_debates' not in state:
                        state['agent_debates'] = {}
                    state['agent_debates']['verifier_debates'] = debate_results
                    logger.info(f"[{self.name}] ğŸ’¬ Saved {len(debate_results)} debate results to state")
                
                # ë‹¤ë¥¸ Verifierë“¤ì˜ ê²€ì¦ ê²°ê³¼ì™€ ë…¼ë°•
                other_verified = await self.context.shared_results_manager.get_shared_results(
                    agent_id=None,
                    exclude_agent_id=self.context.agent_id
                )
                
                # ê²€ì¦ëœ ê²°ê³¼ë§Œ í•„í„°ë§
                other_verified_results = [r for r in other_verified if isinstance(r.result, dict) and r.result.get('status') == 'verified']

                if other_verified_results:
                    logger.info(f"[{self.name}] ğŸ’¬ Found {len(other_verified_results)} verified results from other verifiers for debate")

                    # ì²« ë²ˆì§¸ ê²€ì¦ ê²°ê³¼ì— ëŒ€í•´ ë…¼ë°•
                    first_verified = verified[0]
                    result_id = f"verification_{first_verified.get('index', 0)}"
                    logger.info(f"[{self.name}] ğŸ’¬ Starting debate on verification result {first_verified.get('index', 0)} with {len(other_verified_results[:3])} other verifiers")

                    debate_result = await self.context.discussion_manager.agent_discuss_result(
                        result_id=result_id,
                        agent_id=self.context.agent_id,
                        other_agent_results=other_verified_results[:3],
                        discussion_type="verification"
                    )
                    
                    if debate_result:
                        logger.info(f"[{self.name}] ğŸ’¬ Debate completed: consistency={debate_result.get('consistency_check', 'unknown')}, validity={debate_result.get('logical_validity', 'unknown')}")
                        logger.info(f"[{self.name}] ğŸ¤ Agent debate: Analyzed verification consistency with {len(other_verified_results[:3])} peer agents")
                        
                        # ë…¼ë°• ê²°ê³¼ ì €ì¥
                        if 'agent_debates' not in state:
                            state['agent_debates'] = {}
                        if 'verifier_peer_debates' not in state['agent_debates']:
                            state['agent_debates']['verifier_peer_debates'] = []
                        state['agent_debates']['verifier_peer_debates'].append(debate_result)
                    else:
                        logger.info(f"[{self.name}] ğŸ’¬ No debate generated for verification result")
                else:
                    logger.info(f"[{self.name}] ğŸ’¬ No other verified results found for debate")
            else:
                logger.info(f"[{self.name}] Agent debate disabled or no verified results to debate")
        
        # Council í™œì„±í™” í™•ì¸ ë° ì ìš© (ì‚¬ì‹¤ í™•ì¸ì´ ì¤‘ìš”í•œ ê²½ìš° - ê¸°ë³¸ í™œì„±í™”)
        use_council = state.get('use_council', None)  # ìˆ˜ë™ í™œì„±í™” ì˜µì…˜
        if use_council is None:
            # ìë™ í™œì„±í™” íŒë‹¨ (ê¸°ë³¸ í™œì„±í™”)
            from src.core.council_activator import get_council_activator
            activator = get_council_activator()
            
            context = {
                'low_confidence_sources': len([r for r in verified if r.get('confidence', 1.0) < 0.7]),
                'verification_count': len(verified)
            }
            
            activation_decision = activator.should_activate(
                process_type='verification',
                query=state['user_query'],
                context=context
            )
            use_council = activation_decision.should_activate
            if use_council:
                logger.info(f"[{self.name}] ğŸ›ï¸ Council auto-activated: {activation_decision.reason}")
        
        # Council ì ìš© (í™œì„±í™”ëœ ê²½ìš°)
        if use_council and verified:
            try:
                from src.core.llm_council import run_full_council
                logger.info(f"[{self.name}] ğŸ›ï¸ Running Council review for verification results...")
                
                # ê²€ì¦ ê²°ê³¼ ìš”ì•½ ìƒì„±
                verification_summary = "\n\n".join([
                    f"Result {i+1}:\nTitle: {r.get('title', 'N/A')}\nStatus: {r.get('status', 'N/A')}\nConfidence: {r.get('confidence', 0.0):.2f}\nNote: {r.get('verification_note', 'N/A')[:100]}"
                    for i, r in enumerate(verified[:10])  # ìµœëŒ€ 10ê°œë§Œ ê²€í† 
                ])
                
                council_query = f"""Review the verification results and assess their reliability. Check for consistency and identify any potential issues.

Research Query: {state['user_query']}

Verification Results:
{verification_summary}

Provide a review with:
1. Overall verification quality assessment
2. Consistency check across results
3. Recommendations for improvement"""
                
                stage1_results, stage2_results, stage3_result, metadata = await run_full_council(
                    council_query
                )
                
                # Council ê²€í†  ê²°ê³¼
                review_report = stage3_result.get('response', '')
                logger.info(f"[{self.name}] âœ… Council review completed.")
                logger.info(f"[{self.name}] Council aggregate rankings: {metadata.get('aggregate_rankings', [])}")
                
                # Council ë©”íƒ€ë°ì´í„°ë¥¼ stateì— ì €ì¥
                if 'council_metadata' not in state:
                    state['council_metadata'] = {}
                state['council_metadata']['verification'] = {
                    'stage1_results': stage1_results,
                    'stage2_results': stage2_results,
                    'stage3_result': stage3_result,
                    'metadata': metadata,
                    'review_report': review_report
                }
            except Exception as e:
                logger.warning(f"[{self.name}] Council review failed: {e}. Using original verification results.")
                # Council ì‹¤íŒ¨ ì‹œ ì›ë³¸ ê²€ì¦ ê²°ê³¼ ì‚¬ìš© (fallback ì œê±° - ëª…í™•í•œ ë¡œê¹…ë§Œ)
        
        # Enhanced Quality Assessment (Phase 2)
        quality_assessments = {}
        logger.info(f"[{self.name}] ğŸ“Š Performing quality assessment on {len(verified)} verified results")
        
        for result in verified:
            result_id = result.get('id') or result.get('url', '')
            if result_id:
                quality_assessment = await self._assess_result_quality(result, verified)
                quality_assessments[result_id] = quality_assessment
                
                # Add quality assessment to result
                result['quality_assessment'] = quality_assessment
                
                logger.debug(f"[{self.name}] Quality assessment for {result.get('title', 'N/A')[:50]}: "
                           f"credibility={quality_assessment['source_credibility']:.2f}, "
                           f"academic={quality_assessment['academic_rigor']:.2f}, "
                           f"verifiability={quality_assessment['verifiability']:.2f}")
        
        # Store quality assessments in state
        state['quality_assessments'] = quality_assessments
        state['verified_results'] = verified
        state['current_agent'] = self.name
        state['verification_failed'] = False if verified else True
        
        logger.info(f"[{self.name}] âœ… Quality assessment complete: {len(quality_assessments)} results assessed")
        
        # Write to shared memory
        memory.write(
            key=f"verified_{state['session_id']}",
            value=verified,
            scope=MemoryScope.SESSION,
            session_id=state['session_id'],
            agent_id=self.name
        )
        
        memory.write(
            key=f"quality_assessments_{state['session_id']}",
            value=quality_assessments,
            scope=MemoryScope.SESSION,
            session_id=state['session_id'],
            agent_id=self.name
        )
        
        logger.info(f"[{self.name}] Verified results and quality assessments saved to shared memory")
        logger.info(f"=" * 80)
        
        return state
    
    async def _assess_result_quality(
        self,
        result: Dict[str, Any],
        all_results: List[Dict[str, Any]]
    ) -> Dict[str, float]:
        """
        Assess the quality of a research result across multiple dimensions.
        
        Args:
            result: The result to assess
            all_results: All verified results for cross-validation
            
        Returns:
            Quality assessment dictionary with scores (0-1)
        """
        try:
            # Assess each dimension
            source_credibility = self._assess_source_credibility(result)
            academic_rigor = self._assess_academic_rigor(result)
            verifiability = self._assess_verifiability(result)
            cross_source_support = self._assess_cross_validation(result, all_results)
            information_freshness = self._assess_recency(result)
            
            # Calculate weighted overall quality
            weights = {
                'source_credibility': 0.25,
                'academic_rigor': 0.25,
                'verifiability': 0.25,
                'cross_source_support': 0.15,
                'information_freshness': 0.10
            }
            
            overall_quality = (
                source_credibility * weights['source_credibility'] +
                academic_rigor * weights['academic_rigor'] +
                verifiability * weights['verifiability'] +
                cross_source_support * weights['cross_source_support'] +
                information_freshness * weights['information_freshness']
            )
            
            return {
                'source_credibility': source_credibility,
                'academic_rigor': academic_rigor,
                'verifiability': verifiability,
                'cross_source_support': cross_source_support,
                'information_freshness': information_freshness,
                'overall_quality': overall_quality
            }
            
        except Exception as e:
            logger.warning(f"[{self.name}] Error assessing result quality: {e}")
            # Return default scores on error
            return {
                'source_credibility': 0.5,
                'academic_rigor': 0.5,
                'verifiability': 0.5,
                'cross_source_support': 0.5,
                'information_freshness': 0.5,
                'overall_quality': 0.5
            }
    
    def _assess_source_credibility(self, result: Dict[str, Any]) -> float:
        """
        Assess source credibility based on domain and source type.
        
        Priority: Academic > Official > News > General
        
        Returns:
            Credibility score (0-1)
        """
        url = result.get('url', '').lower()
        title = result.get('title', '').lower()
        
        # Academic sources (highest credibility)
        academic_indicators = [
            'arxiv.org', 'doi.org', 'scholar.google', 'pubmed', 'ncbi.nlm.nih.gov',
            'ieee.org', 'acm.org', 'springer.com', 'sciencedirect.com',
            'nature.com', 'science.org', 'cell.com', 'wiley.com',
            '.edu', 'university', 'journal', 'peer-reviewed'
        ]
        
        if any(indicator in url for indicator in academic_indicators):
            return 0.95
        
        # Official/Government sources
        official_indicators = [
            '.gov', '.mil', 'who.int', 'un.org', 'europa.eu',
            'nih.gov', 'nasa.gov', 'cdc.gov', 'fda.gov'
        ]
        
        if any(indicator in url for indicator in official_indicators):
            return 0.90
        
        # Reputable news/media
        news_indicators = [
            'reuters.com', 'bbc.com', 'apnews.com', 'nytimes.com',
            'wsj.com', 'economist.com', 'theguardian.com', 'washingtonpost.com'
        ]
        
        if any(indicator in url for indicator in news_indicators):
            return 0.80
        
        # Industry/Professional organizations
        industry_indicators = [
            '.org', 'association', 'institute', 'foundation', 'society'
        ]
        
        if any(indicator in url for indicator in industry_indicators):
            return 0.70
        
        # General web sources
        return 0.50
    
    def _assess_academic_rigor(self, result: Dict[str, Any]) -> float:
        """
        Assess academic rigor based on content indicators.
        
        Checks for: citations, methodology, peer-review, research terms
        
        Returns:
            Academic rigor score (0-1)
        """
        content = (result.get('content', '') + ' ' + 
                  result.get('snippet', '') + ' ' + 
                  result.get('title', '')).lower()
        
        url = result.get('url', '').lower()
        
        rigor_score = 0.0
        
        # Check for peer-reviewed publication
        peer_review_indicators = ['peer-reviewed', 'peer reviewed', 'refereed']
        if any(indicator in content or indicator in url for indicator in peer_review_indicators):
            rigor_score += 0.3
        
        # Check for citations/references
        citation_indicators = ['citation', 'references', 'bibliography', 'doi:', 'cited by']
        if any(indicator in content for indicator in citation_indicators):
            rigor_score += 0.2
        
        # Check for methodology
        methodology_indicators = ['methodology', 'methods', 'experimental', 'study design', 'data collection']
        if any(indicator in content for indicator in methodology_indicators):
            rigor_score += 0.2
        
        # Check for research terms
        research_indicators = ['research', 'study', 'analysis', 'investigation', 'findings', 'results', 'conclusion']
        matching_indicators = sum(1 for indicator in research_indicators if indicator in content)
        rigor_score += min(0.3, matching_indicators * 0.05)
        
        return min(1.0, rigor_score)
    
    def _assess_verifiability(self, result: Dict[str, Any]) -> float:
        """
        Assess verifiability based on evidence, data, and references.
        
        Returns:
            Verifiability score (0-1)
        """
        content = (result.get('content', '') + ' ' + 
                  result.get('snippet', '')).lower()
        
        url = result.get('url', '').lower()
        
        verifiability_score = 0.0
        
        # Check for data/evidence indicators
        data_indicators = ['data', 'evidence', 'statistics', 'figures', 'table', 'chart']
        matching_data = sum(1 for indicator in data_indicators if indicator in content)
        verifiability_score += min(0.3, matching_data * 0.05)
        
        # Check for specific claims with sources
        source_indicators = ['according to', 'source:', 'reference:', 'published in', 'reported by']
        if any(indicator in content for indicator in source_indicators):
            verifiability_score += 0.3
        
        # Check for URL availability and type
        if url:
            verifiability_score += 0.2
            
            # Bonus for direct document links
            if any(ext in url for ext in ['.pdf', '.doc', '.html']):
                verifiability_score += 0.1
        
        # Check for author information
        author_indicators = ['author:', 'by ', 'written by', 'published by']
        if any(indicator in content for indicator in author_indicators):
            verifiability_score += 0.1
        
        return min(1.0, verifiability_score)
    
    def _assess_cross_validation(
        self,
        result: Dict[str, Any],
        all_results: List[Dict[str, Any]]
    ) -> float:
        """
        Assess cross-validation by checking if information is supported by other sources.
        
        Returns:
            Cross-validation score (0-1)
        """
        if len(all_results) < 2:
            return 0.5  # Can't cross-validate with single result
        
        try:
            # Extract key terms from current result
            current_title = result.get('title', '').lower()
            current_content = (result.get('content', '') + ' ' + 
                             result.get('snippet', '')).lower()
            
            # Extract significant words (simple approach)
            import re
            words = re.findall(r'\b\w{4,}\b', current_title + ' ' + current_content)
            significant_words = set(words[:20])  # Take top 20 words
            
            # Check overlap with other results
            overlap_scores = []
            
            for other_result in all_results:
                if other_result.get('url') == result.get('url'):
                    continue  # Skip same result
                
                other_content = (other_result.get('title', '') + ' ' +
                               other_result.get('content', '') + ' ' +
                               other_result.get('snippet', '')).lower()
                
                other_words = set(re.findall(r'\b\w{4,}\b', other_content))
                
                if significant_words and other_words:
                    overlap = len(significant_words & other_words) / len(significant_words)
                    overlap_scores.append(overlap)
            
            if overlap_scores:
                # Average overlap with other sources
                avg_overlap = sum(overlap_scores) / len(overlap_scores)
                return min(1.0, avg_overlap * 2)  # Scale up
            else:
                return 0.5
                
        except Exception as e:
            logger.debug(f"Error in cross-validation: {e}")
            return 0.5
    
    def _assess_recency(self, result: Dict[str, Any]) -> float:
        """
        Assess information freshness based on publication date.
        
        Returns:
            Recency score (0-1)
        """
        published_date = result.get('published_date', '')
        
        if not published_date:
            return 0.5  # Unknown date
        
        try:
            from datetime import datetime, timezone
            
            # Parse date
            if isinstance(published_date, str):
                date_obj = datetime.fromisoformat(published_date.replace('Z', '+00:00'))
            else:
                date_obj = published_date
            
            # Calculate age in days
            now = datetime.now(timezone.utc)
            age_days = (now - date_obj).days
            
            # Scoring based on age
            if age_days < 30:
                return 1.0  # Very recent (< 1 month)
            elif age_days < 90:
                return 0.9  # Recent (< 3 months)
            elif age_days < 365:
                return 0.8  # This year
            elif age_days < 730:
                return 0.7  # Last 2 years
            elif age_days < 1825:
                return 0.6  # Last 5 years
            else:
                return 0.4  # Older than 5 years
                
        except Exception as e:
            logger.debug(f"Error assessing recency: {e}")
            return 0.5


class GeneratorAgent:
    """Generator agent - creates final report (Skills-based)."""
    
    def __init__(self, context: AgentContext, skill: Optional[Skill] = None):
        self.context = context
        self.name = "generator"
        self.available_tools: list = []  # MCP ìë™ í• ë‹¹ ë„êµ¬
        self.tool_infos: list = []  # ë„êµ¬ ë©”íƒ€ë°ì´í„°
        self.skill = skill
        
        # Skillì´ ì—†ìœ¼ë©´ ë¡œë“œ ì‹œë„
        if self.skill is None:
            skill_manager = get_skill_manager()
            self.skill = skill_manager.load_skill("synthesizer")
        
        # Skill instruction ì‚¬ìš©
        if self.skill:
            self.instruction = self.skill.instructions
        else:
            self.instruction = "You are a report generation agent."
    
    def _validate_and_enhance_citations(self, report: str, source_mapping: Dict[int, Dict[str, Any]], verified_results: List[Dict[str, Any]]) -> str:
        """
        ë³´ê³ ì„œì˜ ì¶œì²˜ ì¸ìš©ì„ ê²€ì¦í•˜ê³  ë³´ì™„í•©ë‹ˆë‹¤.
        ë³¸ë¬¸ì—ì„œ ì‹¤ì œë¡œ ì¸ìš©ëœ ì¶œì²˜ë§Œ ì°¸ê³ ë¬¸í—Œì— í¬í•¨í•©ë‹ˆë‹¤.
        
        Args:
            report: ìƒì„±ëœ ë³´ê³ ì„œ
            source_mapping: ì¶œì²˜ ë²ˆí˜¸ -> ì¶œì²˜ ì •ë³´ ë§¤í•‘
            verified_results: ê²€ì¦ëœ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            ì¶œì²˜ ì¸ìš©ì´ ë³´ì™„ëœ ë³´ê³ ì„œ
        """
        
        # ë³¸ë¬¸ì—ì„œ ì¸ìš©ëœ ì¶œì²˜ ë²ˆí˜¸ ì¶”ì¶œ
        body_text = report
        references_section_match = re.search(r'##?\s*ì°¸ê³ \s*ë¬¸í—Œ|##?\s*References|##?\s*ì¶œì²˜', report, re.IGNORECASE)
        if references_section_match:
            body_text = report[:references_section_match.start()]
        
        # ë³¸ë¬¸ì—ì„œ ì¸ìš© íŒ¨í„´ ì°¾ê¸°: [1], [1,2], ì¶œì²˜ 1, (ì¶œì²˜ 1), ì¶œì²˜1 ë“±
        cited_patterns = [
            r'\[(\d+)\]',  # [1]
            r'\[(\d+),\s*(\d+)\]',  # [1, 2]
            r'ì¶œì²˜\s*(\d+)',  # ì¶œì²˜ 1
            r'\(ì¶œì²˜\s*(\d+)\)',  # (ì¶œì²˜ 1)
            r'ì¶œì²˜(\d+)',  # ì¶œì²˜1
        ]
        
        cited_numbers = set()
        for pattern in cited_patterns:
            matches = re.findall(pattern, body_text)
            for match in matches:
                if isinstance(match, tuple):
                    for num in match:
                        if num and num.isdigit():
                            cited_numbers.add(int(num))
                elif match and match.isdigit():
                    cited_numbers.add(int(match))
        
        logger.info(f"[{self.name}] ğŸ“‹ Found {len(cited_numbers)} cited sources in body: {sorted(cited_numbers)}")
        
        # ì°¸ê³  ë¬¸í—Œ ì„¹ì…˜ í™•ì¸ ë° ì¬ìƒì„±
        if references_section_match:
            # ê¸°ì¡´ ì°¸ê³  ë¬¸í—Œ ì„¹ì…˜ ì œê±°
            report = report[:references_section_match.start()].rstrip()
        
        # ë³¸ë¬¸ì—ì„œ ì¸ìš©ëœ ì¶œì²˜ë§Œ í¬í•¨í•˜ëŠ” ì°¸ê³  ë¬¸í—Œ ìƒì„±
        if cited_numbers:
            references_text = "\n\n## ì°¸ê³  ë¬¸í—Œ\n\n"
            
            # ì¸ìš©ëœ ë²ˆí˜¸ ìˆœì„œëŒ€ë¡œ ì •ë ¬
            sorted_cited = sorted(cited_numbers)
            new_source_mapping = {}
            
            for new_idx, old_num in enumerate(sorted_cited, 1):
                if old_num in source_mapping:
                    source_info = source_mapping[old_num]
                    new_source_mapping[new_idx] = source_info
                    references_text += f"{new_idx}. [{source_info['title']}]({source_info['url']})\n"
                    if source_info.get('published_date'):
                        references_text += f"   ë°œí–‰ì¼: {source_info['published_date']}\n"
                    references_text += "\n"
                else:
                    # source_mappingì— ì—†ëŠ” ê²½ìš° verified_resultsì—ì„œ ì°¾ê¸°
                    if old_num <= len(verified_results):
                        result = verified_results[old_num - 1]
                        if isinstance(result, dict):
                            title = result.get('title', '')
                            url = result.get('url', '')
                            if title and url:
                                new_source_mapping[new_idx] = {
                                    'title': title,
                                    'url': url,
                                    'published_date': result.get('published_date', '')
                                }
                                references_text += f"{new_idx}. [{title}]({url})\n"
                                if result.get('published_date'):
                                    references_text += f"   ë°œí–‰ì¼: {result['published_date']}\n"
                                references_text += "\n"
            
            # ë³¸ë¬¸ì˜ ì¶œì²˜ ë²ˆí˜¸ë¥¼ ìƒˆë¡œìš´ ë²ˆí˜¸ë¡œ ì—…ë°ì´íŠ¸
            for old_idx, new_idx in enumerate(sorted_cited, 1):
                old_num = sorted_cited[old_idx - 1]
                # [old_num] -> [new_idx]ë¡œ ë³€ê²½
                report = re.sub(rf'\[{old_num}\]', f'[{new_idx}]', report)
                report = re.sub(rf'ì¶œì²˜\s*{old_num}\b', f'ì¶œì²˜ {new_idx}', report)
                report = re.sub(rf'\(ì¶œì²˜\s*{old_num}\)', f'(ì¶œì²˜ {new_idx})', report)
            
            report += references_text
            logger.info(f"[{self.name}] âœ… Rebuilt references section with {len(sorted_cited)} cited sources (removed uncited sources)")
        else:
            # ì¸ìš©ì´ ì—†ìœ¼ë©´ ì°¸ê³  ë¬¸í—Œ ì„¹ì…˜ ì œê±°
            logger.warning(f"[{self.name}] âš ï¸ No citations found in body, removing references section")
        
        return report
    
    async def execute(self, state: AgentState) -> AgentState:
        """Generate final report."""
        logger.info(f"[{self.name}] Generating final report...")
        
        # ì—°êµ¬ ë˜ëŠ” ê²€ì¦ ì‹¤íŒ¨ í™•ì¸ - Fallback ì œê±°, ëª…í™•í•œ ì—ëŸ¬ë§Œ ë°˜í™˜
        if state.get('research_failed') or state.get('verification_failed'):
            error_msg = state.get('error')
            if not error_msg:
                if state.get('verification_failed'):
                    error_msg = "ê²€ì¦ ì‹¤íŒ¨: ê²€ì¦ëœ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤"
                elif state.get('research_failed'):
                    error_msg = "ì—°êµ¬ ì‹¤í–‰ ì‹¤íŒ¨"
                else:
                    error_msg = "ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜"

            # ìƒì„¸ ë””ë²„ê¹… ì •ë³´ ì¶œë ¥
            logger.error(f"[{self.name}] âŒ Research or verification failed: {error_msg}")
            logger.error(f"[{self.name}] ğŸ” Debugging Information:")
            logger.error(f"[{self.name}]   - Research failed: {state.get('research_failed', False)}")
            logger.error(f"[{self.name}]   - Verification failed: {state.get('verification_failed', False)}")
            logger.error(f"[{self.name}]   - User query: '{state.get('user_query', 'N/A')}'")
            
            # ê²€ì¦ ê²°ê³¼ í™•ì¸
            verified_results = state.get('verified_results', [])
            research_results = state.get('research_results', [])
            logger.error(f"[{self.name}]   - Verified results count: {len(verified_results) if verified_results else 0}")
            logger.error(f"[{self.name}]   - Research results count: {len(research_results) if research_results else 0}")
            
            # SharedResultsManagerì—ì„œ ê²°ê³¼ í™•ì¸
            if self.context.shared_results_manager:
                try:
                    shared_results = await self.context.shared_results_manager.get_shared_results(
                        agent_id=None
                    )
                    logger.error(f"[{self.name}]   - Shared results count: {len(shared_results) if shared_results else 0}")
                except Exception as e:
                    logger.error(f"[{self.name}]   - Failed to get shared results: {e}")
            
            # ê²€ì¦ ì‹¤íŒ¨ ì›ì¸ ë¶„ì„
            if state.get('verification_failed'):
                logger.error(f"[{self.name}] ğŸ” Verification Failure Analysis:")
                logger.error(f"[{self.name}]   - Possible causes:")
                logger.error(f"[{self.name}]     1. Search queries did not match user query")
                logger.error(f"[{self.name}]     2. Verification criteria were too strict")
                logger.error(f"[{self.name}]     3. Search results were genuinely irrelevant")
                logger.error(f"[{self.name}]     4. LLM verification service issues")
                
                # ì›ë³¸ ê²€ìƒ‰ ê²°ê³¼ê°€ ìˆìœ¼ë©´ ì¼ë¶€ í‘œì‹œ
                if research_results and len(research_results) > 0:
                    logger.error(f"[{self.name}]   - Sample research results (first 3):")
                    for i, result in enumerate(research_results[:3], 1):
                        if isinstance(result, dict):
                            title = result.get('title', result.get('name', 'N/A'))[:60]
                            logger.error(f"[{self.name}]     {i}. {title}")
            
            state['final_report'] = None
            state['current_agent'] = self.name
            state['report_failed'] = True
            state['error'] = error_msg
            return state
        
        memory = self.context.shared_memory
        
        # Read verified results from state or shared memory
        verified_results = state.get('verified_results', [])
        if not verified_results:
            verified_results = memory.read(
                key=f"verified_{state['session_id']}",
                scope=MemoryScope.SESSION,
                session_id=state['session_id']
            ) or []
        
        # SharedResultsManagerì—ì„œ ëª¨ë“  ê³µìœ ëœ ê²€ì¦ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
        if self.context.shared_results_manager:
            all_shared_results = await self.context.shared_results_manager.get_shared_results()
            logger.info(f"[{self.name}] ğŸ” Found {len(all_shared_results)} total shared results from all agents")

            # ê³µìœ  ê²°ê³¼ í†µê³„
            verification_results = [r for r in all_shared_results if isinstance(r.result, dict) and r.result.get('status') == 'verified']
            search_results = [r for r in all_shared_results if not isinstance(r.result, dict) or r.result.get('status') != 'verified']

            logger.info(f"[{self.name}] ğŸ“Š Shared results breakdown: {len(verification_results)} verified, {len(search_results)} search results")

            # ê²€ì¦ëœ ê²°ê³¼ë§Œ í•„í„°ë§í•˜ì—¬ ì¶”ê°€
            added_from_shared = 0
            for shared_result in all_shared_results:
                if isinstance(shared_result.result, dict):
                    # ê²€ì¦ëœ ê²°ê³¼ì¸ ê²½ìš°
                    if shared_result.result.get('status') == 'verified':
                        # ì¤‘ë³µ ì œê±° (URL ê¸°ì¤€)
                        existing_urls = {r.get('url', '') for r in verified_results if isinstance(r, dict)}
                        result_url = shared_result.result.get('url', '')
                        if result_url and result_url not in existing_urls:
                            verified_results.append(shared_result.result)
                            added_from_shared += 1
                            logger.info(f"[{self.name}] â• Added shared verified result from agent {shared_result.agent_id}: {shared_result.result.get('title', '')[:50]}...")

            logger.info(f"[{self.name}] ğŸ“¥ Added {added_from_shared} verified results from shared agent communications")
            logger.info(f"[{self.name}] ğŸ¤ Agent communication: Incorporated results from agents: {list(set(r.agent_id for r in all_shared_results))}")
        
        # ê²€ì¦ ìš”ì•½ ê°€ì ¸ì˜¤ê¸° (VerifierAgentì—ì„œ ì „ë‹¬ëœ ì •ë³´)
        verification_summary = state.get('verification_summary', {})
        if not verification_summary:
            verification_summary = memory.read(
                key=f"verification_summary_{state['session_id']}",
                scope=MemoryScope.SESSION,
                session_id=state['session_id']
            ) or {}
        
        logger.info(f"[{self.name}] Found {len(verified_results)} verified results for report generation (including shared results)")
        
        # ê²€ì¦ ìš”ì•½ ì •ë³´ ë¡œê¹…
        if verification_summary:
            logger.info(f"[{self.name}] ğŸ“Š Verification Summary received:")
            logger.info(f"[{self.name}]   - Total verified: {verification_summary.get('total_verified', 0)}")
            logger.info(f"[{self.name}]   - High confidence: {verification_summary.get('high_confidence_count', 0)}")
            logger.info(f"[{self.name}]   - Low confidence: {verification_summary.get('low_confidence_count', 0)}")
            logger.info(f"[{self.name}]   - Additional research needed: {verification_summary.get('additional_research_needed_count', 0)}")
        
        if not verified_results or len(verified_results) == 0:
            # Fallback ì œê±° - ëª…í™•í•œ ì—ëŸ¬ë§Œ ë°˜í™˜
            error_msg = "ë³´ê³ ì„œ ìƒì„± ì‹¤íŒ¨: ê²€ì¦ëœ ì—°êµ¬ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."
            logger.error(f"[{self.name}] âŒ {error_msg}")
            state['final_report'] = None
            state['current_agent'] = self.name
            state['report_failed'] = True
            state['error'] = error_msg
            return state
        
        # ì‹¤ì œ ê²°ê³¼ê°€ ìˆëŠ” ê²½ìš° LLMìœ¼ë¡œ ë³´ê³ ì„œ ìƒì„±
        logger.info(f"[{self.name}] Generating report with LLM from {len(verified_results)} verified results...")
        
        # ê²€ì¦ëœ ê²°ê³¼ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ (full_content ìš°ì„  ì‚¬ìš©)
        verified_text = ""
        for i, result in enumerate(verified_results, 1):
            if isinstance(result, dict):
                title = result.get('title', '')
                url = result.get('url', '')
                
                # full_contentê°€ ìˆìœ¼ë©´ ìš°ì„  ì‚¬ìš©, ì—†ìœ¼ë©´ snippet ì‚¬ìš©
                content = result.get('full_content', '')
                if not content:
                    content = result.get('snippet', '')
                
                # ë‚ ì§œ ì •ë³´ ì¶”ê°€
                published_date = result.get('published_date', '')
                date_str = ""
                if published_date:
                    try:
                        date_obj = datetime.fromisoformat(published_date.replace('Z', '+00:00'))
                        date_str = f" (ë°œí–‰ì¼: {date_obj.strftime('%Y-%m-%d')})"
                    except:
                        date_str = f" (ë°œí–‰ì¼: {published_date[:10]})"
                
                # ê²€í†  ì •ë³´ ì¶”ê°€
                review = result.get('review', {})
                review_str = ""
                if review:
                    relevance = review.get('relevance_score', 'N/A')
                    recency = review.get('recency', 'N/A')
                    reliability = review.get('reliability', 'N/A')
                    review_str = f" [ê´€ë ¨ì„±: {relevance}/10, ìµœì‹ ì„±: {recency}, ì‹ ë¢°ë„: {reliability}]"
                
                verified_text += f"\n--- ì¶œì²˜ {i}: {title}{date_str}{review_str} ---\n"
                verified_text += f"URL: {url}\n"
                verified_text += f"ë‚´ìš©:\n{content[:10000] if len(content) > 10000 else content}\n"  # ìµœëŒ€ 10000ì
            else:
                verified_text += f"\n--- ì¶œì²˜ {i} ---\n{str(result)}\n"
        
        # Agent ë…¼ë°• ê²°ê³¼ ìˆ˜ì§‘ ë° ì¢…í•©
        agent_debates_summary = ""
        if state.get('agent_debates'):
            debates = state['agent_debates']
            logger.info(f"[{self.name}] ğŸ’¬ Collecting agent debate results for synthesis...")
            
            # Verifier ë…¼ë°• ê²°ê³¼
            if debates.get('verifier_debates'):
                verifier_debates = debates['verifier_debates']
                agent_debates_summary += "\n\n=== Verifier Agent ë…¼ë°• ê²°ê³¼ ===\n"
                for i, debate in enumerate(verifier_debates, 1):
                    agent_debates_summary += f"\n[ë…¼ë°• {i}] Agent: {debate.get('agent_id', 'unknown')}\n"
                    agent_debates_summary += f"ì¼ê´€ì„±: {debate.get('consistency_check', 'unknown')}\n"
                    agent_debates_summary += f"ë…¼ë¦¬ì  ì˜¬ë°”ë¦„: {debate.get('logical_validity', 'unknown')}\n"
                    agent_debates_summary += f"ë…¼ë°• ë‚´ìš©: {debate.get('message', '')[:500]}\n"
            
            # Verifier Peer ë…¼ë°• ê²°ê³¼
            if debates.get('verifier_peer_debates'):
                peer_debates = debates['verifier_peer_debates']
                agent_debates_summary += "\n\n=== Verifier Agent ê°„ ë…¼ë°• ê²°ê³¼ ===\n"
                for i, debate in enumerate(peer_debates, 1):
                    agent_debates_summary += f"\n[ë…¼ë°• {i}] Agent: {debate.get('agent_id', 'unknown')}\n"
                    agent_debates_summary += f"ì¼ê´€ì„±: {debate.get('consistency_check', 'unknown')}\n"
                    agent_debates_summary += f"ë…¼ë¦¬ì  ì˜¬ë°”ë¦„: {debate.get('logical_validity', 'unknown')}\n"
                    agent_debates_summary += f"ë…¼ë°• ë‚´ìš©: {debate.get('message', '')[:500]}\n"
            
            # Evaluation ë…¼ë°• ê²°ê³¼ (stateì—ì„œ ê°€ì ¸ì˜¤ê¸°)
            evaluation_result = state.get('evaluation_result')
            if evaluation_result and evaluation_result.get('evaluation_debates'):
                eval_debates = evaluation_result['evaluation_debates']
                agent_debates_summary += "\n\n=== Evaluator Agent ë…¼ë°• ê²°ê³¼ ===\n"
                for i, debate in enumerate(eval_debates, 1):
                    agent_debates_summary += f"\n[ë…¼ë°• {i}] Agent: {debate.get('agent_id', 'unknown')}\n"
                    agent_debates_summary += f"ì¼ê´€ì„±: {debate.get('consistency_check', 'unknown')}\n"
                    agent_debates_summary += f"ë…¼ë¦¬ì  ì˜¬ë°”ë¦„: {debate.get('logical_validity', 'unknown')}\n"
                    agent_debates_summary += f"ë…¼ë°• ë‚´ìš©: {debate.get('message', '')[:500]}\n"
            
            # Discussion Managerì—ì„œ ëª¨ë“  ë…¼ë°• ê°€ì ¸ì˜¤ê¸°
            if self.context.discussion_manager:
                try:
                    all_discussions = await self.context.discussion_manager.get_discussion_summary()
                    if all_discussions.get('topics'):
                        agent_debates_summary += "\n\n=== ì „ì²´ ë…¼ë°• ìš”ì•½ ===\n"
                        for topic, info in all_discussions['topics'].items():
                            agent_debates_summary += f"\nì£¼ì œ: {topic}\n"
                            agent_debates_summary += f"ì°¸ì—¬ Agent: {', '.join(info.get('participating_agents', []))}\n"
                            agent_debates_summary += f"ë…¼ë°• ë©”ì‹œì§€ ìˆ˜: {info.get('message_count', 0)}\n"
                except Exception as e:
                    logger.warning(f"[{self.name}] Failed to get discussion summary: {e}")
        
        # í˜„ì¬ ì‹œê°„ ê°€ì ¸ì˜¤ê¸° (ëª¨ë“ˆ ë ˆë²¨ import ì‚¬ìš©)
        current_time = datetime.now()
        current_date_str = current_time.strftime('%Yë…„ %mì›” %dì¼')
        current_datetime_str = current_time.strftime('%Y-%m-%d %H:%M:%S')
        
        # LLMìœ¼ë¡œ ì‚¬ìš©ì ìš”ì²­ì— ë§ëŠ” í˜•ì‹ìœ¼ë¡œ ìƒì„±
        from src.core.llm_manager import execute_llm_task, TaskType
        
        # ê²€ì¦ ìš”ì•½ ì •ë³´ë¥¼ í”„ë¡¬í”„íŠ¸ì— í¬í•¨
        verification_summary_text = ""
        if verification_summary:
            verification_summary_text = f"""
**ê²€ì¦ ìš”ì•½ ì •ë³´ (VerifierAgentì—ì„œ ì „ë‹¬):**
- ì´ ê²€ì¦ëœ ê²°ê³¼: {verification_summary.get('total_verified', 0)}ê°œ
- ë†’ì€ ì‹ ë¢°ë„: {verification_summary.get('high_confidence_count', 0)}ê°œ
- ì¤‘ê°„ ì‹ ë¢°ë„: {verification_summary.get('medium_confidence_count', 0)}ê°œ
- ë‚®ì€ ì‹ ë¢°ë„: {verification_summary.get('low_confidence_count', 0)}ê°œ
- í‰ê·  ì‹ ë¢°ë„: {verification_summary.get('average_confidence', 0.0):.2f}
- ë¶ˆí™•ì‹¤ì„± ì´ìŠˆ: {verification_summary.get('uncertainty_issues_count', 0)}ê°œ
- ì¶”ê°€ ì¡°ì‚¬ í•„ìš”: {verification_summary.get('additional_research_needed_count', 0)}ê°œ

"""
            if verification_summary.get('low_confidence_topics'):
                verification_summary_text += "\n**ë‚®ì€ ì‹ ë¢°ë„ ì£¼ì œ (ì£¼ì˜ í•„ìš”):**\n"
                for topic in verification_summary['low_confidence_topics']:
                    verification_summary_text += f"- {topic.get('title', '')}: {topic.get('reason', '')} (ì‹ ë¢°ë„: {topic.get('confidence', 0.0):.2f})\n"
            
            if verification_summary.get('additional_research_topics'):
                verification_summary_text += "\n**ì¶”ê°€ ì¡°ì‚¬ í•„ìš”í•œ ì£¼ì œ:**\n"
                for topic in verification_summary['additional_research_topics']:
                    verification_summary_text += f"- {topic.get('topic', '')}: {topic.get('reason', '')}\n"
        
        # ì‚¬ìš©ì ìš”ì²­ì„ ê·¸ëŒ€ë¡œ ì „ë‹¬ - LLMì´ í˜•ì‹ì„ ê²°ì •í•˜ë„ë¡
        generation_prompt = f"""ì‚¬ìš©ì ìš”ì²­: {state['user_query']}

í˜„ì¬ ì‹œê°: {current_datetime_str} (ëª¨ë“  ì‹œì  ê¸°ì¤€ì€ ì´ ì‹œê°ì„ ë”°ë¦„)

ê²€ì¦ëœ ì—°êµ¬ ê²°ê³¼ (ì‹¤ì œ ì›¹ í˜ì´ì§€ ì „ì²´ ë‚´ìš© í¬í•¨):
{verified_text}

{verification_summary_text}

**Agent ë…¼ë°• ê²°ê³¼ (ëª¨ë“  Agentë“¤ì˜ ë…¼ë°•ì„ í†µí•œ ì¼ê´€ì„± ë° ë…¼ë¦¬ì  ì˜¬ë°”ë¦„ ê²€ì¦):**
{agent_debates_summary if agent_debates_summary else "ë…¼ë°• ê²°ê³¼ ì—†ìŒ - Executor ê²°ê³¼ê°€ ì§ì ‘ ì‚¬ìš©ë¨"}

âš ï¸ **ê¹Šì´ ìˆëŠ” ë¶„ì„ê³¼ ì‚¬ê³ ë¥¼ í†µí•œ ë³´ê³ ì„œ ì‘ì„± í•„ìˆ˜**
âš ï¸ **ë¶ˆí™•ì‹¤ì„± ëª…ì‹œ í•„ìˆ˜**: ë‚®ì€ ì‹ ë¢°ë„ ì •ë³´ë‚˜ ë¶ˆí™•ì‹¤í•œ ë¶€ë¶„ì€ ë°˜ë“œì‹œ ëª…ì‹œí•˜ì„¸ìš”
âš ï¸ **ê·¼ê±° ì—†ëŠ” í™•ì‹  ê¸ˆì§€**: í™•ì‹¤í•˜ì§€ ì•Šì€ ì •ë³´ëŠ” "~ë¡œ ë³´ì¸ë‹¤", "~ì¼ ê°€ëŠ¥ì„±ì´ ìˆë‹¤" ë“±ìœ¼ë¡œ í‘œí˜„í•˜ì„¸ìš”

**DEEP ANALYSIS REQUIREMENTS - ë°˜ë“œì‹œ í¬í•¨í•´ì•¼ í•  ê¹Šì´ ìˆëŠ” ì‚¬ê³ :**

1. **í˜„ì¬ ìƒíƒœ ë¶„ì„ (Current State Analysis)**:
   - í˜„ì¬ ìƒí™©ì€ ë¬´ì—‡ì¸ê°€? ìš°ë¦¬ê°€ ì•Œê³  ìˆëŠ” ê²ƒì€ ë¬´ì—‡ì¸ê°€?
   - ì£¼ìš” ì‚¬ì‹¤, íŠ¸ë Œë“œ, ìµœê·¼ ë°œì „ ìƒí™©ì€ ë¬´ì—‡ì¸ê°€?
   - ë§¥ë½ê³¼ ë°°ê²½ì€ ë¬´ì—‡ì¸ê°€?
   - ì´ ì •ë³´ê°€ ì˜ë¯¸í•˜ëŠ” ë°”ëŠ” ë¬´ì—‡ì¸ê°€?

2. **íŒ¨í„´ ì¸ì‹ ë° ì—°ê²° (Pattern Recognition & Connections)**:
   - ì—¬ëŸ¬ ì¶œì²˜ì—ì„œ ë‚˜íƒ€ë‚˜ëŠ” íŒ¨í„´, íŠ¸ë Œë“œ, ê´€ê³„ëŠ” ë¬´ì—‡ì¸ê°€?
   - ì–´ë–¤ ì—°ê²°ê³ ë¦¬ì™€ ìƒê´€ê´€ê³„ê°€ ìˆëŠ”ê°€?
   - ì—­ì‚¬ì  ë§¥ë½ì´ë‚˜ ì„ ë¡€ëŠ” ë¬´ì—‡ì¸ê°€?
   - ë‹¤ë¥¸ ë¶„ì•¼ë‚˜ ì£¼ì œì™€ì˜ ì—°ê²°ì€ ë¬´ì—‡ì¸ê°€?

3. **ë¹„íŒì  í†µì°° (Critical Insights)**:
   - ë‹¨ìˆœí•œ ì‚¬ì‹¤ ë‚˜ì—´ì´ ì•„ë‹Œ, ê¹Šì€ í†µì°°ê³¼ í•¨ì˜ë¥¼ ì œê³µí•˜ì„¸ìš”
   - ì´ ì •ë³´ì˜ ë” ê¹Šì€ ì˜ë¯¸ëŠ” ë¬´ì—‡ì¸ê°€?
   - ì–´ë–¤ ê´€ì ë“¤ì´ ìˆê³ , ì–´ë–¤ ê²ƒì´ ëˆ„ë½ë˜ì—ˆëŠ”ê°€?
   - ì–´ë–¤ ê°€ì •ì´ ìˆê³ , ê·¸ê²ƒë“¤ì´ ìœ íš¨í•œê°€?

4. **ì¢…í•©ì  ì´í•´ (Comprehensive Understanding)**:
   - ì „ì²´ì ì¸ ê·¸ë¦¼ì„ ê·¸ë¦¬ì„¸ìš” - ê°œë³„ ì‚¬ì‹¤ì´ ì•„ë‹Œ ì¢…í•©ì  ì´í•´
   - ì„œë¡œ ë‹¤ë¥¸ ì •ë³´ë“¤ì´ ì–´ë–»ê²Œ ì—°ê²°ë˜ëŠ”ê°€?
   - ì–´ë–¤ ì§ˆë¬¸ì´ ë‚¨ì•„ìˆëŠ”ê°€? ì–´ë–¤ ì •ë³´ê°€ ë¶€ì¡±í•œê°€?

5. **Agent ë…¼ë°• ê²°ê³¼ ì¢…í•© (Agent Debate Synthesis)**:
   - ìœ„ì˜ "Agent ë…¼ë°• ê²°ê³¼"ë¥¼ ë°˜ë“œì‹œ ì°¸ê³ í•˜ì„¸ìš”
   - ëª¨ë“  Agentë“¤ì˜ ë…¼ë°•ì„ í†µí•´ ê²€ì¦ëœ ì¼ê´€ì„±ê³¼ ë…¼ë¦¬ì  ì˜¬ë°”ë¦„ì„ ë°˜ì˜í•˜ì„¸ìš”
   - ë…¼ë°•ì—ì„œ í•©ì˜ëœ ë¶€ë¶„ê³¼ ë…¼ìŸì´ ìˆëŠ” ë¶€ë¶„ì„ ëª…í™•íˆ êµ¬ë¶„í•˜ì„¸ìš”
   - ë…¼ë°• ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìµœì¢… ê²°ë¡ ì„ ë„ì¶œí•˜ì„¸ìš”
   - ë…¼ë°•ì—ì„œ ì§€ì ëœ ë¬¸ì œì ì´ë‚˜ ê°œì„ ì‚¬í•­ì„ ë°˜ì˜í•˜ì„¸ìš”

**ì¶œì²˜ ì¸ìš© ìš”êµ¬ì‚¬í•­ (í•„ìˆ˜):**

âš ï¸ **ëª¨ë“  ì •ë³´ëŠ” ë°˜ë“œì‹œ ì¶œì²˜ë¥¼ ëª…ì‹œí•´ì•¼ í•©ë‹ˆë‹¤:**

1. **ìˆ«ì/í†µê³„ ì¸ìš©**: ëª¨ë“  ìˆ«ì, í†µê³„, ìˆ˜ì¹˜ëŠ” ë°˜ë“œì‹œ ì¶œì²˜ë¥¼ ëª…ì‹œí•˜ì„¸ìš”
   - ì˜ˆ: "2025ë…„ ìƒë°˜ê¸° ë§¤ì¶œ 3.2ì¡° ì›(ì¶œì²˜ 1)" ë˜ëŠ” "ë§¤ì¶œ 3.2ì¡° ì›[1]"
   - ì¶œì²˜ ë²ˆí˜¸ëŠ” ì•„ë˜ ì°¸ê³  ë¬¸í—Œ ì„¹ì…˜ê³¼ ì¼ì¹˜í•´ì•¼ í•©ë‹ˆë‹¤

2. **ì£¼ì¥(Claims) ì¸ìš©**: ëª¨ë“  ì£¼ì¥, ì‚¬ì‹¤, ì£¼ì¥ì€ ë°˜ë“œì‹œ ì¶œì²˜ë¥¼ ëª…ì‹œí•˜ì„¸ìš”
   - ì˜ˆ: "í•œí™”ì‹œìŠ¤í…œì€ ë°©ì‚° ë¶„ì•¼ì˜ í•µì‹¬ ê¸°ì—…ì´ë‹¤(ì¶œì²˜ 1, ì¶œì²˜ 2)" ë˜ëŠ” "í•µì‹¬ ê¸°ì—…ì´ë‹¤[1,2]"

3. **ë‚ ì§œ/ì‹œì  ì¸ìš©**: ë‚ ì§œ, ì‹œì  ì •ë³´ë„ ì¶œì²˜ë¥¼ ëª…ì‹œí•˜ì„¸ìš”
   - ì˜ˆ: "2025ë…„ 11ì›” ë°œí‘œ(ì¶œì²˜ 3)" ë˜ëŠ” "2025ë…„ 11ì›”[3]"

4. **ì°¸ê³  ë¬¸í—Œ ì„¹ì…˜**: ë³´ê³ ì„œ ëì— ë°˜ë“œì‹œ ì°¸ê³  ë¬¸í—Œ ì„¹ì…˜ì„ í¬í•¨í•˜ì„¸ìš”
   - ê° ì¶œì²˜ëŠ” ë²ˆí˜¸ì™€ í•¨ê»˜ ì œëª©, URLì„ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤
   - ë³¸ë¬¸ì—ì„œ ì¸ìš©í•œ ëª¨ë“  ì¶œì²˜ê°€ ì°¸ê³  ë¬¸í—Œì— í¬í•¨ë˜ì–´ì•¼ í•©ë‹ˆë‹¤
   - ì°¸ê³  ë¬¸í—Œì— ìˆëŠ” ì¶œì²˜ëŠ” ë³¸ë¬¸ì—ì„œ ì¸ìš©ë˜ì–´ì•¼ í•©ë‹ˆë‹¤

5. **ì¶œì²˜ ì—†ëŠ” ì •ë³´**: ì¶œì²˜ë¥¼ í™•ì¸í•  ìˆ˜ ì—†ëŠ” ì •ë³´ëŠ” ë¶ˆí™•ì‹¤ì„±ì„ í‘œì‹œí•˜ì„¸ìš”
   - ì˜ˆ: "ì¶”ì •", "ì˜ˆìƒ", "~ë¡œ ì•Œë ¤ì§" ë“±ì˜ í‘œí˜„ ì‚¬ìš©

6. **ì‹ ë¢°ë„ ê¸°ë°˜ í‘œí˜„** (ê²€ì¦ ìš”ì•½ ì •ë³´ ë°˜ì˜):
   - ë†’ì€ ì‹ ë¢°ë„ ì •ë³´ (ì‹ ë¢°ë„ 0.8 ì´ìƒ): í™•ì‹¤í•œ í‘œí˜„ ì‚¬ìš© ê°€ëŠ¥
   - ì¤‘ê°„ ì‹ ë¢°ë„ ì •ë³´ (ì‹ ë¢°ë„ 0.6-0.8): "~ë¡œ ë³´ì¸ë‹¤", "~ì¼ ê°€ëŠ¥ì„±ì´ ìˆë‹¤" ë“±ìœ¼ë¡œ í‘œí˜„
   - ë‚®ì€ ì‹ ë¢°ë„ ì •ë³´ (ì‹ ë¢°ë„ 0.6 ë¯¸ë§Œ): "~ë¼ê³  ì£¼ì¥ë˜ì§€ë§Œ", "í™•ì¸ í•„ìš”", "ì¶”ê°€ ì¡°ì‚¬ í•„ìš”" ë“±ìœ¼ë¡œ ëª…ì‹œ
   - ë¶ˆí™•ì‹¤ì„± ì´ìŠˆê°€ ìˆëŠ” ì •ë³´: "ë¶ˆí™•ì‹¤", "ì¶”ê°€ ê²€ì¦ í•„ìš”" ë“±ìœ¼ë¡œ ëª…ì‹œ

7. **ë¶ˆí™•ì‹¤ì„± ëª…ì‹œ**: ê²€ì¦ ìš”ì•½ì—ì„œ ì–¸ê¸‰ëœ ë¶ˆí™•ì‹¤ì„± ì´ìŠˆëŠ” ë°˜ë“œì‹œ ë³´ê³ ì„œì— ëª…ì‹œí•˜ì„¸ìš”
   - ë‚®ì€ ì‹ ë¢°ë„ ì£¼ì œëŠ” "ì£¼ì˜: ì‹ ë¢°ë„ ë‚®ìŒ" ë“±ìœ¼ë¡œ í‘œì‹œ
   - ë¶ˆí™•ì‹¤í•œ ë¶€ë¶„ì€ "~ë¡œ ì•Œë ¤ì ¸ ìˆìœ¼ë‚˜ í™•ì¸ í•„ìš”" ë“±ìœ¼ë¡œ í‘œí˜„

8. **ì¶”ê°€ ì¡°ì‚¬ í•„ìš”ì„±**: ê²€ì¦ ìš”ì•½ì—ì„œ ì–¸ê¸‰ëœ ì¶”ê°€ ì¡°ì‚¬ í•„ìš”í•œ ë¶€ë¶„ì€ ë³´ê³ ì„œì— í¬í•¨í•˜ì„¸ìš”
   - "ì¶”ê°€ ì¡°ì‚¬ê°€ í•„ìš”í•œ ì˜ì—­" ì„¹ì…˜ì— í¬í•¨
   - ë˜ëŠ” í•´ë‹¹ ë¶€ë¶„ì—ì„œ "ì¶”ê°€ ì¡°ì‚¬ í•„ìš”"ë¡œ ëª…ì‹œ

9. **ê´€ë ¨ì„± í™•ì¸**: ì°¸ê³  ë¬¸í—Œì— í¬í•¨í•  ì¶œì²˜ëŠ” ë°˜ë“œì‹œ ì¿¼ë¦¬ì™€ ê´€ë ¨ì´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤
   - ì—”ë¹„ë””ì•„ ë¶„ì„ì¸ë° ë¶€ë™ì‚° ê´€ë ¨ ì¶œì²˜ë¥¼ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”
   - ì¿¼ë¦¬ì™€ ë¬´ê´€í•œ ì¶œì²˜ëŠ” ì œì™¸í•˜ì„¸ìš”
   - ë³¸ë¬¸ì—ì„œ ì¸ìš©í•˜ì§€ ì•Šì€ ì¶œì²˜ëŠ” ì°¸ê³  ë¬¸í—Œì— í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”

**ë³´ê³ ì„œ êµ¬ì¡° (ê¹Šì´ ìˆëŠ” ì‚¬ê³  ë°˜ì˜):**

1. **í˜„ì¬ ìƒíƒœ ì„¹ì…˜**: í˜„ì¬ ìƒíƒœ, ë§¥ë½, ì•Œë ¤ì§„ ì •ë³´ì— ëŒ€í•œ ëª…í™•í•œ í‰ê°€ (ëª¨ë“  ì •ë³´ì— ì¶œì²˜ ì¸ìš©)
2. **ê¹Šì´ ìˆëŠ” ë¶„ì„**: íŒ¨í„´, ì—°ê²°, í•¨ì˜ë¥¼ í¬í•¨í•œ ì‹¬ì¸µ ë¶„ì„ (ëª¨ë“  ì •ë³´ì— ì¶œì²˜ ì¸ìš©)
3. **ë¹„íŒì  í†µì°°**: ê¹Šì€ ì‚¬ê³ ë¥¼ í†µí•´ ë„ì¶œëœ ì˜ë¯¸ ìˆëŠ” í†µì°°
4. **Agent ë…¼ë°• ì¢…í•©**: ëª¨ë“  Agentë“¤ì˜ ë…¼ë°• ê²°ê³¼ë¥¼ ì¢…í•©í•˜ì—¬ ì¼ê´€ì„±ê³¼ ë…¼ë¦¬ì  ì˜¬ë°”ë¦„ì´ ê²€ì¦ëœ ë‚´ìš© ë°˜ì˜
5. **ì¢…í•©ì  ì´í•´**: ê¹Šì€ ì´í•´ë¥¼ ë³´ì—¬ì£¼ëŠ” ì™„ì „í•œ ê·¸ë¦¼ (ë…¼ë°• ê²°ê³¼ ë°˜ì˜)
6. **ì˜ë¯¸ ìˆëŠ” ê²°ë¡ **: í‘œë©´ì  ì‚¬ì‹¤ì´ ì•„ë‹Œ ê¹Šì€ ë¶„ì„ê³¼ ë…¼ë°• ê²€ì¦ì— ê¸°ë°˜í•œ ê²°ë¡ 

âš ï¸ **ì¤‘ìš” ì§€ì¹¨:**
1. **ìµœì‹  ì •ë³´ ìš°ì„ **: ë‚ ì§œê°€ í‘œì‹œëœ ì¶œì²˜ ì¤‘ ê°€ì¥ ìµœì‹  ì •ë³´ë¥¼ ìš°ì„ ì ìœ¼ë¡œ ì‚¬ìš©í•˜ì„¸ìš”.
2. **ì „ì²´ ë‚´ìš© í™œìš©**: ê° ì¶œì²˜ì˜ ì „ì²´ ë‚´ìš©(full_content)ì„ ì°¸ê³ í•˜ì—¬ ì •í™•í•˜ê³  ìƒì„¸í•œ ì •ë³´ë¥¼ ì œê³µí•˜ì„¸ìš”.
3. **ë‹¤ì–‘í•œ ì¶œì²˜ ì¢…í•©**: ì—¬ëŸ¬ ì¶œì²˜ì˜ ì •ë³´ë¥¼ ì¢…í•©í•˜ì—¬ ê· í˜• ì¡íŒ ë¶„ì„ì„ ì œê³µí•˜ì„¸ìš”.
4. **í˜„ì¬ ì‹œê°„ ê¸°ì¤€**: ë³´ê³ ì„œ ì‘ì„±ì¼ì€ {current_date_str} ({current_datetime_str})ë¡œ ì„¤ì •í•˜ì„¸ìš”.
5. **ìµœì‹  ë™í–¥ ë°˜ì˜**: ìµœì‹  ë‰´ìŠ¤ë‚˜ ë™í–¥ì´ ìˆë‹¤ë©´ ë°˜ë“œì‹œ í¬í•¨í•˜ì„¸ìš”.
6. **ê¹Šì´ ìˆëŠ” ì‚¬ê³ **: ë‹¨ìˆœíˆ ì •ë³´ë¥¼ ë‚˜ì—´í•˜ì§€ ë§ê³ , ê¹Šì´ ìˆëŠ” ë¶„ì„, íŒ¨í„´ ì¸ì‹, í†µì°°ì„ ì œê³µí•˜ì„¸ìš”.

**ì ˆëŒ€ í•˜ì§€ ë§ì•„ì•¼ í•  ê²ƒ:**
- ë‹¨ìˆœíˆ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë‚˜ì—´í•˜ëŠ” ê²ƒ
- í˜„ì¬ ìƒíƒœë‚˜ ë§¥ë½ ì—†ì´ ì •ë³´ë§Œ ì œê³µí•˜ëŠ” ê²ƒ
- íŒ¨í„´ì´ë‚˜ ì—°ê²°ê³ ë¦¬ë¥¼ ì°¾ì§€ ì•ŠëŠ” ê²ƒ
- ê¹Šì´ ìˆëŠ” í†µì°° ì—†ì´ í‘œë©´ì  ì‚¬ì‹¤ë§Œ ë‚˜ì—´í•˜ëŠ” ê²ƒ

ì‚¬ìš©ìì˜ ìš”ì²­ì„ ì •í™•íˆ ì´í•´í•˜ê³ , ìš”ì²­í•œ í˜•ì‹ì— ë§ê²Œ **ê¹Šì´ ìˆëŠ” ë¶„ì„ê³¼ í†µì°°**ì„ í¬í•¨í•œ ê²°ê³¼ë¥¼ ìƒì„±í•˜ì„¸ìš”.
- ë³´ê³ ì„œë¥¼ ìš”ì²­í–ˆë‹¤ë©´ ë³´ê³ ì„œ í˜•ì‹ìœ¼ë¡œ (ì‘ì„±ì¼: {current_date_str} í¬í•¨, í˜„ì¬ ìƒíƒœ ë¶„ì„ í¬í•¨)
- ì½”ë“œë¥¼ ìš”ì²­í–ˆë‹¤ë©´ ì‹¤í–‰ ê°€ëŠ¥í•œ ì½”ë“œë¡œ
- ë¬¸ì„œë¥¼ ìš”ì²­í–ˆë‹¤ë©´ ë¬¸ì„œ í˜•ì‹ìœ¼ë¡œ

ìš”ì²­ëœ í˜•ì‹ì— ë§ê²Œ **ê¹Šì´ ìˆëŠ” ì‚¬ê³ ì™€ ë¶„ì„**ì„ í¬í•¨í•œ ì™„ì „í•˜ê³  ì‹¤í–‰ ê°€ëŠ¥í•œ ê²°ê³¼ë¥¼ ìƒì„±í•˜ì„¸ìš”."""

        try:
            report_result = await execute_llm_task(
                prompt=generation_prompt,
                task_type=TaskType.GENERATION,
                model_name=None,
                system_message=None
            )
            
            report = report_result.content or f"# Report: {state['user_query']}\n\nNo report generated."
            
            # Safety filter ì°¨ë‹¨ í™•ì¸ - Fallback ì œê±°, ëª…í™•í•œ ì˜¤ë¥˜ ë°˜í™˜
            if "blocked by safety" in report.lower() or "content blocked" in report.lower() or len(report) < 100:
                error_msg = "ë³´ê³ ì„œ ìƒì„± ì‹¤íŒ¨: Safety filterì— ì˜í•´ ì°¨ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤. í”„ë¡¬í”„íŠ¸ë¥¼ ìˆ˜ì •í•˜ê±°ë‚˜ ë‹¤ë¥¸ ëª¨ë¸ì„ ì‚¬ìš©í•´ì£¼ì„¸ìš”."
                logger.error(f"[{self.name}] âŒ {error_msg}")
                state['final_report'] = None
                state['report_failed'] = True
                state['error'] = error_msg
                state['current_agent'] = self.name
                return state
            else:
                logger.info(f"[{self.name}] âœ… Report generated: {len(report)} characters")
            
            # ë³´ê³ ì„œ ì™„ì„±ë„ ê²€ì¦ ë° ë³´ì™„
            max_retry_attempts = 3
            retry_count = 0
            
            while retry_count < max_retry_attempts:
                completeness_check = await self._validate_report_completeness(
                    report, state['user_query'], verified_text
                )
                
                if completeness_check['is_complete']:
                    logger.info(f"[{self.name}] âœ… Report completeness validated: {completeness_check['completeness_score']:.2f}")
                    break
                
                logger.warning(f"[{self.name}] âš ï¸ Report incomplete (score: {completeness_check['completeness_score']:.2f}): {completeness_check['issues']}")
                
                # ë¯¸ì™„ì„± ë¶€ë¶„ ë³´ì™„
                if retry_count < max_retry_attempts - 1:
                    report = await self._complete_incomplete_report(
                        report, completeness_check, state['user_query'], verified_text, agent_debates_summary
                    )
                    retry_count += 1
                    logger.info(f"[{self.name}] ğŸ”„ Retrying report completion (attempt {retry_count}/{max_retry_attempts})")
                else:
                    # ìµœì¢… ì‹œë„ ì‹¤íŒ¨ ì‹œ ê²½ê³ ë§Œ ë¡œê¹…í•˜ê³  í˜„ì¬ ë³´ê³ ì„œ ì‚¬ìš©
                    logger.warning(f"[{self.name}] âš ï¸ Report completion failed after {max_retry_attempts} attempts. Using current report with warnings.")
                    break
            
            # Council í™œì„±í™” í™•ì¸ ë° ì ìš© (ìµœì¢… ë³´ê³ ì„œ ìƒì„± ì‹œ - ê¸°ë³¸ í™œì„±í™”)
            use_council = state.get('use_council', None)  # ìˆ˜ë™ í™œì„±í™” ì˜µì…˜
            if use_council is None:
                # ìë™ í™œì„±í™” íŒë‹¨ (ê¸°ë³¸ í™œì„±í™”)
                from src.core.council_activator import get_council_activator
                activator = get_council_activator()
                
                activation_decision = activator.should_activate(
                    process_type='synthesis',
                    query=state['user_query'],
                    context={'important_conclusion': True}  # ìµœì¢… ë³´ê³ ì„œëŠ” í•­ìƒ ì¤‘ìš”í•œ ê²°ë¡ 
                )
                use_council = activation_decision.should_activate
                if use_council:
                    logger.info(f"[{self.name}] ğŸ›ï¸ Council auto-activated: {activation_decision.reason}")
            
            # Council ì ìš© (í™œì„±í™”ëœ ê²½ìš°)
            if use_council:
                try:
                    from src.core.llm_council import run_full_council
                    logger.info(f"[{self.name}] ğŸ›ï¸ Running Council review for final report...")
                    
                    # ë³´ê³ ì„œ ìƒ˜í”Œ (ìµœëŒ€ 2000ì)
                    report_sample = report[:2000]
                    
                    council_query = f"""Review the final report and assess its completeness and accuracy. Check for any missing information or potential improvements.

Research Query: {state['user_query']}

Final Report Sample:
{report_sample}

Provide a review with:
1. Completeness assessment
2. Accuracy check
3. Recommendations for improvement"""
                    
                    stage1_results, stage2_results, stage3_result, metadata = await run_full_council(
                        council_query
                    )
                    
                    # Council ê²€í†  ê²°ê³¼
                    review_report = stage3_result.get('response', '')
                    logger.info(f"[{self.name}] âœ… Council review completed.")
                    logger.info(f"[{self.name}] Council aggregate rankings: {metadata.get('aggregate_rankings', [])}")
                    
                    # Council ë©”íƒ€ë°ì´í„°ë¥¼ stateì— ì €ì¥
                    if 'council_metadata' not in state:
                        state['council_metadata'] = {}
                    state['council_metadata']['synthesis'] = {
                        'stage1_results': stage1_results,
                        'stage2_results': stage2_results,
                        'stage3_result': stage3_result,
                        'metadata': metadata,
                        'review_report': review_report
                    }
                    
                    # Council ê²€í†  ê²°ê³¼ë¥¼ ë³´ê³ ì„œì— ì¶”ê°€ (ì„ íƒì )
                    if review_report:
                        report += f"\n\n--- Council Review ---\n{review_report}"
                except Exception as e:
                    logger.warning(f"[{self.name}] Council review failed: {e}. Using original report.")
                    # Council ì‹¤íŒ¨ ì‹œ ì›ë³¸ ë³´ê³ ì„œ ì‚¬ìš© (fallback ì œê±° - ëª…í™•í•œ ë¡œê¹…ë§Œ)
        except Exception as e:
            logger.error(f"[{self.name}] âŒ Report generation failed: {e}")
            # Fallback ì œê±° - ëª…í™•í•œ ì˜¤ë¥˜ ë°˜í™˜
            error_msg = f"ë³´ê³ ì„œ ìƒì„± ì‹¤íŒ¨: {str(e)}"
            state['final_report'] = None
            state['report_failed'] = True
            state['error'] = error_msg
            state['current_agent'] = self.name
            return state
        
        # ìµœì¢… ì™„ì„±ë„ ì¬ê²€ì¦ (ì¢…ë£Œ ì „)
        final_completeness = await self._validate_report_completeness(
            report, state['user_query'], verified_text
        )
        
        if not final_completeness['is_complete']:
            logger.error(f"[{self.name}] âŒ Final report validation failed: {final_completeness['issues']}")
            logger.error(f"[{self.name}] Completeness score: {final_completeness['completeness_score']:.2f}")
            # ì™„ì„±ë„ê°€ ë„ˆë¬´ ë‚®ìœ¼ë©´ ì—ëŸ¬ ë°˜í™˜
            if final_completeness['completeness_score'] < 0.5:
                error_msg = f"ë³´ê³ ì„œ ì™„ì„±ë„ ê²€ì¦ ì‹¤íŒ¨: {', '.join(final_completeness['issues'][:3])}"
                state['final_report'] = None
                state['report_failed'] = True
                state['error'] = error_msg
                state['current_agent'] = self.name
                return state
            else:
                # ì™„ì„±ë„ê°€ ë‚®ì§€ë§Œ ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš° ê²½ê³ ë§Œ
                logger.warning(f"[{self.name}] âš ï¸ Report has completeness issues but will be saved: {final_completeness['issues']}")
        
        # ì¶œì²˜ ì¸ìš© ê²€ì¦ ë° ë³´ì™„
        source_mapping = state.get('source_mapping', {})
        if source_mapping:
            report = self._validate_and_enhance_citations(report, source_mapping, verified_results)
            logger.info(f"[{self.name}] âœ… Citation validation completed")
        
        state['final_report'] = report
        
        # A2UI í˜•ì‹ìœ¼ë¡œë„ ìƒì„± ì‹œë„ (ì„ íƒì )
        try:
            from src.core.a2ui_generator import get_a2ui_generator
            a2ui_generator = get_a2ui_generator()
            a2ui_json = a2ui_generator.generate_research_report_a2ui(
                query=state['user_query'],
                verified_results=verified_results,
                report_text=report
            )
            state['final_report_a2ui'] = a2ui_json
            logger.info(f"[{self.name}] âœ… A2UI í˜•ì‹ ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ")
        except Exception as e:
            logger.debug(f"[{self.name}] A2UI ìƒì„± ì‹¤íŒ¨ (ë¬´ì‹œ): {e}")
            state['final_report_a2ui'] = None
        state['current_agent'] = self.name
        state['report_failed'] = False
        state['report_completeness'] = final_completeness  # ì™„ì„±ë„ ì •ë³´ ì €ì¥
        
        # Write to shared memory
        memory.write(
            key=f"report_{state['session_id']}",
            value=report,
            scope=MemoryScope.SESSION,
            session_id=state['session_id'],
            agent_id=self.name
        )
        
        logger.info(f"[{self.name}] âœ… Report saved to shared memory (completeness: {final_completeness['completeness_score']:.2f})")
        logger.info(f"=" * 80)
        
        return state
    
    async def _validate_report_completeness(
        self,
        report: str,
        user_query: str,
        verified_text: str
    ) -> Dict[str, Any]:
        """
        ë³´ê³ ì„œ ì™„ì„±ë„ ê²€ì¦.
        
        Returns:
            Dict with 'is_complete', 'completeness_score', 'issues'
        """
        from src.core.llm_manager import execute_llm_task, TaskType
        
        validation_prompt = f"""ë‹¤ìŒ ë³´ê³ ì„œì˜ ì™„ì„±ë„ë¥¼ ê²€ì¦í•˜ì„¸ìš”:

ì‚¬ìš©ì ìš”ì²­: {user_query}

ë³´ê³ ì„œ ë‚´ìš©:
{report[:5000]}  # ì²˜ìŒ 5000ìë§Œ ê²€ì¦ìš©ìœ¼ë¡œ ì‚¬ìš©

**ì™„ì„±ë„ ê²€ì¦ ê¸°ì¤€:**

1. **êµ¬ì¡°ì  ì™„ì„±ë„ (Structural Completeness)**:
   - ëª¨ë“  ì„¹ì…˜ì´ ì™„ì„±ë˜ì—ˆëŠ”ê°€? (ì‹œì‘í–ˆì§€ë§Œ ëë‚˜ì§€ ì•Šì€ ì„¹ì…˜ì´ ìˆëŠ”ê°€?)
   - í‘œë‚˜ ë¦¬ìŠ¤íŠ¸ê°€ ì¤‘ê°„ì— ì˜ë ¸ëŠ”ê°€?
   - ë§ˆì§€ë§‰ ë¬¸ì¥ì´ ì™„ì„±ë˜ì—ˆëŠ”ê°€?

2. **ë‚´ìš© ì™„ì„±ë„ (Content Completeness)**:
   - ê° ì„¹ì…˜ì— ì¶©ë¶„í•œ ë‚´ìš©ì´ ìˆëŠ”ê°€?
   - ì‚¬ìš©ì ìš”ì²­ì— ëŒ€í•œ ë‹µë³€ì´ ì™„ì „í•œê°€?
   - ê²°ë¡  ì„¹ì…˜ì´ ìˆëŠ”ê°€?

3. **ë…¼ë¦¬ì  ì™„ì„±ë„ (Logical Completeness)**:
   - ë…¼ë¦¬ì  íë¦„ì´ ì™„ì„±ë˜ì—ˆëŠ”ê°€?
   - ì¤‘ê°„ì— ê°‘ìê¸° ëë‚˜ëŠ” ë¶€ë¶„ì´ ìˆëŠ”ê°€?
   - ë¶ˆì™„ì „í•œ ë¬¸ì¥ì´ë‚˜ í‘œê°€ ìˆëŠ”ê°€?

4. **í˜•ì‹ì  ì™„ì„±ë„ (Format Completeness)**:
   - ë§ˆí¬ë‹¤ìš´ í˜•ì‹ì´ ì˜¬ë°”ë¥¸ê°€?
   - í‘œê°€ ì œëŒ€ë¡œ ë‹«í˜”ëŠ”ê°€?
   - ì½”ë“œ ë¸”ë¡ì´ ì œëŒ€ë¡œ ë‹«í˜”ëŠ”ê°€?

**ê²€ì¦ ê²°ê³¼ë¥¼ ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ë°˜í™˜í•˜ì„¸ìš”:**
{{
    "is_complete": true/false,
    "completeness_score": 0.0-1.0,
    "issues": ["ë¬¸ì œ1", "ë¬¸ì œ2", ...],
    "missing_sections": ["ëˆ„ë½ëœ ì„¹ì…˜1", ...],
    "incomplete_elements": ["ë¶ˆì™„ì „í•œ ìš”ì†Œ1", ...],
    "recommendations": ["ê¶Œì¥ì‚¬í•­1", ...]
}}

ì¤‘ìš”: ë³´ê³ ì„œê°€ ì¤‘ê°„ì— ì˜ë ¸ê±°ë‚˜ ë¶ˆì™„ì „í•œ ê²½ìš° is_completeëŠ” ë°˜ë“œì‹œ falseì—¬ì•¼ í•©ë‹ˆë‹¤."""
        
        try:
            validation_result = await execute_llm_task(
                prompt=validation_prompt,
                task_type=TaskType.VERIFICATION,
                system_message="You are an expert document completeness validator. You must detect any incomplete sections, truncated content, or formatting issues."
            )

            # JSON ì¶”ì¶œ ì‹œë„
            content = validation_result.content
            json_match = re.search(r'\{[\s\S]*\}', content)
            if json_match:
                try:
                    result = json.loads(json_match.group())
                    return {
                        'is_complete': result.get('is_complete', False),
                        'completeness_score': result.get('completeness_score', 0.0),
                        'issues': result.get('issues', []),
                        'missing_sections': result.get('missing_sections', []),
                        'incomplete_elements': result.get('incomplete_elements', []),
                        'recommendations': result.get('recommendations', [])
                    }
                except json.JSONDecodeError:
                    pass
            
            # JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ íœ´ë¦¬ìŠ¤í‹± ê²€ì¦
            return self._heuristic_completeness_check(report, user_query)
            
        except Exception as e:
            logger.warning(f"[{self.name}] Completeness validation failed: {e}. Using heuristic check.")
            return self._heuristic_completeness_check(report, user_query)
    
    def _heuristic_completeness_check(self, report: str, user_query: str) -> Dict[str, Any]:
        """íœ´ë¦¬ìŠ¤í‹± ê¸°ë°˜ ì™„ì„±ë„ ê²€ì¦ (fallback)."""
        issues = []
        score = 1.0
        
        # 1. ì¤‘ê°„ ì˜ë¦¼ ê°ì§€
        if report.endswith('|') or report.endswith('| '):
            issues.append("í‘œê°€ ì¤‘ê°„ì— ì˜ë¦¼")
            score -= 0.3
        
        # 2. ë¶ˆì™„ì „í•œ ë§ˆí¬ë‹¤ìš´ ê°ì§€
        if report.count('```') % 2 != 0:
            issues.append("ì½”ë“œ ë¸”ë¡ì´ ë‹«íˆì§€ ì•ŠìŒ")
            score -= 0.2
        
        # 3. ë§ˆì§€ë§‰ ë¬¸ì¥ ì™„ì„±ë„
        last_sentence = report.strip().split('\n')[-1] if report.strip() else ""
        if last_sentence and not last_sentence.endswith(('.', '!', '?', ':', ')')):
            if len(last_sentence) > 20:  # ì§§ì€ ë¬¸ì¥ì€ ë¬´ì‹œ
                issues.append("ë§ˆì§€ë§‰ ë¬¸ì¥ì´ ë¶ˆì™„ì „í•  ìˆ˜ ìˆìŒ")
                score -= 0.1
        
        # 4. ì„¹ì…˜ ì™„ì„±ë„
        open_sections = report.count('##') - report.count('###')
        if open_sections > 5:  # ë„ˆë¬´ ë§ì€ ì„¹ì…˜ì´ ì—´ë ¤ìˆìœ¼ë©´
            issues.append("ë„ˆë¬´ ë§ì€ ì„¹ì…˜ì´ ì—´ë ¤ìˆìŒ")
            score -= 0.2
        
        # 5. ìµœì†Œ ê¸¸ì´ ê²€ì¦
        if len(report) < 500:
            issues.append("ë³´ê³ ì„œê°€ ë„ˆë¬´ ì§§ìŒ")
            score -= 0.3
        
        # 6. ê²°ë¡  ì„¹ì…˜ í™•ì¸
        if 'ê²°ë¡ ' not in report and 'Conclusion' not in report and '##' in report:
            # ì„¹ì…˜ì´ ìˆì§€ë§Œ ê²°ë¡ ì´ ì—†ëŠ” ê²½ìš°
            issues.append("ê²°ë¡  ì„¹ì…˜ì´ ì—†ì„ ìˆ˜ ìˆìŒ")
            score -= 0.1
        
        # 7. í‘œ ì¤‘ê°„ ì˜ë¦¼ ê°ì§€ (ë” ì •í™•í•œ ê²€ì¦)
        lines = report.split('\n')
        in_table = False
        for i, line in enumerate(lines):
            if '|' in line and line.strip().startswith('|'):
                in_table = True
            elif in_table and line.strip() and '|' not in line:
                # í‘œê°€ ì‹œì‘ë˜ì—ˆëŠ”ë° ê°‘ìê¸° ëë‚¨
                if i < len(lines) - 5:  # ë§ˆì§€ë§‰ 5ì¤„ì´ ì•„ë‹ˆë©´
                    issues.append(f"í‘œê°€ {i+1}ë²ˆì§¸ ì¤„ì—ì„œ ì¤‘ê°„ì— ì˜ë¦¼")
                    score -= 0.2
                    break
        
        score = max(0.0, min(1.0, score))
        
        return {
            'is_complete': score >= 0.7 and len(issues) == 0,
            'completeness_score': score,
            'issues': issues,
            'missing_sections': [],
            'incomplete_elements': issues,
            'recommendations': []
        }
    
    async def _complete_incomplete_report(
        self,
        current_report: str,
        completeness_check: Dict[str, Any],
        user_query: str,
        verified_text: str,
        agent_debates_summary: str
    ) -> str:
        """ë¯¸ì™„ì„± ë³´ê³ ì„œ ë³´ì™„."""
        from src.core.llm_manager import execute_llm_task, TaskType
        
        completion_prompt = f"""ë‹¤ìŒ ë³´ê³ ì„œê°€ ë¶ˆì™„ì „í•©ë‹ˆë‹¤. ì™„ì„±í•˜ì„¸ìš”:

ì‚¬ìš©ì ìš”ì²­: {user_query}

í˜„ì¬ ë³´ê³ ì„œ (ë¶ˆì™„ì „):
{current_report}

ì™„ì„±ë„ ê²€ì¦ ê²°ê³¼:
- ì™„ì„±ë„ ì ìˆ˜: {completeness_check['completeness_score']:.2f}
- ë°œê²¬ëœ ë¬¸ì œ: {', '.join(completeness_check['issues'])}
- ëˆ„ë½ëœ ì„¹ì…˜: {', '.join(completeness_check.get('missing_sections', []))}
- ë¶ˆì™„ì „í•œ ìš”ì†Œ: {', '.join(completeness_check.get('incomplete_elements', []))}

ê²€ì¦ëœ ì—°êµ¬ ê²°ê³¼:
{verified_text[:3000]}

Agent ë…¼ë°• ê²°ê³¼:
{agent_debates_summary[:1000] if agent_debates_summary else "ì—†ìŒ"}

**ë³´ì™„ ì‘ì—…:**

1. **ë¶ˆì™„ì „í•œ ë¶€ë¶„ ì™„ì„±**:
   - ì¤‘ê°„ì— ì˜ë¦° í‘œë‚˜ ë¦¬ìŠ¤íŠ¸ë¥¼ ì™„ì„±í•˜ì„¸ìš”
   - ë¶ˆì™„ì „í•œ ë¬¸ì¥ì„ ì™„ì„±í•˜ì„¸ìš”
   - ë‹«íˆì§€ ì•Šì€ ë§ˆí¬ë‹¤ìš´ ìš”ì†Œë¥¼ ë‹«ìœ¼ì„¸ìš”

2. **ëˆ„ë½ëœ ì„¹ì…˜ ì¶”ê°€**:
   - ëˆ„ë½ëœ ì„¹ì…˜ì„ ì¶”ê°€í•˜ì„¸ìš”
   - ê²°ë¡  ì„¹ì…˜ì´ ì—†ìœ¼ë©´ ì¶”ê°€í•˜ì„¸ìš”

3. **ë‚´ìš© ë³´ì™„**:
   - ê° ì„¹ì…˜ì— ì¶©ë¶„í•œ ë‚´ìš©ì„ ì¶”ê°€í•˜ì„¸ìš”
   - ì‚¬ìš©ì ìš”ì²­ì— ëŒ€í•œ ì™„ì „í•œ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”

**ì¤‘ìš”:**
- ê¸°ì¡´ ë³´ê³ ì„œì˜ ë‚´ìš©ì„ ìœ ì§€í•˜ë©´ì„œ ë³´ì™„í•˜ì„¸ìš”
- ìƒˆë¡œìš´ ë‚´ìš©ì„ ì¶”ê°€í•  ë•ŒëŠ” ê¸°ì¡´ ë‚´ìš©ê³¼ ì¼ê´€ì„±ì„ ìœ ì§€í•˜ì„¸ìš”
- ë³´ê³ ì„œì˜ ì „ì²´ êµ¬ì¡°ì™€ ìŠ¤íƒ€ì¼ì„ ìœ ì§€í•˜ì„¸ìš”
- ë°˜ë“œì‹œ ì™„ì „í•œ ë³´ê³ ì„œë¥¼ ìƒì„±í•˜ì„¸ìš” (ì¤‘ê°„ì— ì˜ë¦¬ì§€ ì•Šë„ë¡)

ì™„ì„±ëœ ì „ì²´ ë³´ê³ ì„œë¥¼ ìƒì„±í•˜ì„¸ìš”."""
        
        try:
            completion_result = await execute_llm_task(
                prompt=completion_prompt,
                task_type=TaskType.GENERATION,
                system_message="You are an expert report completer. You must complete incomplete reports while maintaining consistency and quality."
            )
            
            completed_report = completion_result.content if hasattr(completion_result, 'content') else str(completion_result)
            
            # ê¸°ì¡´ ë³´ê³ ì„œë³´ë‹¤ ê¸¸ê±°ë‚˜ ê°™ì•„ì•¼ í•¨
            if len(completed_report) >= len(current_report):
                logger.info(f"[{self.name}] âœ… Report completed: {len(completed_report)} characters (was {len(current_report)})")
                return completed_report
            else:
                logger.warning(f"[{self.name}] âš ï¸ Completed report is shorter than original. Using original.")
                return current_report
                
        except Exception as e:
            logger.error(f"[{self.name}] Report completion failed: {e}")
            return current_report


###################
# Orchestrator
###################

class AgentOrchestrator:
    """Orchestrator for multi-agent workflow."""
    
    def __init__(self, config: Any = None):
        """Initialize orchestrator."""
        self.config = config
        self.shared_memory = get_shared_memory()
        self.skill_manager = get_skill_manager()
        self.agent_config = get_agent_config()
        self.graph = None
        # GraphëŠ” ì²« ì‹¤í–‰ ì‹œ ì¿¼ë¦¬ ê¸°ë°˜ìœ¼ë¡œ ë¹Œë“œ

        # SharedResultsManagerì™€ AgentDiscussionManagerëŠ” execute ì‹œì ì— ì´ˆê¸°í™”
        # (objective_idê°€ í•„ìš”í•˜ë¯€ë¡œ)
        self.shared_results_manager: Optional[SharedResultsManager] = None
        self.discussion_manager: Optional[AgentDiscussionManager] = None

        # MCP ë„êµ¬ ìë™ ë°œê²¬ ë° ì„ íƒ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        self.mcp_servers = self._initialize_mcp_servers()
        self.tool_loader = MCPToolLoader(FastMCPMulti(self.mcp_servers))
        self.tool_selector = AgentToolSelector()

        # ì„¸ì…˜ ê´€ë¦¬ì ì´ˆê¸°í™”
        self.session_manager = get_session_manager()
        self.session_manager.set_shared_memory(self.shared_memory)
        self.session_manager.set_context_engineer(get_context_engineer())
        
        # ë°±ê·¸ë¼ìš´ë“œ ë©”ëª¨ë¦¬ ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
        self.memory_service = get_background_memory_service()
        # ì„œë¹„ìŠ¤ ì‹œì‘ì€ execute ë©”ì„œë“œì—ì„œ ë¹„ë™ê¸°ë¡œ ì²˜ë¦¬ë¨

        logger.info("AgentOrchestrator initialized with MCP tool auto-discovery, session management, and background memory service")

    def _initialize_mcp_servers(self) -> dict[str, Any]:
        """í™˜ê²½ ë³€ìˆ˜ ë° êµ¬ì„±ì—ì„œ MCP ì„œë²„ ì„¤ì •ì„ ì´ˆê¸°í™”.
        
        Returns:
            mcp_config.json ì›ë³¸ í˜•ì‹ì˜ dict (FastMCPê°€ ì§ì ‘ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” í˜•ì‹)
        """
        servers: dict[str, Any] = {}
        
        try:
            # í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì°¾ê¸°
            current_file = Path(__file__)
            project_root = current_file.parent.parent.parent
            
            # configs í´ë”ì—ì„œ ë¡œë“œ ì‹œë„ (ìš°ì„ )
            config_file = project_root / "configs" / "mcp_config.json"
            if not config_file.exists():
                # í•˜ìœ„ í˜¸í™˜ì„±: ë£¨íŠ¸ì—ì„œë„ ì‹œë„
                config_file = project_root / "mcp_config.json"
            
            if config_file.exists():
                with open(config_file, 'r') as f:
                    config_data = json.load(f)
                    raw_configs = config_data.get("mcpServers", {})
                    
                    # í™˜ê²½ë³€ìˆ˜ ì¹˜í™˜
                    resolved_configs = self._resolve_env_vars_in_value(raw_configs)
                    
                    # FastMCPê°€ ê¸°ëŒ€í•˜ëŠ” í˜•ì‹ìœ¼ë¡œ ì •ë¦¬
                    # - stdio ì„œë²„: command, args, env, cwdë§Œ ìœ ì§€
                    # - HTTP ì„œë²„: type í•„ë“œ ì œê±°, httpUrl ë˜ëŠ” urlë§Œ ìœ ì§€
                    for server_name, server_config in resolved_configs.items():
                        cleaned_config = {}
                        
                        # stdio ì„œë²„ì¸ ê²½ìš°
                        if "command" in server_config:
                            cleaned_config["command"] = server_config["command"]
                            if "args" in server_config:
                                cleaned_config["args"] = server_config["args"]
                            if "env" in server_config and server_config["env"]:
                                cleaned_config["env"] = server_config["env"]
                            if "cwd" in server_config and server_config["cwd"]:
                                cleaned_config["cwd"] = server_config["cwd"]
                        # HTTP ì„œë²„ì¸ ê²½ìš°
                        elif "httpUrl" in server_config or "url" in server_config:
                            # FastMCPëŠ” url í•„ë“œë¥¼ ê¸°ëŒ€í•¨ (httpUrlì„ urlë¡œ ë³€í™˜)
                            if "httpUrl" in server_config:
                                cleaned_config["url"] = server_config["httpUrl"]
                            elif "url" in server_config:
                                cleaned_config["url"] = server_config["url"]
                            if "headers" in server_config and server_config["headers"]:
                                cleaned_config["headers"] = server_config["headers"]
                            if "params" in server_config and server_config["params"]:
                                cleaned_config["params"] = server_config["params"]
                        
                        if cleaned_config:
                            servers[server_name] = cleaned_config
                    
                    logger.info(f"âœ… Loaded {len(servers)} MCP servers from config: {list(servers.keys())}")
            else:
                logger.warning(f"MCP config file not found at {config_file}")
                
        except Exception as e:
            logger.warning(f"Failed to load MCP server configs: {e}")

        logger.info(f"Initialized {len(servers)} MCP servers for auto-discovery")
        return servers
    
    def _resolve_env_vars_in_value(self, value: Any) -> Any:
        """
        ì¬ê·€ì ìœ¼ë¡œ ê°ì²´ ë‚´ì˜ í™˜ê²½ë³€ìˆ˜ í”Œë ˆì´ìŠ¤í™€ë”ë¥¼ ì‹¤ì œ ê°’ìœ¼ë¡œ ì¹˜í™˜.
        ${VAR_NAME} ë˜ëŠ” $VAR_NAME í˜•ì‹ ì§€ì›.
        """
        if isinstance(value, str):
            # ${VAR_NAME} ë˜ëŠ” $VAR_NAME íŒ¨í„´ ì°¾ê¸°
            pattern = r'\$\{([^}]+)\}|\$(\w+)'
            
            def replace_env_var(match):
                var_name = match.group(1) or match.group(2)
                env_value = os.getenv(var_name)
                if env_value is not None:
                    return env_value
                # í™˜ê²½ë³€ìˆ˜ê°€ ì—†ìœ¼ë©´ ì›ë³¸ ìœ ì§€ (ë˜ëŠ” ê²½ê³ )
                logger.warning(f"Environment variable '{var_name}' not found, keeping placeholder")
                return match.group(0)
            
            result = re.sub(pattern, replace_env_var, value)
            return result
        elif isinstance(value, dict):
            return {k: self._resolve_env_vars_in_value(v) for k, v in value.items()}
        elif isinstance(value, list):
            return [self._resolve_env_vars_in_value(item) for item in value]
        else:
            return value

    async def _assign_tools_to_agents(self, session_id: str) -> None:
        """ëª¨ë“  ì—ì´ì „íŠ¸ì— ìë™ìœ¼ë¡œ MCP ë„êµ¬ í• ë‹¹."""
        try:
            # MCP ë„êµ¬ ìë™ ë°œê²¬
            discovered_tools = await self.tool_loader.get_all_tools()
            tool_infos = await self.tool_loader.list_tool_info()

            logger.info(f"Discovered {len(discovered_tools)} MCP tools from {len(self.mcp_servers)} servers")

            # ê° ì—ì´ì „íŠ¸ë³„ ë„êµ¬ ì„ íƒ ë° í• ë‹¹
            assignments = self.tool_selector.select_tools_for_all_agents(
                discovered_tools, tool_infos
            )

            # ê° ì—ì´ì „íŠ¸ì— ë„êµ¬ í• ë‹¹
            for agent_type, assignment in assignments.items():
                agent = getattr(self, agent_type.value, None)
                if agent:
                    agent.available_tools = assignment.tools
                    agent.tool_infos = assignment.tool_infos
                    logger.info(f"Assigned {len(assignment.tools)} tools to {agent_type.value} agent")

                    # ë„êµ¬ í• ë‹¹ ìš”ì•½ ë¡œê¹…
                    summary = self.tool_selector.get_agent_tool_summary(assignment)
                    logger.info(f"Tool assignment summary for {agent_type.value}: {summary}")

        except Exception as e:
            logger.warning(f"Failed to assign MCP tools to agents: {e}")
            # ë„êµ¬ í• ë‹¹ ì‹¤íŒ¨ ì‹œì—ë„ ê³„ì† ì§„í–‰ (ê¸°ì¡´ ë¡œì§ ìœ ì§€)

    def _build_graph(self, user_query: Optional[str] = None, session_id: Optional[str] = None) -> None:
        """Build LangGraph workflow with Skills auto-selection."""
        
        # Create context for all agents
        context = AgentContext(
            agent_id="orchestrator",
            session_id=session_id or "default",
            shared_memory=self.shared_memory,
            config=self.config,
            shared_results_manager=self.shared_results_manager,
            discussion_manager=self.discussion_manager
        )
        
        # Skills ìë™ ì„ íƒ (ì¿¼ë¦¬ê°€ ìˆìœ¼ë©´)
        selected_skills = {}
        if user_query:
            skill_selector = get_skill_selector()
            matches = skill_selector.select_skills_for_task(user_query)
            for match in matches:
                skill = self.skill_manager.load_skill(match.skill_id)
                if skill:
                    selected_skills[match.skill_id] = skill
        
        # Initialize agents with Skills
        self.planner = PlannerAgent(context, selected_skills.get("research_planner"))
        self.executor = ExecutorAgent(context, selected_skills.get("research_executor"))
        self.verifier = VerifierAgent(context, selected_skills.get("evaluator"))
        self.generator = GeneratorAgent(context, selected_skills.get("synthesizer"))

        # ê° ì—ì´ì „íŠ¸ì— MCP ë„êµ¬ ìë™ í• ë‹¹ (ë¹„ë™ê¸°)
        if session_id:
            asyncio.create_task(self._assign_tools_to_agents(session_id))

        # Build graph
        workflow = StateGraph(AgentState)
        
        # Add nodes
        workflow.add_node("planner", self._planner_node)
        workflow.add_node("executor", self._executor_node)  # Legacy
        workflow.add_node("parallel_executor", self._parallel_executor_node)  # New parallel executor
        workflow.add_node("verifier", self._verifier_node)  # Legacy
        workflow.add_node("parallel_verifier", self._parallel_verifier_node)  # New parallel verifier
        workflow.add_node("generator", self._generator_node)
        workflow.add_node("end", self._end_node)
        
        # Define edges - ë³‘ë ¬ ì‹¤í–‰ ë…¸ë“œ ì‚¬ìš©
        workflow.set_entry_point("planner")
        workflow.add_edge("planner", "parallel_executor")  # ë³‘ë ¬ ì‹¤í–‰ ì‚¬ìš©
        workflow.add_edge("parallel_executor", "parallel_verifier")  # ë³‘ë ¬ ê²€ì¦ ì‚¬ìš©
        workflow.add_edge("parallel_verifier", "generator")
        workflow.add_edge("generator", "end")
        
        # Compile graph
        self.graph = workflow.compile()
        
        logger.info("LangGraph workflow built")
    
    async def _planner_node(self, state: AgentState) -> AgentState:
        """Planner node execution with tracking."""
        logger.info("=" * 80)
        logger.info("ğŸ”µ [WORKFLOW] â†’ Planner Node")
        logger.info("=" * 80)
        
        # Progress tracker ì—…ë°ì´íŠ¸
        try:
            from src.core.progress_tracker import get_progress_tracker, WorkflowStage
            progress_tracker = get_progress_tracker()
            if progress_tracker:
                progress_tracker.set_workflow_stage(WorkflowStage.PLANNING, {"message": "ì—°êµ¬ ê³„íš ìˆ˜ë¦½ ì¤‘..."})
        except Exception as e:
            logger.debug(f"Failed to update progress tracker: {e}")
        
        result = await self.planner.execute(state)
        logger.info(f"ğŸ”µ [WORKFLOW] âœ“ Planner completed: {result.get('current_agent')}")
        return result
    
    async def _executor_node(self, state: AgentState) -> AgentState:
        """Executor node execution with tracking (legacy - for backward compatibility)."""
        logger.info("=" * 80)
        logger.info("ğŸŸ¢ [WORKFLOW] â†’ Executor Node (legacy)")
        logger.info("=" * 80)
        result = await self.executor.execute(state)
        logger.info(f"ğŸŸ¢ [WORKFLOW] âœ“ Executor completed: {len(result.get('research_results', []))} results")
        return result
    
    async def _parallel_executor_node(self, state: AgentState) -> AgentState:
        """Parallel executor node - runs multiple ExecutorAgent instances simultaneously."""
        logger.info("=" * 80)
        logger.info("ğŸŸ¢ [WORKFLOW] â†’ Parallel Executor Node")
        logger.info("=" * 80)
        
        # Progress tracker ì—…ë°ì´íŠ¸
        try:
            from src.core.progress_tracker import get_progress_tracker, WorkflowStage
            progress_tracker = get_progress_tracker()
            if progress_tracker:
                progress_tracker.set_workflow_stage(WorkflowStage.EXECUTING, {"message": "ì—°êµ¬ ì‹¤í–‰ ì¤‘..."})
        except Exception as e:
            logger.debug(f"Failed to update progress tracker: {e}")
        
        # ì‘ì—… ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
        tasks = state.get('research_tasks', [])
        if not tasks:
            # ë©”ëª¨ë¦¬ì—ì„œ ì½ê¸°
            memory = self.shared_memory
            tasks = memory.read(
                key=f"tasks_{state['session_id']}",
                scope=MemoryScope.SESSION,
                session_id=state['session_id']
            ) or []
        
        if not tasks:
            logger.warning("[WORKFLOW] No tasks found, falling back to single executor")
            return await self._executor_node(state)
        
        logger.info(f"[WORKFLOW] Executing {len(tasks)} tasks in parallel with {len(tasks)} ExecutorAgent instances")
        
        # ë™ì  ë™ì‹œì„± ê´€ë¦¬ í†µí•©
        from src.core.concurrency_manager import get_concurrency_manager
        concurrency_manager = get_concurrency_manager()
        max_concurrent = concurrency_manager.get_current_concurrency() or self.agent_config.max_concurrent_research_units
        max_concurrent = min(max_concurrent, len(tasks))  # ì‘ì—… ìˆ˜ë¥¼ ì´ˆê³¼í•˜ì§€ ì•Šë„ë¡
        
        logger.info(f"[WORKFLOW] Using concurrency limit: {max_concurrent} (from concurrency_manager)")
        
        # Skills ìë™ ì„ íƒ
        selected_skills = {}
        if state.get('user_query'):
            skill_selector = get_skill_selector()
            matches = skill_selector.select_skills_for_task(state['user_query'])
            for match in matches:
                skill = self.skill_manager.load_skill(match.skill_id)
                if skill:
                    selected_skills[match.skill_id] = skill
        
        # ì—¬ëŸ¬ ExecutorAgent ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ë° ë³‘ë ¬ ì‹¤í–‰
        async def execute_single_task(task: Dict[str, Any], task_index: int) -> AgentState:
            """ë‹¨ì¼ ì‘ì—…ì„ ì‹¤í–‰í•˜ëŠ” ExecutorAgent."""
            agent_id = f"executor_{task_index}"
            context = AgentContext(
                agent_id=agent_id,
                session_id=state['session_id'],
                shared_memory=self.shared_memory,
                config=self.config,
                shared_results_manager=self.shared_results_manager,
                discussion_manager=self.discussion_manager
            )
            
            executor_agent = ExecutorAgent(context, selected_skills.get("research_executor"))
            
            try:
                logger.info(f"[WORKFLOW] ExecutorAgent {agent_id} starting task {task.get('task_id', 'unknown')}")
                result_state = await executor_agent.execute(state, assigned_task=task)
                logger.info(f"[WORKFLOW] ExecutorAgent {agent_id} completed: {len(result_state.get('research_results', []))} results")
                return result_state
            except Exception as e:
                logger.error(f"[WORKFLOW] ExecutorAgent {agent_id} failed: {e}")
                # ì‹¤íŒ¨í•œ ì—ì´ì „íŠ¸ì˜ ìƒíƒœ ë°˜í™˜
                failed_state = state.copy()
                failed_state['research_results'] = []
                failed_state['research_failed'] = True
                failed_state['error'] = f"Task {task.get('task_id', 'unknown')} failed: {str(e)}"
                failed_state['current_agent'] = agent_id
                return failed_state
        
        # ëª¨ë“  ì‘ì—…ì„ ë³‘ë ¬ë¡œ ì‹¤í–‰ (ë™ì  ë™ì‹œì„± ì œí•œ ì ìš©)
        if max_concurrent < len(tasks):
            # Semaphoreë¥¼ ì‚¬ìš©í•˜ì—¬ ë™ì‹œ ì‹¤í–‰ ìˆ˜ ì œí•œ
            semaphore = asyncio.Semaphore(max_concurrent)
            
            async def execute_with_limit(task: Dict[str, Any], task_index: int) -> AgentState:
                async with semaphore:
                    return await execute_single_task(task, task_index)
            
            executor_tasks = [execute_with_limit(task, i) for i, task in enumerate(tasks)]
        else:
            # ë™ì‹œì„± ì œí•œì´ ì‘ì—… ìˆ˜ë³´ë‹¤ í¬ë©´ ëª¨ë“  ì‘ì—…ì„ ë™ì‹œì— ì‹¤í–‰
            executor_tasks = [execute_single_task(task, i) for i, task in enumerate(tasks)]
        
        # ë³‘ë ¬ ì‹¤í–‰
        executor_results = await asyncio.gather(*executor_tasks, return_exceptions=True)
        
        # ê²°ê³¼ í†µí•© ë° í†µì‹  ìƒíƒœ í™•ì¸
        all_results = []
        all_failed = False
        errors = []
        communication_stats = {
            'agents_contributed': 0,
            'results_shared': 0,
            'communication_errors': 0
        }

        for i, result in enumerate(executor_results):
            if isinstance(result, Exception):
                logger.error(f"[WORKFLOW] ExecutorAgent {i} raised exception: {result}")
                all_failed = True
                errors.append(f"Task {tasks[i].get('task_id', 'unknown')}: {str(result)}")
                communication_stats['communication_errors'] += 1
            elif isinstance(result, dict):
                # ê²°ê³¼ ìˆ˜ì§‘
                task_results = result.get('research_results', [])
                if task_results:
                    all_results.extend(task_results)
                    communication_stats['agents_contributed'] += 1
                    logger.info(f"[WORKFLOW] ExecutorAgent {i} contributed {len(task_results)} results")

                # SharedResultsManager í†µì‹  ìƒíƒœ í™•ì¸
                if self.shared_results_manager:
                    agent_id = f"executor_{i}"
                    agent_results = await self.shared_results_manager.get_shared_results(agent_id=agent_id)
                    if agent_results:
                        communication_stats['results_shared'] += len(agent_results)
                        logger.info(f"[WORKFLOW] ğŸ¤ ExecutorAgent {agent_id} shared {len(agent_results)} results via SharedResultsManager")

                # ì‹¤íŒ¨ ìƒíƒœ í™•ì¸
                if result.get('research_failed'):
                    all_failed = True
                    if result.get('error'):
                        errors.append(result['error'])
                        communication_stats['communication_errors'] += 1
        
        # í†µí•©ëœ ìƒíƒœ ìƒì„±
        final_state = state.copy()
        final_state['research_results'] = all_results
        final_state['research_failed'] = all_failed
        final_state['current_agent'] = "parallel_executor"
        
        if errors:
            final_state['error'] = "; ".join(errors)
        
        logger.info(f"[WORKFLOW] âœ… Parallel execution completed: {len(all_results)} total results from {len(tasks)} tasks")
        logger.info(f"[WORKFLOW] ğŸ¤ Agent communication summary: {communication_stats['agents_contributed']} agents contributed, {communication_stats['results_shared']} results shared")
        if communication_stats['communication_errors'] > 0:
            logger.warning(f"[WORKFLOW] âš ï¸ Communication errors: {communication_stats['communication_errors']}")
        logger.info(f"[WORKFLOW] Failed: {all_failed}")
        
        return final_state
    
    async def _verifier_node(self, state: AgentState) -> AgentState:
        """Verifier node execution with tracking (legacy - for backward compatibility)."""
        logger.info("=" * 80)
        logger.info("ğŸŸ¡ [WORKFLOW] â†’ Verifier Node (legacy)")
        logger.info("=" * 80)
        result = await self.verifier.execute(state)
        logger.info(f"ğŸŸ¡ [WORKFLOW] âœ“ Verifier completed: {len(result.get('verified_results', []))} verified")
        return result
    
    async def _parallel_verifier_node(self, state: AgentState) -> AgentState:
        """Parallel verifier node - runs multiple VerifierAgent instances simultaneously."""
        logger.info("=" * 80)
        logger.info("ğŸŸ¡ [WORKFLOW] â†’ Parallel Verifier Node")
        logger.info("=" * 80)
        
        # Progress tracker ì—…ë°ì´íŠ¸
        try:
            from src.core.progress_tracker import get_progress_tracker, WorkflowStage
            progress_tracker = get_progress_tracker()
            if progress_tracker:
                progress_tracker.set_workflow_stage(WorkflowStage.VERIFYING, {"message": "ê²°ê³¼ ê²€ì¦ ì¤‘..."})
        except Exception as e:
            logger.debug(f"Failed to update progress tracker: {e}")
        
        # ì—°êµ¬ ì‹¤íŒ¨ í™•ì¸
        if state.get('research_failed'):
            logger.error("[WORKFLOW] Research execution failed, skipping verification")
            state['verified_results'] = []
            state['verification_failed'] = True
            state['current_agent'] = "parallel_verifier"
            return state
        
        # ê²€ì¦í•  ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
        results = state.get('research_results', [])
        if not results:
            memory = self.shared_memory
            results = memory.read(
                key=f"research_results_{state['session_id']}",
                scope=MemoryScope.SESSION,
                session_id=state['session_id']
            ) or []
        
        if not results:
            logger.warning("[WORKFLOW] No results to verify, falling back to single verifier")
            return await self._verifier_node(state)
        
        # ê²°ê³¼ë¥¼ ì—¬ëŸ¬ ì²­í¬ë¡œ ë¶„í• í•˜ì—¬ ì—¬ëŸ¬ VerifierAgentì— í• ë‹¹
        num_verifiers = min(len(results), self.agent_config.max_concurrent_research_units or 3)
        chunk_size = max(1, len(results) // num_verifiers)
        result_chunks = [results[i:i + chunk_size] for i in range(0, len(results), chunk_size)]
        
        logger.info(f"[WORKFLOW] Verifying {len(results)} results with {len(result_chunks)} VerifierAgent instances")
        
        # ë™ì  ë™ì‹œì„± ê´€ë¦¬ í†µí•©
        from src.core.concurrency_manager import get_concurrency_manager
        concurrency_manager = get_concurrency_manager()
        max_concurrent = concurrency_manager.get_current_concurrency() or self.agent_config.max_concurrent_research_units
        max_concurrent = min(max_concurrent, len(result_chunks))
        
        logger.info(f"[WORKFLOW] Using concurrency limit: {max_concurrent} (from concurrency_manager)")
        
        # Skills ìë™ ì„ íƒ
        selected_skills = {}
        if state.get('user_query'):
            skill_selector = get_skill_selector()
            matches = skill_selector.select_skills_for_task(state['user_query'])
            for match in matches:
                skill = self.skill_manager.load_skill(match.skill_id)
                if skill:
                    selected_skills[match.skill_id] = skill
        
        # ì—¬ëŸ¬ VerifierAgent ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ë° ë³‘ë ¬ ì‹¤í–‰
        async def verify_single_chunk(chunk: List[Dict[str, Any]], chunk_index: int) -> List[Dict[str, Any]]:
            """ë‹¨ì¼ ì²­í¬ë¥¼ ê²€ì¦í•˜ëŠ” VerifierAgent."""
            agent_id = f"verifier_{chunk_index}"
            logger.info(f"[WORKFLOW] ğŸ’¬ Creating VerifierAgent {agent_id} for {len(chunk)} results")
            context = AgentContext(
                agent_id=agent_id,
                session_id=state['session_id'],
                shared_memory=self.shared_memory,
                config=self.config,
                shared_results_manager=self.shared_results_manager,
                discussion_manager=self.discussion_manager
            )
            
            verifier_agent = VerifierAgent(context, selected_skills.get("evaluator"))
            
            # ì²­í¬ë§Œ í¬í•¨í•˜ëŠ” ì„ì‹œ state ìƒì„±
            chunk_state = state.copy()
            chunk_state['research_results'] = chunk
            
            try:
                logger.info(f"[WORKFLOW] VerifierAgent {agent_id} starting verification of {len(chunk)} results")
                result_state = await verifier_agent.execute(chunk_state)
                verified_chunk = result_state.get('verified_results', [])
                logger.info(f"[WORKFLOW] VerifierAgent {agent_id} completed: {len(verified_chunk)} verified")
                return verified_chunk
            except Exception as e:
                logger.error(f"[WORKFLOW] VerifierAgent {agent_id} failed: {e}")
                return []  # ì‹¤íŒ¨ ì‹œ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
        
        # ëª¨ë“  ì²­í¬ë¥¼ ë³‘ë ¬ë¡œ ê²€ì¦ (ë™ì  ë™ì‹œì„± ì œí•œ ì ìš©)
        if max_concurrent < len(result_chunks):
            semaphore = asyncio.Semaphore(max_concurrent)
            
            async def verify_with_limit(chunk: List[Dict[str, Any]], chunk_index: int) -> List[Dict[str, Any]]:
                async with semaphore:
                    return await verify_single_chunk(chunk, chunk_index)
            
            verifier_tasks = [verify_with_limit(chunk, i) for i, chunk in enumerate(result_chunks)]
        else:
            verifier_tasks = [verify_single_chunk(chunk, i) for i, chunk in enumerate(result_chunks)]
        
        # ë³‘ë ¬ ì‹¤í–‰
        verifier_results = await asyncio.gather(*verifier_tasks, return_exceptions=True)
        
        # ê²°ê³¼ í†µí•© ë° í†µì‹  ìƒíƒœ í™•ì¸
        all_verified = []
        communication_stats = {
            'verifiers_contributed': 0,
            'verification_results_shared': 0,
            'discussion_participants': 0
        }

        for i, result in enumerate(verifier_results):
            if isinstance(result, Exception):
                logger.error(f"[WORKFLOW] VerifierAgent {i} raised exception: {result}")
            elif isinstance(result, list):
                all_verified.extend(result)
                communication_stats['verifiers_contributed'] += 1
                logger.info(f"[WORKFLOW] VerifierAgent {i} contributed {len(result)} verified results")

                # SharedResultsManager í†µì‹  ìƒíƒœ í™•ì¸
                if self.shared_results_manager:
                    agent_id = f"verifier_{i}"
                    agent_results = await self.shared_results_manager.get_shared_results(agent_id=agent_id)
                    verification_shared = [r for r in agent_results if isinstance(r.result, dict) and r.result.get('status') == 'verified']
                    if verification_shared:
                        communication_stats['verification_results_shared'] += len(verification_shared)
                        logger.info(f"[WORKFLOW] ğŸ¤ VerifierAgent {agent_id} shared {len(verification_shared)} verification results")

        # ì¤‘ë³µ ì œê±° (URL ê¸°ì¤€)
        seen_urls = set()
        unique_verified = []
        for verified_result in all_verified:
            if isinstance(verified_result, dict):
                url = verified_result.get('url', '')
                if url and url not in seen_urls:
                    seen_urls.add(url)
                    unique_verified.append(verified_result)
                elif not url:
                    unique_verified.append(verified_result)

        logger.info(f"[WORKFLOW] ğŸ“Š Verification deduplication: {len(all_verified)} â†’ {len(unique_verified)} unique results")

        # ì—¬ëŸ¬ VerifierAgent ê°„ í† ë¡  (ê²€ì¦ ê²°ê³¼ê°€ ë‹¤ë¥¸ ê²½ìš°)
        if self.discussion_manager and len(unique_verified) > 0:
            # ë‹¤ë¥¸ VerifierAgentì˜ ê²€ì¦ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
            if self.shared_results_manager:
                other_verified = await self.shared_results_manager.get_shared_results()
                other_verified_results = [r for r in other_verified if isinstance(r.result, dict) and r.result.get('status') == 'verified']

                if other_verified_results:
                    communication_stats['discussion_participants'] = len(set(r.agent_id for r in other_verified_results))
                    logger.info(f"[WORKFLOW] ğŸ’¬ Starting inter-verifier discussion with {len(other_verified_results)} results from {communication_stats['discussion_participants']} agents")

                    # ì²« ë²ˆì§¸ ê²€ì¦ ê²°ê³¼ì— ëŒ€í•´ í† ë¡ 
                    first_verified = unique_verified[0]
                    result_id = f"verification_{first_verified.get('index', 0)}"
                    discussion = await self.discussion_manager.agent_discuss_result(
                        result_id=result_id,
                        agent_id="parallel_verifier",
                        other_agent_results=other_verified_results[:3]
                    )
                    if discussion:
                        logger.info(f"[WORKFLOW] ğŸ’¬ Inter-verifier discussion completed: {discussion[:150]}...")
                        logger.info(f"[WORKFLOW] ğŸ¤ Agent discussion: {communication_stats['discussion_participants']} verifiers participated in result validation")
                    else:
                        logger.info(f"[WORKFLOW] ğŸ’¬ No discussion generated between verifiers")
                else:
                    logger.info(f"[WORKFLOW] ğŸ’¬ No other verified results available for inter-verifier discussion")
        
        # í†µí•©ëœ ìƒíƒœ ìƒì„±
        final_state = state.copy()
        final_state['verified_results'] = unique_verified
        final_state['verification_failed'] = False if unique_verified else True
        final_state['current_agent'] = "parallel_verifier"
        
        logger.info(f"[WORKFLOW] âœ… Parallel verification completed: {len(unique_verified)} total verified results from {len(result_chunks)} verifiers")
        logger.info(f"[WORKFLOW] ğŸ¤ Agent communication summary: {communication_stats['verifiers_contributed']} verifiers contributed, {communication_stats['verification_results_shared']} verification results shared")
        if communication_stats['discussion_participants'] > 0:
            logger.info(f"[WORKFLOW] ğŸ’¬ Inter-verifier discussion: {communication_stats['discussion_participants']} agents participated")
        
        return final_state
    
    async def _generator_node(self, state: AgentState) -> AgentState:
        """Generator node execution with tracking."""
        logger.info("=" * 80)
        logger.info("ğŸŸ£ [WORKFLOW] â†’ Generator Node")
        logger.info("=" * 80)
        
        # Progress tracker ì—…ë°ì´íŠ¸
        try:
            from src.core.progress_tracker import get_progress_tracker, WorkflowStage
            progress_tracker = get_progress_tracker()
            if progress_tracker:
                progress_tracker.set_workflow_stage(WorkflowStage.GENERATING, {"message": "ë³´ê³ ì„œ ìƒì„± ì¤‘..."})
        except Exception as e:
            logger.debug(f"Failed to update progress tracker: {e}")
        
        result = await self.generator.execute(state)
        final_report = result.get('final_report') or ''
        report_length = len(final_report) if final_report else 0
        logger.info(f"ğŸŸ£ [WORKFLOW] âœ“ Generator completed: report_length={report_length}")
        return result
    
    async def _end_node(self, state: AgentState) -> AgentState:
        """End node - final state with summary."""
        logger.info("=" * 80)
        logger.info("âœ… [WORKFLOW] â†’ End Node - Workflow Completed")
        logger.info("=" * 80)
        logger.info(f"Session: {state.get('session_id')}")
        logger.info(f"Final Agent: {state.get('current_agent')}")
        logger.info(f"Research Results: {len(state.get('research_results', []))}")
        logger.info(f"Verified Results: {len(state.get('verified_results', []))}")
        logger.info(f"Report Generated: {bool(state.get('final_report'))}")
        logger.info(f"Failed: {state.get('research_failed') or state.get('verification_failed') or state.get('report_failed')}")
        logger.info("=" * 80)
        
        # ë°±ê·¸ë¼ìš´ë“œ ë©”ëª¨ë¦¬ ìƒì„± íŠ¸ë¦¬ê±° (ì„¸ì…˜ ì¢…ë£Œ ì‹œ)
        try:
            session_id = state.get('session_id')
            user_id = state.get('metadata', {}).get('user_id') or 'default_user'
            
            # ëŒ€í™” íˆìŠ¤í† ë¦¬ êµ¬ì„± (messagesì—ì„œ)
            conversation_history = []
            for msg in state.get('messages', []):
                if isinstance(msg, dict):
                    conversation_history.append({
                        'role': msg.get('type', msg.get('role', 'unknown')),
                        'content': msg.get('content', msg.get('text', ''))
                    })
                else:
                    # LangChain Message ê°ì²´ì¸ ê²½ìš°
                    conversation_history.append({
                        'role': getattr(msg, 'type', 'unknown'),
                        'content': getattr(msg, 'content', str(msg))
                    })
            
            # ë°±ê·¸ë¼ìš´ë“œ ë©”ëª¨ë¦¬ ìƒì„± ì‘ì—… ì œì¶œ (non-blocking)
            if conversation_history:
                task_id = await self.memory_service.submit_memory_generation(
                    session_id=session_id,
                    user_id=user_id,
                    conversation_history=conversation_history,
                    metadata={
                        'research_results_count': len(state.get('research_results', [])),
                        'verified_results_count': len(state.get('verified_results', [])),
                        'has_report': bool(state.get('final_report'))
                    }
                )
                logger.info(f"Background memory generation task submitted: {task_id}")
        except Exception as e:
            logger.warning(f"Failed to trigger background memory generation: {e}")
        
        return state
    
    async def execute(self, user_query: str, session_id: Optional[str] = None, restore_session: bool = False) -> Dict[str, Any]:
        """
        Execute multi-agent workflow with Skills auto-selection.
        
        Args:
            user_query: User's research query
            session_id: Session ID (if None, generates new session)
            restore_session: If True and session_id exists, restore from saved session
            
        Returns:
            Final result from the workflow
        """
        if session_id is None:
            session_id = f"session_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        # ë°±ê·¸ë¼ìš´ë“œ ë©”ëª¨ë¦¬ ì„œë¹„ìŠ¤ ì‹œì‘ (ì´ë¯¸ ì‹œì‘ë˜ì–´ ìˆìœ¼ë©´ ë¬´ì‹œ)
        try:
            if not self.memory_service.is_running:
                await self.memory_service.start()
                logger.info("Background memory service started")
        except Exception as e:
            logger.warning(f"Failed to start memory service: {e}")
        
        logger.info(f"Starting workflow for query: {user_query}, session: {session_id}")
        
        # ì„¸ì…˜ ë³µì› ì‹œë„
        initial_state = None
        if restore_session:
            logger.info(f"Attempting to restore session: {session_id}")
            restored_state = await self.restore_session(session_id)
            if restored_state:
                logger.info(f"âœ… Session restored: {session_id}")
                initial_state = AgentState(**restored_state)
                # ë³µì›ëœ ì„¸ì…˜ì˜ ì¿¼ë¦¬ì™€ ìƒˆ ì¿¼ë¦¬ê°€ ë‹¤ë¥¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì—…ë°ì´íŠ¸
                if user_query:
                    initial_state['user_query'] = user_query
            else:
                logger.info(f"Session not found or restore failed: {session_id}, starting new session")
        
        # Objective ID ìƒì„± (ë³‘ë ¬ ì‹¤í–‰ ë° ê²°ê³¼ ê³µìœ ìš©)
        objective_id = f"objective_{session_id}"
        
        # SharedResultsManagerì™€ AgentDiscussionManager ì´ˆê¸°í™” (ë³‘ë ¬ ì‹¤í–‰ í™œì„±í™” ì‹œ)
        if self.agent_config.enable_agent_communication:
            self.shared_results_manager = SharedResultsManager(objective_id=objective_id)
            self.discussion_manager = AgentDiscussionManager(
                objective_id=objective_id,
                shared_results_manager=self.shared_results_manager
            )
            logger.info("âœ… Agent result sharing and discussion enabled")
            logger.info(f"ğŸ¤ SharedResultsManager initialized for objective: {objective_id}")
            logger.info(f"ğŸ’¬ AgentDiscussionManager initialized with agent communication support")
        else:
            self.shared_results_manager = None
            self.discussion_manager = None
            logger.info("Agent communication disabled")
        
        # Graphê°€ ì—†ê±°ë‚˜ ì¿¼ë¦¬ ê¸°ë°˜ ì¬ë¹Œë“œê°€ í•„ìš”í•œ ê²½ìš° ë¹Œë“œ
        if self.graph is None:
            self._build_graph(user_query, session_id)
        
        # Initialize state if not restored
        if initial_state is None:
            initial_state = AgentState(
                messages=[],
                user_query=user_query,
                research_plan=None,
                research_tasks=[],
                research_results=[],
                verified_results=[],
                final_report=None,
                current_agent=None,
                iteration=0,
                session_id=session_id,
                research_failed=False,
                verification_failed=False,
                report_failed=False,
                error=None
            )
        
        # Execute workflow
        try:
            # DataFlow Pipeline ì˜µì…˜ í™•ì¸ (í™˜ê²½ ë³€ìˆ˜ ë˜ëŠ” ì„¤ì •ì—ì„œ)
            use_pipeline = os.getenv("USE_DATAFLOW_PIPELINE", "false").lower() == "true"
            
            if use_pipeline:
                try:
                    from src.dataflow.integration.orchestrator_pipeline_integration import OrchestratorPipelineIntegration
                    pipeline_integration = OrchestratorPipelineIntegration(use_pipeline=True)
                    
                    # Pipelineì„ ì‚¬ìš©í•˜ì—¬ ì‹¤í–‰
                    logger.info("Using DataFlow Pipeline for execution")
                    result = await pipeline_integration.execute_with_pipeline(
                        agent_state=dict(initial_state),
                        session_id=session_id
                    )
                    
                    # ê²°ê³¼ë¥¼ AgentStateë¡œ ë³€í™˜
                    result = AgentState(**result)
                except Exception as e:
                    logger.warning(f"Pipeline execution failed, falling back to traditional workflow: {e}")
                    # Fallback to traditional workflow
                    result = await self.graph.ainvoke(initial_state)
            else:
                # Traditional workflow execution
                result = await self.graph.ainvoke(initial_state)
            
            # ì„¸ì…˜ ìë™ ì €ì¥ (ì›Œí¬í”Œë¡œìš° ì™„ë£Œ í›„)
            try:
                await self.save_session(session_id, result)
            except Exception as e:
                logger.warning(f"Failed to auto-save session: {e}")
            
            # ë°±ê·¸ë¼ìš´ë“œ ë©”ëª¨ë¦¬ ìƒì„±ì€ _end_nodeì—ì„œ ì´ë¯¸ íŠ¸ë¦¬ê±°ë¨
            
            logger.info("Workflow execution completed successfully")
            return result
        except Exception as e:
            logger.error(f"Workflow execution failed: {e}")
            raise
    
    async def restore_session(self, session_id: str) -> Optional[Dict[str, Any]]:
        """
        ì„¸ì…˜ ë³µì›.
        
        Args:
            session_id: ì„¸ì…˜ ID
            
        Returns:
            ë³µì›ëœ AgentState ë˜ëŠ” None
        """
        try:
            context_engineer = get_context_engineer()
            restored_state = self.session_manager.restore_session(
                session_id=session_id,
                context_engineer=context_engineer,
                shared_memory=self.shared_memory
            )
            
            if restored_state:
                logger.info(f"Session restored successfully: {session_id}")
                return restored_state
            else:
                logger.warning(f"Session restore returned None: {session_id}")
                return None
                
        except Exception as e:
            logger.error(f"Error restoring session {session_id}: {e}", exc_info=True)
            return None
    
    async def save_session(self, session_id: str, state: Dict[str, Any], metadata: Optional[Dict[str, Any]] = None) -> bool:
        """
        ì„¸ì…˜ ì €ì¥.
        
        Args:
            session_id: ì„¸ì…˜ ID
            state: AgentState ë°ì´í„°
            metadata: ì¶”ê°€ ë©”íƒ€ë°ì´í„°
            
        Returns:
            ì„±ê³µ ì—¬ë¶€
        """
        try:
            context_engineer = get_context_engineer()
            success = self.session_manager.save_session(
                session_id=session_id,
                agent_state=state,
                context_engineer=context_engineer,
                shared_memory=self.shared_memory,
                metadata=metadata
            )
            
            if success:
                logger.info(f"Session saved successfully: {session_id}")
            else:
                logger.warning(f"Session save returned False: {session_id}")
            
            return success
            
        except Exception as e:
            logger.error(f"Error saving session {session_id}: {e}", exc_info=True)
            return False
    
    def list_sessions(self, user_id: Optional[str] = None, limit: int = 100) -> List[Dict[str, Any]]:
        """
        ì„¸ì…˜ ëª©ë¡ ì¡°íšŒ.
        
        Args:
            user_id: ì‚¬ìš©ì ID í•„í„°
            limit: ìµœëŒ€ ê²°ê³¼ ìˆ˜
            
        Returns:
            ì„¸ì…˜ ë©”íƒ€ë°ì´í„° ëª©ë¡
        """
        from dataclasses import asdict
        sessions = self.session_manager.list_sessions(user_id=user_id, limit=limit)
        return [asdict(session) for session in sessions]
    
    def delete_session(self, session_id: str) -> bool:
        """
        ì„¸ì…˜ ì‚­ì œ.
        
        Args:
            session_id: ì„¸ì…˜ ID
            
        Returns:
            ì„±ê³µ ì—¬ë¶€
        """
        return self.session_manager.delete_session(session_id)
    
    def create_snapshot(self, session_id: str) -> Optional[str]:
        """
        ì„¸ì…˜ ìŠ¤ëƒ…ìƒ· ìƒì„±.
        
        Args:
            session_id: ì„¸ì…˜ ID
            
        Returns:
            ìŠ¤ëƒ…ìƒ· ID ë˜ëŠ” None
        """
        return self.session_manager.create_snapshot(session_id)
    
    def restore_from_snapshot(self, session_id: str, snapshot_id: str) -> bool:
        """
        ìŠ¤ëƒ…ìƒ·ì—ì„œ ì„¸ì…˜ ë³µì›.
        
        Args:
            session_id: ë³µì›í•  ì„¸ì…˜ ID
            snapshot_id: ìŠ¤ëƒ…ìƒ· ID
            
        Returns:
            ì„±ê³µ ì—¬ë¶€
        """
        return self.session_manager.restore_from_snapshot(session_id, snapshot_id)
    
    async def stream(self, user_query: str, session_id: Optional[str] = None, initial_state: Optional[Dict[str, Any]] = None):
        """Stream workflow execution."""
        if session_id is None:
            session_id = f"session_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        # Initialize state
        if initial_state is None:
            initial_state = {}
        
        agent_initial_state = AgentState(
            messages=[],
            user_query=user_query,
            research_plan=None,
            research_tasks=[],
            research_results=[],
            verified_results=[],
            final_report=None,
            current_agent=None,
            iteration=0,
            session_id=session_id,
            research_failed=False,
            verification_failed=False,
            report_failed=False,
            error=None,
            pending_questions=initial_state.get('pending_questions'),
            user_responses=initial_state.get('user_responses'),
            clarification_context=initial_state.get('clarification_context'),
            waiting_for_user=initial_state.get('waiting_for_user', False)
        )
        
        # Stream execution
        async for event in self.graph.astream(agent_initial_state):
            yield event


# Global orchestrator instance
_orchestrator: Optional[AgentOrchestrator] = None


def get_orchestrator(config: Any = None) -> AgentOrchestrator:
    """Get global orchestrator instance."""
    global _orchestrator
    
    if _orchestrator is None:
        _orchestrator = AgentOrchestrator(config=config)
    
    return _orchestrator


def init_orchestrator(config: Any = None) -> AgentOrchestrator:
    """Initialize orchestrator."""
    global _orchestrator
    
    _orchestrator = AgentOrchestrator(config=config)
    
    return _orchestrator

