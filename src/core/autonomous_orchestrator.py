"""
LangGraph Orchestrator (v2.0 - 8ëŒ€ í˜ì‹  í†µí•©)

Adaptive Supervisor, Hierarchical Compression, Multi-Model Orchestration,
Continuous Verification, Streaming Pipeline, Universal MCP Hub,
Adaptive Context Window, Production-Grade Reliabilityë¥¼ í†µí•©í•œ
ê³ ë„í™”ëœ LangGraph ê¸°ë°˜ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°.
"""

import asyncio
import logging
from typing import Dict, Any, List, Optional, Tuple, TypedDict, Annotated
from datetime import datetime
import json
from pathlib import Path
import os
from datetime import timezone

# LangGraph imports
from langgraph.graph import StateGraph, END
from langgraph.prebuilt import ToolNode
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, ToolMessage
from langchain_core.tools import BaseTool

from src.core.researcher_config import get_llm_config, get_agent_config, get_research_config, get_mcp_config
from src.core.llm_manager import execute_llm_task, TaskType, get_best_model_for_task
from src.core.mcp_integration import execute_tool, ToolCategory, health_check
from src.core.reliability import execute_with_reliability, get_system_status
from src.core.compression import compress_data, get_compression_stats
from src.core.streaming_manager import EventType, AgentStatus

logger = logging.getLogger(__name__)


class ResearchState(TypedDict):
    """LangGraph ì—°êµ¬ ì›Œí¬í”Œë¡œìš° ìƒíƒœ ì •ì˜ (8ëŒ€ í˜ì‹  í†µí•©)."""
    # Input
    user_request: str
    context: Optional[Dict[str, Any]]
    objective_id: str
    
    # Adaptive Supervisor (í˜ì‹  1)
    complexity_score: float
    allocated_researchers: int
    priority_queue: List[Dict[str, Any]]
    quality_threshold: float
    
    # Analysis
    analyzed_objectives: List[Dict[str, Any]]
    intent_analysis: Dict[str, Any]
    domain_analysis: Dict[str, Any]
    scope_analysis: Dict[str, Any]
    
    # Planning Agent (ìƒˆ í•„ë“œ)
    preliminary_research: Dict[str, Any]  # MCP ë„êµ¬ë¡œ ìˆ˜ì§‘í•œ ì‚¬ì „ ì¡°ì‚¬ ê²°ê³¼
    planned_tasks: List[Dict[str, Any]]  # ì„¸ë¶€ task ëª©ë¡
    agent_assignments: Dict[str, List[str]]  # agentë³„ í• ë‹¹ëœ task
    execution_plan: Dict[str, Any]  # ì‹¤í–‰ ì „ëµ (ìˆœì„œ, ë³‘ë ¬ì„±)
    plan_approved: bool  # Plan ê²€ì¦ í†µê³¼ ì—¬ë¶€
    plan_feedback: Optional[str]  # Plan ê²€ì¦ í”¼ë“œë°±
    plan_iteration: int  # Plan ì¬ì‘ì„± íšŸìˆ˜
    
    
    # Execution (Universal MCP Hub + Streaming Pipeline)
    execution_results: List[Dict[str, Any]]
    agent_status: Dict[str, Any]
    execution_metadata: Dict[str, Any]
    streaming_data: List[Dict[str, Any]]
    streaming_events: List[Dict[str, Any]]  # ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ì´ë²¤íŠ¸
    
    # Hierarchical Compression (í˜ì‹  2)
    compression_results: List[Dict[str, Any]]
    compression_metadata: Dict[str, Any]
    
    # Continuous Verification (í˜ì‹  4)
    verification_results: Dict[str, Any]
    confidence_scores: Dict[str, float]
    verification_stages: List[Dict[str, Any]]
    
    # Evaluation
    evaluation_results: Dict[str, Any]
    quality_metrics: Dict[str, float]
    improvement_areas: List[str]
    
    # Validation
    validation_results: Dict[str, Any]
    validation_score: float
    missing_elements: List[str]
    
    # Synthesis (Adaptive Context Window)
    final_synthesis: Dict[str, Any]
    deliverable_path: Optional[str]
    synthesis_metadata: Dict[str, Any]
    
    # Human-in-the-loop ê´€ë ¨ í•„ë“œ
    pending_questions: List[Dict[str, Any]]  # ëŒ€ê¸° ì¤‘ì¸ ì§ˆë¬¸ë“¤
    user_responses: Dict[str, Any]  # ì§ˆë¬¸ ID -> ì‚¬ìš©ì ì‘ë‹µ
    clarification_context: Dict[str, Any]  # ëª…í™•í™”ëœ ì •ë³´
    waiting_for_user: bool  # ì‚¬ìš©ì ì‘ë‹µ ëŒ€ê¸° ì¤‘ì¸ì§€
    autopilot_mode: bool  # CLI ëª¨ë“œì—ì„œ ìë™ ì„ íƒ ëª¨ë“œ
    context_window_usage: Dict[str, Any]
    
    # Greedy Overseer í•„ë“œ
    overseer_iterations: int  # Overseer ë°˜ë³µ íšŸìˆ˜
    overseer_requirements: List[Dict[str, Any]]  # ì¶”ê°€ ìš”êµ¬ì‚¬í•­
    overseer_evaluations: List[Dict[str, Any]]  # ê° iterationì˜ í‰ê°€
    completeness_scores: Dict[str, float]  # ëª©í‘œë³„ ì™„ì „ì„± ì ìˆ˜
    quality_assessments: Dict[str, Dict[str, float]]  # ê²°ê³¼ë³„ í’ˆì§ˆ í‰ê°€
    overseer_decision: Optional[str]  # 'continue', 'retry', 'ask_user', 'proceed'
    
    # Control Flow
    current_step: str
    iteration: int
    max_iterations: int
    should_continue: bool
    error_message: Optional[str]
    
    # Innovation Stats
    innovation_stats: Dict[str, Any]
    
    # Messages for LangGraph
    messages: Annotated[List[BaseMessage], "Messages in the conversation"]


class AutonomousOrchestrator:
    """9ëŒ€ í˜ì‹ ì„ í†µí•©í•œ LangGraph ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°."""
    
    def __init__(self):
        """ì´ˆê¸°í™”."""
        self.llm_config = get_llm_config()
        self.agent_config = get_agent_config()
        self.research_config = get_research_config()
        self.mcp_config = get_mcp_config()
        
        # ìŠ¤íŠ¸ë¦¬ë° ë§¤ë‹ˆì € ì´ˆê¸°í™”
        from src.core.streaming_manager import get_streaming_manager
        self.streaming_manager = get_streaming_manager()
        
        # ë©”ëª¨ë¦¬ ë° í•™ìŠµ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        from src.storage.hybrid_storage import HybridStorage
        from src.learning.user_profiler import UserProfiler
        from src.learning.research_recommender import ResearchRecommender
        from src.agents.creativity_agent import CreativityAgent
        
        self.hybrid_storage = HybridStorage()
        self.user_profiler = UserProfiler()
        self.research_recommender = ResearchRecommender(self.hybrid_storage, self.user_profiler)
        self.creativity_agent = CreativityAgent()
        
        # 9ë²ˆì§¸ í˜ì‹ : Adaptive Research Depth
        from src.core.adaptive_research_depth import AdaptiveResearchDepth
        depth_config = self.research_config.research_depth if hasattr(self.research_config, "research_depth") else {}
        if isinstance(depth_config, dict):
            self.research_depth = AdaptiveResearchDepth(depth_config)
        else:
            # AdaptiveResearchDepthConfig ê°ì²´ì¸ ê²½ìš°
            self.research_depth = AdaptiveResearchDepth({
                "default_preset": getattr(depth_config, "default_preset", "auto"),
                "presets": getattr(depth_config, "presets", {})
            })
        
        # ì™„ì „ ìë™í˜• ê¸°ëŠ¥: ì½”ë“œë² ì´ìŠ¤ ì—ì´ì „íŠ¸ ë° ë¬¸ì„œ ì •ë¦¬ ì—ì´ì „íŠ¸
        from src.agents.codebase_agent import CodebaseAgent
        from src.agents.document_organizer_agent import DocumentOrganizerAgent
        from src.core.checkpoint_manager import CheckpointManager
        from src.core.context_loader import ContextLoader
        from src.core.session_control import get_session_control
        
        self.codebase_agent = CodebaseAgent()
        self.document_organizer = DocumentOrganizerAgent()
        self.checkpoint_manager = CheckpointManager()
        self.context_loader = ContextLoader()
        self.session_control = get_session_control()
        
        # ì¬ê·€ì  ì»¨í…ìŠ¤íŠ¸ ê´€ë¦¬ì ì´ˆê¸°í™”
        from src.core.recursive_context_manager import get_recursive_context_manager
        self.context_manager = get_recursive_context_manager()
        
        self.graph = None
        self._build_langgraph_workflow()
    
    def _build_langgraph_workflow(self):
        """LangGraph ì›Œí¬í”Œë¡œìš° êµ¬ì¶•."""
        # StateGraph ìƒì„±
        workflow = StateGraph(ResearchState)
        
        # ë…¸ë“œ ì¶”ê°€ (8ëŒ€ í˜ì‹  í†µí•© + Planning Agent + Greedy Overseer)
        workflow.add_node("analyze_objectives", self._analyze_objectives)
        workflow.add_node("planning_agent", self._planning_agent)
        workflow.add_node("verify_plan", self._verify_plan)
        workflow.add_node("overseer_initial_review", self._overseer_initial_review)
        workflow.add_node("adaptive_supervisor", self._adaptive_supervisor)
        workflow.add_node("execute_research", self._execute_research)
        workflow.add_node("hierarchical_compression", self._hierarchical_compression)
        workflow.add_node("continuous_verification", self._continuous_verification)
        workflow.add_node("overseer_evaluation", self._overseer_evaluation)
        workflow.add_node("evaluate_results", self._evaluate_results)
        workflow.add_node("validate_results", self._validate_results)
        workflow.add_node("synthesize_deliverable", self._synthesize_deliverable)
        
        # ì—£ì§€ ì¶”ê°€ (Planning Agent í†µí•©)
        workflow.set_entry_point("analyze_objectives")
        
        # Planning Agent ì›Œí¬í”Œë¡œìš°
        workflow.add_edge("analyze_objectives", "planning_agent")
        
        # Planning Agent í›„ ì¡°ê±´ë¶€ ë¶„ê¸° (ì‚¬ìš©ì ì‘ë‹µ ëŒ€ê¸° ì—¬ë¶€ í™•ì¸)
        workflow.add_conditional_edges(
            "planning_agent",
            lambda state: "waiting_for_clarification" if state.get("waiting_for_user", False) else "verify_plan",
            {
                "waiting_for_clarification": "planning_agent",  # ì‚¬ìš©ì ì‘ë‹µ ëŒ€ê¸° ì¤‘ì´ë©´ ë‹¤ì‹œ planning_agentë¡œ
                "verify_plan": "verify_plan"
            }
        )
        
        # Plan ê²€ì¦ í›„ ì¡°ê±´ë¶€ ë¶„ê¸° (ì¬ì‹œë„ ë¡œì§)
        workflow.add_conditional_edges(
            "verify_plan",
            lambda state: "approved" if state.get("plan_approved", False) else "planning_agent",
            {
                "approved": "overseer_initial_review",
                "planning_agent": "planning_agent"
            }
        )
        
        # Overseer Initial Review -> Adaptive Supervisor
        workflow.add_edge("overseer_initial_review", "adaptive_supervisor")
        
        # ê¸°ì¡´ ì›Œí¬í”Œë¡œìš° (Overseer í†µí•©)
        workflow.add_edge("adaptive_supervisor", "execute_research")
        
        # ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ìë™ ë‹¨ê³„ ê²°ì • (ì¬ê·€ì  ì»¨í…ìŠ¤íŠ¸ ì‚¬ìš©)
        workflow.add_conditional_edges(
            "execute_research",
            self._decide_next_step_based_on_context,
            {
                "continue_research": "execute_research",
                "compress": "hierarchical_compression",
                "verify": "continuous_verification"
            }
        )
        
        workflow.add_edge("hierarchical_compression", "continuous_verification")
        
        # Verification -> Overseer Evaluation
        workflow.add_edge("continuous_verification", "overseer_evaluation")
        
        # Overseer Evaluation -> Decision Router
        workflow.add_conditional_edges(
            "overseer_evaluation",
            self._overseer_decision_router,
            {
                "retry": "execute_research",
                "waiting_for_clarification": "planning_agent",
                "proceed": "evaluate_results"
            }
        )
        
        # Evaluation -> Validation -> Synthesis
        workflow.add_edge("evaluate_results", "validate_results")
        workflow.add_edge("validate_results", "synthesize_deliverable")
        workflow.add_edge("synthesize_deliverable", END)
        
        # ê·¸ë˜í”„ ì»´íŒŒì¼
        self.graph = workflow.compile()
    
    def _log_node_input(self, node_name: str, state: ResearchState):
        """ë…¸ë“œ ì…ë ¥ ë¡œê¹…."""
        logger.info(f"\n{'='*80}\nğŸ”µ NODE INPUT: {node_name}\n{'='*80}")
        logger.info(f"User Request: {state.get('user_request', 'N/A')}")
        logger.info(f"Current Step: {state.get('current_step', 'N/A')}")
        logger.info(f"Iteration: {state.get('iteration', 0)}")
        logger.info(f"Complexity Score: {state.get('complexity_score', 'N/A')}")
        
        # ì£¼ìš” í•„ë“œ ì„ íƒì  ë¡œê¹…
        if 'analyzed_objectives' in state:
            logger.info(f"Objectives Count: {len(state.get('analyzed_objectives', []))}")
        if 'planned_tasks' in state:
            logger.info(f"Planned Tasks Count: {len(state.get('planned_tasks', []))}")
        if 'agent_assignments' in state:
            logger.info(f"Agent Assignments Count: {len(state.get('agent_assignments', {}))}")
        logger.info('='*80)
    
    def _log_node_output(self, node_name: str, state: ResearchState, key_changes: Dict[str, Any] = None):
        """ë…¸ë“œ ì¶œë ¥ ë¡œê¹…."""
        logger.info(f"\n{'='*80}\nğŸŸ¢ NODE OUTPUT: {node_name}\n{'='*80}")
        logger.info(f"Next Step: {state.get('current_step', 'N/A')}")
        logger.info(f"Should Continue: {state.get('should_continue', 'N/A')}")
        logger.info(f"Error Message: {state.get('error_message', 'None')}")
        
        # ì£¼ìš” ë³€ê²½ì‚¬í•­ ë¡œê¹…
        if key_changes:
            logger.info(f"Key Changes:\n{json.dumps(key_changes, indent=2, ensure_ascii=False)}")
        
        # State ì—…ë°ì´íŠ¸ ìš”ì•½
        logger.info(f"Complexity Score: {state.get('complexity_score', 'N/A')}")
        logger.info(f"Allocated Researchers: {state.get('allocated_researchers', 'N/A')}")
        logger.info(f"Iteration: {state.get('iteration', 0)}")
        logger.info('='*80)
    
    async def _analyze_objectives(self, state: ResearchState) -> ResearchState:
        """ëª©í‘œ ë¶„ì„ (Multi-Model Orchestration + ì¬ê·€ì  ì»¨í…ìŠ¤íŠ¸)."""
        # ì…ë ¥ ë¡œê¹…
        self._log_node_input("analyze_objectives", state)
        
        logger.info("ğŸ” Thinking: Analyzing research objectives and requirements")
        logger.info(f"ğŸ“ Research Request: {state['user_request']}")
        
        # ì´ˆê¸° ì»¨í…ìŠ¤íŠ¸ ìƒì„± (ì¬ê·€ì  ì»¨í…ìŠ¤íŠ¸ ì‚¬ìš©)
        initial_context_data = {
            "user_request": state['user_request'],
            "context": state.get('context', {}),
            "objective_id": state.get('objective_id', ''),
            "stage": "analysis"
        }
        context_id = self.context_manager.push_context(
            context_data=initial_context_data,
            depth=0,
            parent_id=None,
            metadata={"node": "analyze_objectives", "timestamp": datetime.now().isoformat()}
        )
        logger.debug(f"Initial context created: {context_id}")
        
        # ìŠ¤íŠ¸ë¦¬ë° ì´ë²¤íŠ¸: ë¶„ì„ ì‹œì‘
        await self.streaming_manager.stream_event(
            event_type=EventType.WORKFLOW_START,
            agent_id="orchestrator",
            workflow_id=state['objective_id'],
            data={
                'stage': 'analysis',
                'message': 'Starting objective analysis',
                'request': state['user_request'][:100] + '...' if len(state['user_request']) > 100 else state['user_request']
            },
            priority=1
        )
        
        analysis_prompt = f"""
        Analyze the following research request comprehensively:
        
        Request: {state['user_request']}
        Context: {state.get('context', {})}
        
        Provide detailed analysis including:
        1. Intent analysis (what the user wants to achieve)
        2. Domain analysis (relevant fields and expertise areas)
        3. Scope analysis (breadth and depth of research needed)
        4. Complexity assessment (1-10 scale)
        5. Resource requirements and constraints
        6. Success criteria and quality metrics
        
        Use production-level analysis with specific, actionable insights.
        Return the result in JSON format with the following structure:
        {{
            "objectives": [{{"id": "obj_1", "description": "Research objective", "priority": "high"}}],
            "intent": {{"primary": "research", "secondary": "analysis"}},
            "domain": {{"fields": ["technology", "research"], "expertise": "general"}},
            "scope": {{"breadth": "comprehensive", "depth": "detailed"}},
            "complexity": 7.0
        }}
        """
        
        try:
            # Multi-Model Orchestrationìœ¼ë¡œ ë¶„ì„
            result = await execute_llm_task(
                prompt=analysis_prompt,
                task_type=TaskType.ANALYSIS,
                system_message="You are an expert research analyst with comprehensive domain knowledge."
            )
            
            logger.info(f"âœ… Analysis completed using model: {result.model_used}")
            logger.info(f"ğŸ“Š Analysis confidence: {result.confidence}")
            
            # ë¶„ì„ ê²°ê³¼ íŒŒì‹±
            analysis_data = self._parse_analysis_result(result.content)
            
            logger.info(f"ğŸ¯ Identified objectives: {len(analysis_data.get('objectives', []))}")
            logger.info(f"ğŸ§  Complexity score: {analysis_data.get('complexity', 5.0)}")
            logger.info(f"ğŸ·ï¸ Domain: {analysis_data.get('domain', {}).get('fields', [])}")
            
            # ìœ ì‚¬ ì—°êµ¬ ê²€ìƒ‰
            similar_research = await self._search_similar_research(
                state['user_request'], 
                state.get('user_id', 'default_user')
            )
            
            state.update({
                "analyzed_objectives": analysis_data.get("objectives", []),
                "intent_analysis": analysis_data.get("intent", {}),
                "domain_analysis": analysis_data.get("domain", {}),
                "scope_analysis": analysis_data.get("scope", {}),
                "complexity_score": analysis_data.get("complexity", 5.0),
                "current_step": "planning_agent",
                "similar_research": similar_research,  # ìœ ì‚¬ ì—°êµ¬ ì¶”ê°€
                "innovation_stats": {
                    "analysis_model": result.model_used,
                    "analysis_confidence": result.confidence,
                    "analysis_time": result.execution_time
                }
            })
            
            # ìŠ¤íŠ¸ë¦¬ë° ì´ë²¤íŠ¸: ë¶„ì„ ì™„ë£Œ
            await self.streaming_manager.stream_event(
                event_type=EventType.AGENT_ACTION,
                agent_id="orchestrator",
                workflow_id=state['objective_id'],
                data={
                    'action': 'analysis_completed',
                    'status': 'completed',
                    'objectives_count': len(analysis_data.get("objectives", [])),
                    'complexity_score': analysis_data.get("complexity", 5.0),
                    'model_used': result.model_used,
                    'confidence': result.confidence
                },
                priority=1
            )
            
        except Exception as e:
            logger.error(f"âŒ Analysis failed: {e}")
            state["error_message"] = str(e)
            state["should_continue"] = False
            raise  # Fail-fast
        
        # ì¶œë ¥ ë¡œê¹…
        key_changes = {
            "analyzed_objectives": len(analysis_data.get("objectives", [])),
            "complexity_score": analysis_data.get("complexity", 5.0),
            "intent_analysis": analysis_data.get("intent", {}),
            "domain_analysis": analysis_data.get("domain", {})
        }
        self._log_node_output("analyze_objectives", state, key_changes)
        
        return state
    
    async def _planning_agent(self, state: ResearchState) -> ResearchState:
        """Planning Agent: MCP ê¸°ë°˜ ì‚¬ì „ ì¡°ì‚¬ â†’ Task ë¶„í•´ â†’ Agent ë™ì  í• ë‹¹ (ì¬ê·€ì  ì»¨í…ìŠ¤íŠ¸ ì‚¬ìš©)."""
        # ì…ë ¥ ë¡œê¹…
        self._log_node_input("planning_agent", state)
        
        logger.info("ğŸ“‹ Thinking: Creating research plan and task breakdown")
        logger.info(f"ğŸ“Š Complexity Score: {state.get('complexity_score', 5.0)}")
        logger.info(f"ğŸ¯ Objectives: {len(state.get('analyzed_objectives', []))}")
        
        # í˜„ì¬ ì»¨í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
        current_context = self.context_manager.get_current_context()
        if not current_context:
            # ì»¨í…ìŠ¤íŠ¸ê°€ ì—†ìœ¼ë©´ ì´ˆê¸° ì»¨í…ìŠ¤íŠ¸ ìƒì„±
            initial_context_data = {
                "user_request": state.get('user_request', ''),
                "context": state.get('context', {}),
                "objective_id": state.get('objective_id', ''),
                "stage": "planning"
            }
            current_context_id = self.context_manager.push_context(
                context_data=initial_context_data,
                depth=0
            )
            current_context = self.context_manager.get_current_context()
        
        # ë¶„ì„ ê²°ê³¼ë¥¼ ì»¨í…ìŠ¤íŠ¸ì— ì¶”ê°€ (ì¬ê·€ì  í™•ì¥)
        if current_context:
            analysis_context = {
                "intent_analysis": state.get("intent_analysis", {}),
                "domain_analysis": state.get("domain_analysis", {}),
                "scope_analysis": state.get("scope_analysis", {}),
                "analyzed_objectives": state.get("analyzed_objectives", []),
                "complexity_score": state.get("complexity_score", 5.0),
                "stage": "planning"
            }
            
            extended_context = self.context_manager.extend_context(
                current_context.context_id,
                analysis_context,
                metadata={"node": "planning_agent", "timestamp": datetime.now().isoformat()}
            )
            
            if extended_context:
                logger.debug(f"Context extended for planning: {extended_context.context_id}")
        
        # ì‚¬ìš©ì ì‘ë‹µ ëŒ€ê¸° ì¤‘ì´ë©´ ì‘ë‹µ ì²˜ë¦¬
        if state.get("waiting_for_user", False):
            user_responses = state.get("user_responses", {})
            if user_responses:
                # ì‘ë‹µì´ ìˆìœ¼ë©´ ëª…í™•í™” ì •ë³´ ì ìš©
                from src.core.human_clarification_handler import get_clarification_handler
                clarification_handler = get_clarification_handler()
                
                for question_id, response_data in user_responses.items():
                    clarification = response_data.get("clarification", {})
                    # ê³„íšì— ëª…í™•í™” ì •ë³´ ì ìš© (ë‚˜ì¤‘ì— ì‚¬ìš©)
                    state["clarification_context"] = state.get("clarification_context", {})
                    state["clarification_context"][question_id] = clarification
                
                # ëŒ€ê¸° ìƒíƒœ í•´ì œ
                state["waiting_for_user"] = False
                state["pending_questions"] = []
                logger.info("âœ… User responses processed, continuing planning")
        
        try:
            # ì»¨í…ìŠ¤íŠ¸ ë¡œë“œ (SPARKLEFORGE.md)
            try:
                project_context = await self.context_loader.load_context()
                if project_context:
                    logger.info("ğŸ“„ Loaded project context from SPARKLEFORGE.md")
                    state["context"] = state.get("context", {})
                    state["context"]["project_context"] = project_context
            except Exception as e:
                logger.debug(f"Failed to load context: {e}")
            
            # CLI ëª¨ë“œ ê°ì§€ (ë” ì •í™•í•œ ë°©ë²•)
            import sys
            is_cli_mode = (
                not hasattr(sys, 'ps1') and  # Interactive shellì´ ì•„ë‹˜
                'streamlit' not in sys.modules and  # Streamlitì´ ë¡œë“œë˜ì§€ ì•ŠìŒ
                not any('streamlit' in str(arg) for arg in sys.argv)  # Streamlit ì‹¤í–‰ ì¸ìê°€ ì—†ìŒ
            )
            
            # ë¶ˆëª…í™•í•œ ë¶€ë¶„ ê°ì§€ (CLI ëª¨ë“œì—ì„œëŠ” ê±´ë„ˆë›°ê¸°, ì‚¬ìš©ì ì‘ë‹µì´ ì—†ì„ ë•Œë§Œ)
            if not state.get("clarification_context") and not is_cli_mode:
                from src.core.human_clarification_handler import get_clarification_handler
                clarification_handler = get_clarification_handler()
                
                # íƒ€ì„ì•„ì›ƒ ì„¤ì • (10ì´ˆ)
                try:
                    ambiguities = await asyncio.wait_for(
                        clarification_handler.detect_ambiguities(
                            state.get('user_request', ''),
                            {
                                'objectives': state.get('analyzed_objectives', []),
                                'domain': state.get('domain_analysis', {}),
                                'scope': state.get('scope_analysis', {})
                            }
                        ),
                        timeout=10.0
                    )
                except asyncio.TimeoutError:
                    logger.warning("detect_ambiguities timeout, skipping clarification")
                    ambiguities = []
            elif is_cli_mode:
                # CLI ëª¨ë“œì—ì„œëŠ” clarification ê±´ë„ˆë›°ê¸°
                ambiguities = []
                logger.info("ğŸ¤– CLI mode: Skipping ambiguity detection")
            else:
                ambiguities = []
            
            # ambiguitiesê°€ ìˆìœ¼ë©´ ì²˜ë¦¬
            if ambiguities:
                    # CLI ëª¨ë“œì´ê±°ë‚˜ autopilot ëª¨ë“œì¸ ê²½ìš° ìë™ ì„ íƒ
                    if is_cli_mode or state.get("autopilot_mode", False):
                        logger.info("ğŸ¤– CLI/Autopilot mode detected - auto-selecting responses")
                        
                        # ê° ì§ˆë¬¸ì— ëŒ€í•´ ìë™ ì‘ë‹µ ìƒì„±
                        user_responses = {}
                        clarification_context = {}
                        
                        for ambiguity in ambiguities:
                            question = await clarification_handler.generate_question(
                                ambiguity,
                                {'user_request': state.get('user_request', '')}
                            )
                            
                            # History ê¸°ë°˜ ìë™ ì„ íƒ
                            shared_memory = getattr(self, 'hybrid_storage', None)
                            if not shared_memory:
                                try:
                                    from src.storage.hybrid_storage import HybridStorage
                                    shared_memory = HybridStorage()
                                except:
                                    shared_memory = None
                            
                            auto_response = await clarification_handler.auto_select_response(
                                question,
                                {'user_request': state.get('user_request', '')},
                                shared_memory
                            )
                            
                            # ì‘ë‹µ ì²˜ë¦¬
                            processed = await clarification_handler.process_user_response(
                                question['id'],
                                auto_response,
                                {'question': question}
                            )
                            
                            if processed.get('validated', False):
                                user_responses[question['id']] = processed
                                clarification_context[question['id']] = processed.get('clarification', {})
                                
                                logger.info(f"âœ… Auto-selected response for {question['type']}: {auto_response}")
                        
                        # ëª…í™•í™” ì •ë³´ë¥¼ stateì— ì €ì¥í•˜ê³  ê³„ì† ì§„í–‰
                        state['clarification_context'] = clarification_context
                        state['user_responses'] = user_responses
                        state['waiting_for_user'] = False
                        state['pending_questions'] = []
                        state['autopilot_mode'] = True
                        
                        logger.info(f"âœ… Auto-processed {len(user_responses)} clarifications in autopilot mode")
                    else:
                        # ì›¹ ëª¨ë“œ: ì‚¬ìš©ìì—ê²Œ ì§ˆë¬¸
                        questions = []
                        for ambiguity in ambiguities:
                            question = await clarification_handler.generate_question(
                                ambiguity,
                                {'user_request': state.get('user_request', '')}
                            )
                            questions.append(question)
                        
                        # ì‚¬ìš©ì ì‘ë‹µ ëŒ€ê¸° ìƒíƒœë¡œ ì „í™˜
                        state['pending_questions'] = questions
                        state['waiting_for_user'] = True
                        state['current_step'] = 'waiting_for_clarification'
                        state['user_responses'] = {}
                        
                        logger.info(f"â“ Generated {len(questions)} questions for user clarification")
                        logger.info("â¸ï¸ Waiting for user responses...")
                        
                        # ì¶œë ¥ ë¡œê¹…
                        key_changes = {
                            "pending_questions_count": len(questions),
                            "waiting_for_user": True,
                            "current_step": "waiting_for_clarification"
                        }
                        self._log_node_output("planning_agent", state, key_changes)
                        
                        return state
            
            # 9ë²ˆì§¸ í˜ì‹ : Adaptive Research Depth - ì—°êµ¬ ê¹Šì´ ê²°ì •
            from src.core.adaptive_research_depth import ResearchPreset
            user_request = state.get("user_request", "")
            preset_str = state.get("research_preset")
            preset = None
            if preset_str:
                try:
                    preset = ResearchPreset(preset_str)
                except ValueError:
                    preset = None
            
            depth_config = self.research_depth.determine_depth(
                user_request,
                preset=preset,
                context=state.get("context")
            )
            
            # ê¹Šì´ ì„¤ì •ì„ stateì— ì €ì¥
            state["research_depth"] = {
                "preset": depth_config.preset.value,
                "planning": depth_config.planning,
                "researching": depth_config.researching,
                "reporting": depth_config.reporting,
                "complexity_score": depth_config.complexity_score
            }
            logger.info(f"ğŸ“Š Research depth determined: {depth_config.preset.value} (complexity: {depth_config.complexity_score:.2f})")
            
            # 1. MCP ë„êµ¬ë¡œ ì‚¬ì „ ì¡°ì‚¬
            preliminary_research = await self._conduct_preliminary_research(state)
            logger.info(f"ğŸ” Preliminary research completed: {preliminary_research.get('sources_count', 0)} sources")
            
            # 2. Task ë¶„í•´ (ë³µì¡ë„ ê¸°ë°˜) - ëª…í™•í™” ì •ë³´ ë° ê¹Šì´ ì„¤ì • ë°˜ì˜
            tasks = await self._decompose_into_tasks(state, preliminary_research, depth_config)
            logger.info(f"ğŸ“‹ Tasks decomposed: {len(tasks)} tasks (depth: {depth_config.preset.value})")
            
            # ëª…í™•í™” ì •ë³´ë¥¼ ì‘ì—…ì— ì ìš©
            clarification_context = state.get("clarification_context", {})
            if clarification_context:
                from src.core.human_clarification_handler import get_clarification_handler
                clarification_handler = get_clarification_handler()
                
                for task in tasks:
                    for question_id, clarification in clarification_context.items():
                        task = clarification_handler.apply_clarification(
                            clarification,
                            task
                        )
            
            # 3. Agent ë™ì  í• ë‹¹ (ë³µì¡ë„ ê¸°ë°˜)
            agent_assignments = await self._assign_agents_dynamically(tasks, state)
            logger.info(f"ğŸ‘¥ Agent assignments: {len(agent_assignments)} task-agent mappings")
            
            # 4. ì‹¤í–‰ ì „ëµ ìˆ˜ë¦½
            execution_plan = await self._create_execution_plan(tasks, agent_assignments)
            logger.info(f"ğŸ“ˆ Execution strategy: {execution_plan.get('strategy', 'sequential')}")
            
            # Planning ê²°ê³¼ë¥¼ stateì— ì €ì¥
            state.update({
                "preliminary_research": preliminary_research,
                "planned_tasks": tasks,
                "agent_assignments": agent_assignments,
                "execution_plan": execution_plan,
                "plan_approved": False,
                "plan_feedback": None,
                "plan_iteration": state.get("plan_iteration", 0) + 1,
                "current_step": "verify_plan",
                "innovation_stats": {
                    **state.get("innovation_stats", {}),
                    "planning_agent": "active",
                    "preliminary_sources": preliminary_research.get('sources_count', 0),
                    "planned_tasks_count": len(tasks),
                    "agent_assignments_count": len(agent_assignments),
                    "execution_strategy": execution_plan.get('strategy', 'sequential')
                }
            })
            
            # ê³„íšì„ ì»¨í…ìŠ¤íŠ¸ì— ì¶”ê°€ (ì¬ê·€ì  í™•ì¥)
            if current_context:
                plan_context = {
                    "planned_tasks": tasks,
                    "agent_assignments": agent_assignments,
                    "execution_plan": execution_plan,
                    "plan_approved": False,
                    "preliminary_research": preliminary_research
                }
                self.context_manager.extend_context(
                    current_context.context_id,
                    plan_context,
                    metadata={"plan_completed": True, "timestamp": datetime.now().isoformat()}
                )
                logger.debug(f"Plan added to context: {current_context.context_id}")
            
            # ì¶œë ¥ ë¡œê¹…
            key_changes = {
                "preliminary_research_sources": preliminary_research.get('sources_count', 0),
                "planned_tasks_count": len(tasks),
                "agent_assignments_count": len(agent_assignments),
                "execution_strategy": execution_plan.get('strategy', 'sequential'),
                "plan_iteration": state.get("plan_iteration", 0),
                "planned_tasks": [{"id": task.get("id"), "type": task.get("type"), "agent": task.get("assigned_agent")} for task in tasks[:3]]  # ì²˜ìŒ 3ê°œë§Œ ë¡œê¹…
            }
            self._log_node_output("planning_agent", state, key_changes)
            
            logger.info("âœ… Planning Agent completed successfully")
            return state
            
        except Exception as e:
            logger.error(f"âŒ Planning Agent failed: {e}")
            state["error_message"] = str(e)
            state["should_continue"] = False
            raise  # Fail-fast
    
    async def _verify_plan(self, state: ResearchState) -> ResearchState:
        """Plan ê²€ì¦: LLM ê¸°ë°˜ plan íƒ€ë‹¹ì„± ê²€ì¦."""
        # ì…ë ¥ ë¡œê¹…
        self._log_node_input("verify_plan", state)
        
        logger.info("âœ… Verifying research plan")
        logger.info(f"ğŸ“‹ Tasks to verify: {len(state.get('planned_tasks', []))}")
        logger.info(f"ğŸ‘¥ Agent assignments: {len(state.get('agent_assignments', {}))}")
        
        try:
            verification_prompt = f"""
            Verify the following research plan for quality and completeness:
            
            Research Request: {state.get('user_request', '')}
            Objectives: {state.get('analyzed_objectives', [])}
            Domain: {state.get('domain_analysis', {})}
            Complexity Score: {state.get('complexity_score', 5.0)}
            
            Planned Tasks: {state.get('planned_tasks', [])}
            Agent Assignments: {state.get('agent_assignments', {})}
            Execution Plan: {state.get('execution_plan', {})}
            
            Check the following criteria:
            1. Completeness: Are all research objectives covered by the tasks?
            2. Agent Allocation: Is the number of agents appropriate for task complexity?
            3. Execution Strategy: Is the execution order and parallelization logical?
            4. Resource Efficiency: Are the estimated costs and time reasonable?
            5. Dependencies: Are task dependencies properly handled?
            6. MCP Tools: Are appropriate tools assigned to each task?
            
            Return your assessment in JSON format:
            {{
                "approved": boolean,
                "confidence": float (0.0-1.0),
                "feedback": "detailed feedback string",
                "suggested_changes": ["list of specific improvements"],
                "critical_issues": ["list of blocking issues if any"]
            }}
            """
            
            result = await execute_llm_task(
                prompt=verification_prompt,
                task_type=TaskType.VERIFICATION,
                system_message="You are an expert research planner and quality auditor with deep knowledge of research methodologies and resource optimization."
            )
            
            logger.info(f"ğŸ” Plan verification completed using model: {result.metadata.get('model', 'unknown') if result.metadata else 'unknown'}")
            logger.info(f"ğŸ“Š Verification confidence: {result.confidence}")
            
            # ì•ˆì „ í•„í„° ê°ì§€ (ModelResultëŠ” dataclassì´ë¯€ë¡œ ì†ì„±ìœ¼ë¡œ ì ‘ê·¼)
            content = result.content if result.content else ""
            if content and ("blocked by safety filters" in content.lower() or 
                           "Unable to extract content" in content):
                logger.warning("âš ï¸ Safety filter triggered in verification. Using default result.")
                verification = {
                    "approved": True,
                    "confidence": 0.5,
                    "feedback": "Verification skipped due to safety filter. Proceeding with plan.",
                    "suggested_changes": [],
                    "critical_issues": []
                }
            else:
                # ê²€ì¦ ê²°ê³¼ íŒŒì‹± (ì•ˆì „í•˜ê²Œ)
                try:
                    verification = self._parse_verification_result(content)
                except Exception as parse_error:
                    logger.warning(f"âš ï¸ Verification parsing failed: {parse_error}. Using default result.")
                    verification = {
                        "approved": True,
                        "confidence": 0.5,
                        "feedback": f"Verification parsing failed: {str(parse_error)}. Proceeding with plan.",
                        "suggested_changes": [],
                        "critical_issues": []
                    }
            
            if verification.get("approved", False):
                state["plan_approved"] = True
                state["plan_feedback"] = verification.get("feedback", "Plan approved")
                logger.info("âœ… Plan approved by verification")
                logger.info(f"ğŸ’¬ Feedback: {verification.get('feedback', '')}")
            else:
                state["plan_approved"] = False
                state["plan_feedback"] = verification.get("feedback", "Plan rejected")
                logger.warning(f"âŒ Plan rejected: {verification.get('feedback')}")
                logger.warning(f"ğŸ”§ Suggested changes: {verification.get('suggested_changes', [])}")
                
                # ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ í™•ì¸ (ë¬´í•œ ë£¨í”„ ë°©ì§€)
                max_iterations = 3
                if state.get("plan_iteration", 0) >= max_iterations:
                    logger.error(f"âŒ Maximum plan iterations ({max_iterations}) reached. Proceeding with current plan.")
                    state["plan_approved"] = True
                    state["plan_feedback"] = f"Plan approved after {max_iterations} iterations (forced)"
            
            state.update({
                "current_step": "adaptive_supervisor" if state.get("plan_approved", False) else "planning_agent",
                "innovation_stats": {
                    **state.get("innovation_stats", {}),
                    "plan_verification": "completed",
                    "plan_approved": state.get("plan_approved", False),
                    "verification_confidence": verification.get("confidence", 0.0),
                    "verification_iteration": state.get("plan_iteration", 0)
                }
            })
            
            # ì¶œë ¥ ë¡œê¹…
            key_changes = {
                "plan_approved": state.get("plan_approved", False),
                "verification_confidence": verification.get("confidence", 0.0),
                "plan_iteration": state.get("plan_iteration", 0),
                "feedback": verification.get("feedback", "")[:200]  # ì²˜ìŒ 200ìë§Œ
            }
            self._log_node_output("verify_plan", state, key_changes)
            
            return state
            
        except Exception as e:
            logger.warning(f"âš ï¸ Plan verification failed: {e}. Proceeding with default verification.")
            # ê²€ì¦ ì‹¤íŒ¨í•´ë„ ì—°êµ¬ ê³„ì† ì§„í–‰
            state["plan_approved"] = True
            state["plan_feedback"] = f"Verification failed but proceeding: {str(e)}"
            state["plan_verification_error"] = str(e)
            
            state.update({
                "current_step": "adaptive_supervisor",  # ê²€ì¦ ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰
                "innovation_stats": {
                    **state.get("innovation_stats", {}),
                    "plan_verification": "failed_but_continuing",
                    "plan_approved": True,
                    "verification_confidence": 0.5,
                    "verification_iteration": state.get("plan_iteration", 0)
                }
            })
            
            self._log_node_output("verify_plan", state, {
                "plan_approved": True,
                "verification_confidence": 0.5,
                "error": str(e),
                "action": "proceeding_despite_error"
            })
            
            return state  # ì˜ˆì™¸ ë°œìƒí•˜ì§€ ì•Šê³  ê³„ì† ì§„í–‰
    
    async def _adaptive_supervisor(self, state: ResearchState) -> ResearchState:
        """Adaptive Supervisor (í˜ì‹  1)."""
        logger.info("ğŸ¯ Adaptive Supervisor allocating resources")
        
        complexity_raw = state.get("complexity_score", 5.0)
        # complexityê°€ dictì¸ ê²½ìš° ì²˜ë¦¬
        if isinstance(complexity_raw, dict):
            complexity = complexity_raw.get('score', complexity_raw.get('value', 5.0))
        elif isinstance(complexity_raw, (int, float)):
            complexity = float(complexity_raw)
        else:
            complexity = 5.0
        
        available_budget = self.llm_config.budget_limit
        
        # ë™ì  ì—°êµ¬ì í• ë‹¹
        allocated_researchers = min(
            max(int(complexity), self.agent_config.min_researchers),
            self.agent_config.max_researchers,
            int(available_budget / 10)  # ì˜ˆìƒ ë¹„ìš© ê¸°ë°˜
        )
        
        # ìš°ì„ ìˆœìœ„ í ìƒì„±
        priority_queue = self._create_priority_queue(state)
        
        # í’ˆì§ˆ ì„ê³„ê°’ ì„¤ì •
        quality_threshold = self.agent_config.quality_threshold
        
        logger.info(f"ğŸ§  Complexity Score: {complexity}")
        logger.info(f"ğŸ‘¥ Allocated Researchers: {allocated_researchers}")
        logger.info(f"ğŸ“Š Quality Threshold: {quality_threshold}")
        logger.info(f"ğŸ“‹ Priority Queue Size: {len(priority_queue)}")
        logger.info(f"ğŸ’° Available Budget: ${available_budget}")
        
        state.update({
            "allocated_researchers": allocated_researchers,
            "priority_queue": priority_queue,
            "quality_threshold": quality_threshold,
            "current_step": "execute_research",
            "innovation_stats": {
                **state.get("innovation_stats", {}),
                "allocated_researchers": allocated_researchers,
                "complexity_score": complexity,
                "priority_queue_size": len(priority_queue)
            }
        })
        
        return state
    
    async def _execute_research(self, state: ResearchState) -> ResearchState:
        """ì—°êµ¬ ì‹¤í–‰ (9ë²ˆì§¸ í˜ì‹ : Progressive Deepening í†µí•©)."""
        """ì—°êµ¬ ì‹¤í–‰ (Universal MCP Hub + Streaming Pipeline + Parallel Execution)."""
        # ì…ë ¥ ë¡œê¹…
        self._log_node_input("execute_research", state)
        
        logger.info("âš™ï¸ Thinking: Executing research tasks and gathering information")
        
        # Planning Agentì—ì„œ ìƒì„±ëœ tasks ì‚¬ìš©
        tasks = state.get("planned_tasks", [])
        agent_assignments = state.get("agent_assignments", {})
        execution_plan = state.get("execution_plan", {})
        objective_id = state.get("objective_id", "default")
        
        logger.info(f"ğŸ“‹ Executing {len(tasks)} planned tasks")
        logger.info(f"ğŸ‘¥ Agent assignments: {len(agent_assignments)} mappings")
        logger.info(f"ğŸ“ˆ Execution strategy: {execution_plan.get('strategy', 'sequential')}")
        
        # ë³‘ë ¬ ì‹¤í–‰ ì‚¬ìš© ì—¬ë¶€ ê²°ì •
        use_parallel = (
            execution_plan.get('strategy') in ['parallel', 'hybrid'] and
            len(tasks) > 1 and
            self.agent_config.max_concurrent_research_units > 1
        )
        
        if use_parallel:
            logger.info("ğŸš€ Using parallel execution with ParallelAgentExecutor")
            
            # ParallelAgentExecutor ì‚¬ìš©
            from src.core.parallel_agent_executor import ParallelAgentExecutor
            
            executor = ParallelAgentExecutor()
            parallel_results = await executor.execute_parallel_tasks(
                tasks=tasks,
                agent_assignments=agent_assignments,
                execution_plan=execution_plan,
                objective_id=objective_id
            )
            
            execution_results = parallel_results.get("execution_results", [])
            streaming_data = [
                {
                    "timestamp": datetime.now().isoformat(),
                    "task_id": r.get("task_id", ""),
                    "status": r.get("status", "completed"),
                    "data": r.get("result"),
                    "tool_used": r.get("tool_used", "")
                }
                for r in execution_results
            ]
            
            logger.info(f"âœ… Parallel execution completed: {len(execution_results)} tasks executed")
        else:
            logger.info("ğŸ“ Using sequential execution (parallel execution conditions not met)")
            # ìˆœì°¨ ì‹¤í–‰ (ê¸°ì¡´ ë¡œì§ - ë³‘ë ¬ ì‹¤í–‰ì´ ë¶ˆê°€ëŠ¥í•œ ê²½ìš°)
            execution_results = []
            streaming_data = []
            
            for task in tasks:
                task_success = False
                tool_attempts = []
                
                try:
                    # MCP ë„êµ¬ ì„ íƒ ë° ì‹¤í–‰ (ëŒ€ì²´ ë„êµ¬ ë¡œì§ í¬í•¨)
                    tool_category = self._get_tool_category_for_task(task)
                    available_tools = self._get_available_tools_for_category(tool_category)
                    
                    # ë„êµ¬ ìš°ì„ ìˆœìœ„ë³„ë¡œ ì‹œë„
                    for tool_name in available_tools:
                        try:
                            logger.info(f"ğŸ”§ Attempting tool: {tool_name}")
                            # íŒŒë¼ë¯¸í„° ìë™ ìƒì„± ë° ê²€ì¦
                            tool_parameters = self._generate_tool_parameters(task, tool_name)
                            tool_result = await execute_tool(
                                tool_name,
                                tool_parameters
                            )
                            
                            tool_attempts.append({
                                "tool": tool_name,
                                "success": tool_result.get("success", False),
                                "error": tool_result.get("error", ""),
                                "execution_time": tool_result.get("execution_time", 0.0)
                            })
                            
                            if tool_result.get("success", False):
                                # ì‹¤ì œ ë°ì´í„° ê²€ì¦
                                if self._validate_tool_result(tool_result, task):
                                    execution_results.append({
                                        "task_id": task.get("id"),
                                        "task_name": task.get("name"),
                                        "tool_used": tool_name,
                                        "result": tool_result.get("data"),
                                        "execution_time": tool_result.get("execution_time", 0.0),
                                        "confidence": tool_result.get("confidence", 0.0),
                                        "attempts": len(tool_attempts),
                                        "status": "completed"
                                    })

                                    # ìŠ¤íŠ¸ë¦¬ë° ë°ì´í„° ì¶”ê°€
                                    streaming_data.append({
                                        "timestamp": datetime.now().isoformat(),
                                        "task_id": task.get("id"),
                                        "status": "completed",
                                        "data": tool_result.get("data"),
                                        "tool_used": tool_name
                                    })
                                    
                                    logger.info(f"âœ… Tool '{tool_name}' executed successfully with valid data")
                                    task_success = True
                                    break
                                else:
                                    logger.warning(f"âš ï¸ Tool '{tool_name}' returned invalid data, trying next tool...")
                            else:
                                logger.warning(f"âŒ Tool '{tool_name}' failed: {tool_result.get('error', 'Unknown error')}")
                                
                        except Exception as tool_error:
                            logger.warning(f"âŒ Tool '{tool_name}' execution error: {tool_error}")
                            tool_attempts.append({
                                "tool": tool_name,
                                "success": False,
                                "error": str(tool_error),
                                "execution_time": 0.0
                            })
                            continue
                    
                    if not task_success:
                        logger.error(f"âŒ All tools failed for task {task.get('id')}. Attempts: {tool_attempts}")
                        # ì‹¤íŒ¨í•œ ì‘ì—…ë„ ê¸°ë¡
                        execution_results.append({
                            "task_id": task.get("id"),
                            "task_name": task.get("name"),
                            "tool_used": "none",
                            "result": None,
                            "execution_time": 0.0,
                            "confidence": 0.0,
                            "attempts": len(tool_attempts),
                            "error": "All tools failed",
                            "tool_attempts": tool_attempts,
                            "status": "failed"
                        })
                        
                except Exception as e:
                    logger.error(f"âŒ Critical error executing task {task.get('id')}: {e}")
                    execution_results.append({
                        "task_id": task.get("id"),
                        "task_name": task.get("name"),
                        "tool_used": "none",
                        "result": None,
                        "execution_time": 0.0,
                        "confidence": 0.0,
                        "attempts": 0,
                        "error": str(e),
                        "status": "failed"
                    })
        
        # 9ë²ˆì§¸ í˜ì‹ : Progressive Deepening - ì—°êµ¬ ì§„í–‰ ìƒí™© ë¶„ì„ ë° ê¹Šì´ ì¡°ì •
        current_depth = state.get("research_depth", {})
        if current_depth and hasattr(self, "research_depth"):
            progress = {
                "iteration_count": state.get("research_iteration", 0) + 1,
                "completion_rate": float(len([r for r in execution_results if r.get("status") == "completed"])) / max(len(tasks), 1),
                "tasks_total": len(tasks),
                "tasks_completed": len([r for r in execution_results if r.get("status") == "completed"]),
            }
            
            # DepthConfig ê°ì²´ ì¬êµ¬ì„±
            from src.core.adaptive_research_depth import DepthConfig, ResearchPreset
            try:
                preset = ResearchPreset(current_depth.get("preset", "medium"))
                current_depth_config = DepthConfig(
                    preset=preset,
                    planning=current_depth.get("planning", {}),
                    researching=current_depth.get("researching", {}),
                    reporting=current_depth.get("reporting", {}),
                    complexity_score=current_depth.get("complexity_score", 0.5)
                )
                
                # Progressive Deepening ì²´í¬
                adjusted_depth = self.research_depth.adjust_depth_progressively(
                    current_depth_config,
                    progress,
                    goals_achieved=False  # TODO: ì‹¤ì œ ëª©í‘œ ë‹¬ì„± ì—¬ë¶€ í™•ì¸
                )
                
                if adjusted_depth:
                    logger.info(f"ğŸ“ˆ Progressive Deepening: {current_depth_config.preset.value} -> {adjusted_depth.preset.value}")
                    state["research_depth"] = {
                        "preset": adjusted_depth.preset.value,
                        "planning": adjusted_depth.planning,
                        "researching": adjusted_depth.researching,
                        "reporting": adjusted_depth.reporting,
                        "complexity_score": adjusted_depth.complexity_score
                    }
                    state["research_depth_adjusted"] = True
            except Exception as e:
                logger.debug(f"Progressive Deepening check failed: {e}")
        
        state.update({
            "execution_results": execution_results,
            "streaming_data": streaming_data,
            "current_step": "hierarchical_compression",
            "research_iteration": state.get("research_iteration", 0) + 1,
            "innovation_stats": {
                **state.get("innovation_stats", {}),
                "tasks_executed": len(execution_results),
                "tools_used": len(set(r.get("tool_used", "") for r in execution_results if r.get("tool_used"))),
                "execution_success_rate": float(len([r for r in execution_results if r.get("status") == "completed"])) / max(len(tasks), 1),
                "parallel_execution_used": use_parallel
            }
        })
        
        # ì¶œë ¥ ë¡œê¹…
        key_changes = {
            "tasks_executed": len(execution_results),
            "tasks_successful": len([r for r in execution_results if r.get("status") == "completed"]),
            "tools_used": len(set(r.get("tool_used", "") for r in execution_results if r.get("tool_used"))),
            "execution_success_rate": float(len([r for r in execution_results if r.get("status") == "completed"])) / max(len(tasks), 1),
            "total_execution_time": sum(r.get("execution_time", 0.0) for r in execution_results),
            "parallel_execution_used": use_parallel
        }
        self._log_node_output("execute_research", state, key_changes)
        
        return state
    
    async def _hierarchical_compression(self, state: ResearchState) -> ResearchState:
        """Hierarchical Compression (í˜ì‹  2)."""
        logger.info("ğŸ—œï¸ Applying Hierarchical Compression")
        
        execution_results = state.get("execution_results", [])
        compression_results = []
        
        # ì‹¤í–‰ ê²°ê³¼ë¥¼ ì»¨í…ìŠ¤íŠ¸ì— ì¶”ê°€ (ì¬ê·€ì  í™•ì¥)
        current_context = self.context_manager.get_current_context()
        if current_context:
            execution_context = {
                "execution_results": execution_results,
                "execution_metadata": state.get("execution_metadata", {}),
                "streaming_data": state.get("streaming_data", []),
                "stage": "execution_completed"
            }
            self.context_manager.extend_context(
                current_context.context_id,
                execution_context,
                metadata={"execution_completed": True, "timestamp": datetime.now().isoformat()}
            )
            logger.debug(f"Execution results added to context: {current_context.context_id}")
        
        # ì‹¤í–‰ ê²°ê³¼ê°€ ì—†ëŠ” ê²½ìš° ì²˜ë¦¬
        if not execution_results:
            logger.warning("âš ï¸ No execution results available for compression. Skipping compression step.")
            state.update({
                "compression_results": [],
                "compression_metadata": {
                    "overall_compression_ratio": 1.0,
                    "total_original_size": 0,
                    "total_compressed_size": 0,
                    "compression_count": 0
                },
                "current_step": "continuous_verification",
                "innovation_stats": {
                    **state.get("innovation_stats", {}),
                    "compression_ratio": 1.0,
                    "compression_applied": 0
                }
            })
            return state
        
        # ê° ì‹¤í–‰ ê²°ê³¼ì— ëŒ€í•´ ì••ì¶• ì ìš©
        for result in execution_results:
            task_id = result.get("task_id", "unknown")
            result_data = result.get("result")
            
            # resultê°€ Noneì´ê±°ë‚˜ ë¹ˆ ë°ì´í„°ì¸ ê²½ìš° ìŠ¤í‚µ
            if result_data is None:
                logger.warning(f"âš ï¸ Skipping compression for task {task_id}: result is None (execution may have failed)")
                compression_results.append({
                    "task_id": task_id,
                    "original_size": 0,
                    "compressed_size": 0,
                    "compression_ratio": 1.0,
                    "validation_score": 0.0,
                    "compressed_data": None,
                    "important_info_preserved": [],
                    "status": "skipped_no_data"
                })
                continue
            
            # ë¹ˆ ë”•ì…”ë„ˆë¦¬ë‚˜ ë¹ˆ ë¬¸ìì—´ì¸ ê²½ìš°ë„ ìŠ¤í‚µ
            if isinstance(result_data, dict) and not result_data:
                logger.warning(f"âš ï¸ Skipping compression for task {task_id}: result is empty dict")
                compression_results.append({
                    "task_id": task_id,
                    "original_size": 0,
                    "compressed_size": 0,
                    "compression_ratio": 1.0,
                    "validation_score": 0.0,
                    "compressed_data": None,
                    "important_info_preserved": [],
                    "status": "skipped_empty_data"
                })
                continue
            
            if isinstance(result_data, str) and not result_data.strip():
                logger.warning(f"âš ï¸ Skipping compression for task {task_id}: result is empty string")
                compression_results.append({
                    "task_id": task_id,
                    "original_size": 0,
                    "compressed_size": 0,
                    "compression_ratio": 1.0,
                    "validation_score": 0.0,
                    "compressed_data": None,
                    "important_info_preserved": [],
                    "status": "skipped_empty_string"
                })
                continue
            
            try:
                # ë°ì´í„° ì••ì¶•
                compressed = await compress_data(result_data)
                
                compression_results.append({
                    "task_id": task_id,
                    "original_size": len(str(result_data)),
                    "compressed_size": len(str(compressed.data)),
                    "compression_ratio": compressed.compression_ratio,
                    "validation_score": compressed.validation_score,
                    "compressed_data": compressed.data,
                    "important_info_preserved": compressed.important_info_preserved,
                    "status": "compressed"
                })
                
            except Exception as e:
                logger.warning(f"âš ï¸ Compression failed for task {task_id}: {e}. Using original data.")
                # ì••ì¶• ì‹¤íŒ¨ ì‹œ ì›ë³¸ ë°ì´í„° ì‚¬ìš©
                compression_results.append({
                    "task_id": task_id,
                    "original_size": len(str(result_data)),
                    "compressed_size": len(str(result_data)),
                    "compression_ratio": 1.0,
                    "validation_score": 1.0,
                    "compressed_data": result_data,
                    "important_info_preserved": [],
                    "status": "compression_failed_using_original"
                })
        
        # ì „ì²´ ì••ì¶• í†µê³„
        total_original = sum(c.get("original_size", 0) for c in compression_results)
        total_compressed = sum(c.get("compressed_size", 0) for c in compression_results)
        overall_compression_ratio = total_compressed / max(total_original, 1)
        
        # ì••ì¶• ê²°ê³¼ë¥¼ ì»¨í…ìŠ¤íŠ¸ì— ì¶”ê°€ (ì¬ê·€ì  í™•ì¥)
        current_context = self.context_manager.get_current_context()
        if current_context:
            compression_context = {
                "compression_results": compression_results,
                "compression_metadata": {
                    "overall_compression_ratio": overall_compression_ratio,
                    "total_original_size": total_original,
                    "total_compressed_size": total_compressed,
                    "compression_count": len(compression_results)
                },
                "stage": "compression_completed"
            }
            self.context_manager.extend_context(
                current_context.context_id,
                compression_context,
                metadata={"compression_completed": True, "timestamp": datetime.now().isoformat()}
            )
            logger.debug(f"Compression results added to context: {current_context.context_id}")
        
        state.update({
            "compression_results": compression_results,
            "compression_metadata": {
                "overall_compression_ratio": overall_compression_ratio,
                "total_original_size": total_original,
                "total_compressed_size": total_compressed,
                "compression_count": len(compression_results)
            },
            "current_step": "continuous_verification",
            "innovation_stats": {
                **state.get("innovation_stats", {}),
                "compression_ratio": float(overall_compression_ratio),
                "compression_applied": len(compression_results)
            }
        })
        
        return state
    
    async def _continuous_verification(self, state: ResearchState) -> ResearchState:
        """Continuous Verification (í˜ì‹  4 + ì¬ê·€ì  ì»¨í…ìŠ¤íŠ¸ ì‚¬ìš©)."""
        logger.info("ğŸ”¬ Applying Continuous Verification")
        
        # í˜„ì¬ ì»¨í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸° ë° ì‹¤í–‰ ê²°ê³¼ ì¶”ê°€
        current_context = self.context_manager.get_current_context()
        if current_context:
            # ì••ì¶• ê²°ê³¼ë¥¼ ì»¨í…ìŠ¤íŠ¸ì— ì¶”ê°€ (ì¬ê·€ì  í™•ì¥)
            compression_context = {
                "compression_results": state.get("compression_results", []),
                "compression_metadata": state.get("compression_metadata", {}),
                "stage": "verification"
            }
            self.context_manager.extend_context(
                current_context.context_id,
                compression_context,
                metadata={"node": "continuous_verification", "timestamp": datetime.now().isoformat()}
            )
            logger.debug(f"Compression results added to context: {current_context.context_id}")
        
        compression_results = state.get("compression_results", [])
        execution_results = state.get("execution_results", [])
        
        # ê²€ìƒ‰ ì‹¤íŒ¨ í™•ì¸: compression_resultsê°€ ë¹„ì–´ìˆê±°ë‚˜ ëª¨ë“  ê²°ê³¼ê°€ ì‹¤íŒ¨í•œ ê²½ìš°
        if not compression_results:
            logger.warning("âš ï¸ No compression results available for verification. Checking execution results...")
            
            # execution_results í™•ì¸
            if not execution_results:
                logger.error("âŒ No execution results available. Research execution may have failed completely.")
                state.update({
                    "verification_stages": [],
                    "confidence_scores": {},
                    "verification_failed": True,
                    "error_message": "No research results available for verification. Search execution may have failed.",
                    "current_step": "evaluate_results",  # ê²€ì¦ ì‹¤íŒ¨í•´ë„ í‰ê°€ ë‹¨ê³„ë¡œ ì§„í–‰
                    "innovation_stats": {
                        **state.get("innovation_stats", {}),
                        "verification_applied": 0,
                        "avg_confidence": 0.0,
                        "verification_status": "skipped_no_results"
                    }
                })
                return state
            
            # execution_resultsì—ì„œ ì‹¤íŒ¨í•œ ì‘ì—…ë§Œ ìˆëŠ”ì§€ í™•ì¸
            successful_results = [r for r in execution_results if r.get("status") == "completed" and r.get("result") is not None]
            if not successful_results:
                logger.error("âŒ All execution results failed. No successful research results to verify.")
                state.update({
                    "verification_stages": [],
                    "confidence_scores": {},
                    "verification_failed": True,
                    "error_message": "All research execution failed. No successful results to verify.",
                    "current_step": "evaluate_results",  # ê²€ì¦ ì‹¤íŒ¨í•´ë„ í‰ê°€ ë‹¨ê³„ë¡œ ì§„í–‰
                    "innovation_stats": {
                        **state.get("innovation_stats", {}),
                        "verification_applied": 0,
                        "avg_confidence": 0.0,
                        "verification_status": "skipped_all_failed",
                        "failed_tasks": len(execution_results)
                    }
                })
                return state
        
        # ìœ íš¨í•œ ê²°ê³¼ë§Œ í•„í„°ë§ (resultê°€ Noneì´ê±°ë‚˜ ë¹ˆ ë°ì´í„°ì¸ ê²½ìš° ì œì™¸)
        valid_results = []
        for result in compression_results:
            compressed_data = result.get("compressed_data")
            if compressed_data is not None and compressed_data != "":
                valid_results.append(result)
            else:
                task_id = result.get("task_id", "unknown")
                logger.warning(f"âš ï¸ Skipping verification for task {task_id}: no valid compressed data")
        
        if not valid_results:
            logger.warning("âš ï¸ No valid compression results after filtering. Proceeding with minimal verification.")
            state.update({
                "verification_stages": [],
                "confidence_scores": {},
                "verification_failed": True,
                "error_message": "No valid compression results available for verification.",
                "current_step": "evaluate_results",  # ê²€ì¦ ì‹¤íŒ¨í•´ë„ í‰ê°€ ë‹¨ê³„ë¡œ ì§„í–‰
                "innovation_stats": {
                    **state.get("innovation_stats", {}),
                    "verification_applied": 0,
                    "avg_confidence": 0.0,
                    "verification_status": "skipped_no_valid_data"
                }
            })
            return state
        
        verification_stages = []
        confidence_scores = {}
        
        # 3ë‹¨ê³„ ê²€ì¦ (ìœ íš¨í•œ ê²°ê³¼ë§Œ)
        for i, result in enumerate(valid_results):
            task_id = result.get("task_id")
            
            try:
                # Stage 1: Self-Verification
                self_score = await self._self_verification(result)
                
                # Stage 2: Cross-Verification
                cross_score = await self._cross_verification(result, valid_results)
                
                # Stage 3: External Verification (ì„ íƒì )
                if self_score < 0.7 or cross_score < 0.7:
                    external_score = await self._external_verification(result)
                else:
                    external_score = 1.0
                
                # ì¢…í•© ì‹ ë¢°ë„ ì ìˆ˜
                final_score = (self_score * 0.3 + cross_score * 0.4 + external_score * 0.3)
                
                verification_stages.append({
                    "task_id": task_id,
                    "stage_1_self": self_score,
                    "stage_2_cross": cross_score,
                    "stage_3_external": external_score,
                    "final_score": final_score
                })
                
                confidence_scores[task_id] = final_score
                
            except Exception as e:
                logger.warning(f"âš ï¸ Verification failed for task {task_id}: {e}. Assigning low confidence score.")
                # ê²€ì¦ ì‹¤íŒ¨ ì‹œ ë‚®ì€ ì‹ ë¢°ë„ ì ìˆ˜ í• ë‹¹
                verification_stages.append({
                    "task_id": task_id,
                    "stage_1_self": 0.3,
                    "stage_2_cross": 0.3,
                    "stage_3_external": 0.3,
                    "final_score": 0.3,
                    "verification_error": str(e)
                })
                confidence_scores[task_id] = 0.3
        
        # ê²€ì¦ ê²°ê³¼ë¥¼ ì»¨í…ìŠ¤íŠ¸ì— ì¶”ê°€ (ì¬ê·€ì  í™•ì¥)
        current_context = self.context_manager.get_current_context()
        if current_context:
            verification_result_context = {
                "verification_results": {
                    "verification_stages": verification_stages,
                    "confidence_scores": confidence_scores
                },
                "verification_failed": False,
                "stage": "verification_completed"
            }
            self.context_manager.extend_context(
                current_context.context_id,
                verification_result_context,
                metadata={"verification_completed": True, "timestamp": datetime.now().isoformat()}
            )
            logger.debug(f"Verification results added to context: {current_context.context_id}")
        
        state.update({
            "verification_stages": verification_stages,
            "confidence_scores": confidence_scores,
            "verification_failed": False,
            "current_step": "evaluate_results",
            "innovation_stats": {
                **state.get("innovation_stats", {}),
                "verification_applied": len(verification_stages),
                "avg_confidence": float(sum(confidence_scores.values())) / max(len(confidence_scores), 1) if confidence_scores else 0.0,
                "valid_results_count": len(valid_results),
                "total_results_count": len(compression_results)
            }
        })
        
        return state
    
    async def _evaluate_results(self, state: ResearchState) -> ResearchState:
        """ê²°ê³¼ í‰ê°€ (Multi-Model Orchestration)."""
        logger.info("ğŸ“Š Evaluating results with Multi-Model Orchestration")
        
        evaluation_prompt = f"""
        Evaluate the following research results comprehensively:
        
        Execution Results: {state.get('execution_results', [])}
        Compression Results: {state.get('compression_results', [])}
        Verification Results: {state.get('verification_stages', [])}
        Confidence Scores: {state.get('confidence_scores', {})}
        
        Provide detailed evaluation including:
        1. Quality assessment with metrics
        2. Completeness analysis
        3. Accuracy verification
        4. Improvement recommendations
        5. Risk assessment
        6. Overall satisfaction score
        
        Use production-level evaluation with specific, actionable insights.
        Return the result in JSON format with the following structure:
        {{
            "overall_score": 0.85,
            "metrics": {{"quality": 0.8, "completeness": 0.9, "accuracy": 0.85}},
            "improvements": ["Add more sources", "Improve analysis depth"]
        }}
        """
        
        # Multi-Model Orchestrationìœ¼ë¡œ í‰ê°€
        result = await execute_llm_task(
            prompt=evaluation_prompt,
            task_type=TaskType.VERIFICATION,
            system_message="You are an expert research evaluator with comprehensive quality assessment capabilities.",
            use_ensemble=True  # Weighted Ensemble ì‚¬ìš©
        )
        
        # í‰ê°€ ê²°ê³¼ íŒŒì‹±
        evaluation_data = self._parse_evaluation_result(result.content)
        
        state.update({
            "evaluation_results": evaluation_data,
            "quality_metrics": evaluation_data.get("metrics", {}),
            "improvement_areas": evaluation_data.get("improvements", []),
            "current_step": "validate_results",
            "innovation_stats": {
                **state.get("innovation_stats", {}),
                "evaluation_model": result.model_used,
                "evaluation_confidence": result.confidence,
                "quality_score": evaluation_data.get("overall_score", 0.8)
            }
        })
        
        return state
    
    async def _validate_results(self, state: ResearchState) -> ResearchState:
        """ê²°ê³¼ ê²€ì¦."""
        logger.info("âœ… Validating results")
        
        # ê²€ì¦ ë¡œì§
        validation_score = self._calculate_validation_score(state)
        missing_elements = self._identify_missing_elements(state)
        
        state.update({
            "validation_score": validation_score,
            "missing_elements": missing_elements,
            "current_step": "synthesize_deliverable",
            "innovation_stats": {
                **state.get("innovation_stats", {}),
                "validation_score": validation_score,
                "missing_elements_count": len(missing_elements)
            }
        })
        
        return state
    
    async def _synthesize_deliverable(self, state: ResearchState) -> ResearchState:
        """ìµœì¢… ê²°ê³¼ ì¢…í•© (Adaptive Context Window + ì¬ê·€ì  ì»¨í…ìŠ¤íŠ¸ ì‚¬ìš©)."""
        logger.info("ğŸ“ Synthesizing final deliverable with Adaptive Context Window")
        
        # í˜„ì¬ ì»¨í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸° ë° ëª¨ë“  ë‹¨ê³„ ê²°ê³¼ í†µí•©
        current_context = self.context_manager.get_current_context()
        if current_context:
            # ëª¨ë“  ë‹¨ê³„ ê²°ê³¼ë¥¼ ì»¨í…ìŠ¤íŠ¸ì— í†µí•© (ì¬ê·€ì  í™•ì¥)
            synthesis_context = {
                "verification_results": state.get("verification_results", {}),
                "confidence_scores": state.get("confidence_scores", {}),
                "evaluation_results": state.get("evaluation_results", {}),
                "quality_metrics": state.get("quality_metrics", {}),
                "validation_results": state.get("validation_results", {}),
                "validation_score": state.get("validation_score", 0.0),
                "stage": "synthesis"
            }
            self.context_manager.extend_context(
                current_context.context_id,
                synthesis_context,
                metadata={"node": "synthesize_deliverable", "timestamp": datetime.now().isoformat()}
            )
            logger.debug(f"All stage results integrated into context: {current_context.context_id}")
            
            # ì»¨í…ìŠ¤íŠ¸ì—ì„œ ì¢…í•© ì •ë³´ ì¶”ì¶œ
            context_data = current_context.context_data
            synthesis_prompt = f"""
        Synthesize the following research findings into a comprehensive deliverable:
        
        User Request: {context_data.get('user_request', state.get('user_request', ''))}
        Intent Analysis: {context_data.get('intent_analysis', {})}
        Domain Analysis: {context_data.get('domain_analysis', {})}
        Planned Tasks: {len(context_data.get('planned_tasks', []))} tasks
        Execution Results: {len(context_data.get('execution_results', []))} results
        Compression Results: {len(context_data.get('compression_results', []))} compressed
        Verification Results: {context_data.get('verification_results', {})}
        Evaluation Results: {context_data.get('evaluation_results', {})}
        Quality Metrics: {context_data.get('quality_metrics', {})}
        
        Create a comprehensive synthesis including:
        1. Executive summary with key insights
        2. Detailed findings with evidence
        3. Analysis and interpretation
        4. Conclusions and recommendations
        5. Limitations and future work
        6. Appendices with supporting data
        
        Use adaptive context management for optimal content organization.
        Use the recursive context to ensure all stages are properly integrated.
        """
        else:
            # ì»¨í…ìŠ¤íŠ¸ê°€ ì—†ìœ¼ë©´ ê¸°ì¡´ ë°©ì‹ ì‚¬ìš©
            synthesis_prompt = f"""
        Synthesize the following research findings into a comprehensive deliverable:
        
        User Request: {state.get('user_request', '')}
        Execution Results: {state.get('execution_results', [])}
        Compression Results: {state.get('compression_results', [])}
        Verification Results: {state.get('verification_stages', [])}
        Evaluation Results: {state.get('evaluation_results', {})}
        Quality Metrics: {state.get('quality_metrics', {})}
        
        Create a comprehensive synthesis including:
        1. Executive summary with key insights
        2. Detailed findings with evidence
        3. Analysis and interpretation
        4. Conclusions and recommendations
        5. Limitations and future work
        6. Appendices with supporting data
        
        Use adaptive context management for optimal content organization.
        """
        
        # Multi-Model Orchestrationìœ¼ë¡œ ì¢…í•©
        result = await execute_llm_task(
            prompt=synthesis_prompt,
            task_type=TaskType.SYNTHESIS,
            system_message="You are an expert research synthesizer with adaptive context window capabilities."
        )
        
        # ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš° ì‚¬ìš©ëŸ‰ ê³„ì‚°
        context_usage = self._calculate_context_usage(state, result.content)
        
        # ìµœì¢… ì¢…í•© ê²°ê³¼ë¥¼ ì»¨í…ìŠ¤íŠ¸ì— ì¶”ê°€ (ì¬ê·€ì  í™•ì¥)
        if current_context:
            final_context = {
                "final_synthesis": {
                    "content": result.content,
                    "model_used": result.model_used,
                    "confidence": result.confidence,
                    "execution_time": result.execution_time
                },
                "context_window_usage": context_usage,
                "stage": "completed"
            }
            self.context_manager.extend_context(
                current_context.context_id,
                final_context,
                metadata={"synthesis_completed": True, "timestamp": datetime.now().isoformat()}
            )
            logger.debug(f"Final synthesis added to context: {current_context.context_id}")
        
        state.update({
            "final_synthesis": {
                "content": result.content,
                "model_used": result.model_used,
                "confidence": result.confidence,
                "execution_time": result.execution_time
            },
            "context_window_usage": context_usage,
            "current_step": "completed",
            "innovation_stats": {
                **state.get("innovation_stats", {}),
                "synthesis_model": result.model_used,
                "synthesis_confidence": result.confidence,
                "context_window_usage": context_usage.get("usage_ratio", 1.0)
            }
        })
        
        # ì—°êµ¬ ê²°ê³¼ë¥¼ ë©”ëª¨ë¦¬ì— ì €ì¥
        await self._save_research_memory(state)
        
        # ì°½ì˜ì  ì¸ì‚¬ì´íŠ¸ ìƒì„±
        await self._generate_creative_insights(state)
        
        return state
    
    async def _generate_creative_insights(self, state: ResearchState) -> None:
        """ì°½ì˜ì  ì¸ì‚¬ì´íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        try:
            context = state.get('user_request', '')
            current_ideas = []
            
            # ê¸°ì¡´ ì•„ì´ë””ì–´ë“¤ ìˆ˜ì§‘
            if 'analyzed_objectives' in state:
                for obj in state['analyzed_objectives']:
                    if 'description' in obj:
                        current_ideas.append(obj['description'])
            
            if 'execution_results' in state:
                for result in state['execution_results']:
                    if 'summary' in result:
                        current_ideas.append(result['summary'])
            
            if not current_ideas:
                logger.warning("No current ideas found for creativity generation")
                return
            
            # ì°½ì˜ì  ì¸ì‚¬ì´íŠ¸ ìƒì„±
            insights = await self.creativity_agent.generate_creative_insights(
                context=context,
                current_ideas=current_ideas[:5]  # ìµœëŒ€ 5ê°œ ì•„ì´ë””ì–´ë§Œ ì‚¬ìš©
            )
            
            if insights:
                # ì¸ì‚¬ì´íŠ¸ë¥¼ ìƒíƒœì— ì €ì¥
                state['creative_insights'] = [
                    {
                        'insight_id': insight.insight_id,
                        'type': insight.type.value,
                        'title': insight.title,
                        'description': insight.description,
                        'related_concepts': insight.related_concepts,
                        'confidence': insight.confidence,
                        'novelty_score': insight.novelty_score,
                        'applicability_score': insight.applicability_score,
                        'reasoning': insight.reasoning,
                        'examples': insight.examples,
                        'metadata': insight.metadata
                    }
                    for insight in insights
                ]
                
                logger.info(f"Generated {len(insights)} creative insights")
                
                # ìŠ¤íŠ¸ë¦¬ë° ì´ë²¤íŠ¸ ë°œìƒ
                await self.streaming_manager.stream_event(
                    event_type=EventType.AGENT_ACTION,
                    agent_id="creativity_agent",
                    workflow_id=state['objective_id'],
                    data={
                        'action': 'creative_insights_generated',
                        'insights_count': len(insights),
                        'insights': [
                            {
                                'title': insight.title,
                                'type': insight.type.value,
                                'confidence': insight.confidence
                            }
                            for insight in insights
                        ]
                    },
                    priority=2
                )
            else:
                logger.warning("No creative insights generated")
                
        except Exception as e:
            logger.error(f"Failed to generate creative insights: {e}")
    
    # ==================== Planning Agent Helper Methods ====================
    
    async def _conduct_preliminary_research(self, state: ResearchState) -> Dict[str, Any]:
        """MCP ë„êµ¬ë¡œ ì‚¬ì „ ì¡°ì‚¬ ìˆ˜í–‰."""
        logger.info("ğŸ” Conducting preliminary research with MCP tools")
        
        objectives = state.get('analyzed_objectives', [])
        domain = state.get('domain_analysis', {})
        
        # í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ
        keywords = self._extract_keywords(objectives, domain)
        logger.info(f"ğŸ”‘ Extracted keywords: {keywords[:5]}")  # ìƒìœ„ 5ê°œë§Œ ë¡œê·¸
        
        # MCP ë„êµ¬ë¡œ ê²€ìƒ‰
        search_results = []
        # ì‹¤ì œ MCP ë„êµ¬ ì´ë¦„ ì‚¬ìš© (ë¼ìš°íŒ… ì§€ì›: g-search, tavily, exaëŠ” _execute_search_toolë¡œ ìë™ ë¼ìš°íŒ…ë¨)
        search_tools = ["g-search", "tavily", "exa"]  # _execute_search_toolë¡œ ë¼ìš°íŒ…ë¨
        
        for i, keyword in enumerate(keywords[:3]):  # ìƒìœ„ 3ê°œ í‚¤ì›Œë“œ
            tool_name = search_tools[i % len(search_tools)]  # ë„êµ¬ ìˆœí™˜ ì‚¬ìš©
            
            try:
                result = await execute_tool(
                    tool_name=tool_name,
                    parameters={"query": keyword, "max_results": 5}
                )
                
                if result.get('success', False):
                    result_data = result.get('data', {})
                    if isinstance(result_data, dict) and 'results' in result_data:
                        data_list = result_data.get('results', [])
                    else:
                        data_list = result_data if isinstance(result_data, list) else []
                    
                    search_results.append({
                        "keyword": keyword,
                        "tool": tool_name,
                        "data": data_list,
                        "sources_count": len(data_list)
                    })
                    logger.info(f"âœ… {tool_name} search for '{keyword}': {len(data_list)} results")
                else:
                    logger.warning(f"âš ï¸ {tool_name} search failed for '{keyword}': {result.get('error', 'Unknown error')}")
                    
            except Exception as e:
                logger.warning(f"âš ï¸ {tool_name} search error for '{keyword}': {e}")
        
        # í•™ìˆ  ê²€ìƒ‰ (ì‹¤ì œ MCP ë„êµ¬ ì´ë¦„ ì‚¬ìš©)
        academic_results = []
        academic_tools = ["semantic_scholar::papers-search-basic"]  # arxiv, scholarëŠ” MCPì— ì—†ìœ¼ë¯€ë¡œ semantic_scholar ì‚¬ìš©
        
        for tool_name in academic_tools:
            try:
                result = await execute_tool(
                    tool_name=tool_name,
                    parameters={"query": " ".join(keywords[:2]), "max_results": 3}
                )
                
                if result.get('success', False):
                    result_data = result.get('data', {})
                    if isinstance(result_data, dict) and 'results' in result_data:
                        data_list = result_data.get('results', [])
                    else:
                        data_list = result_data if isinstance(result_data, list) else []
                    
                    academic_results.append({
                        "tool": tool_name,
                        "data": data_list,
                        "sources_count": len(data_list)
                    })
                    logger.info(f"âœ… {tool_name} academic search: {len(data_list)} results")
                    
            except Exception as e:
                logger.warning(f"âš ï¸ {tool_name} academic search error: {e}")
        
        return {
            "keywords": keywords,
            "search_results": search_results,
            "academic_results": academic_results,
            "sources_count": len(search_results) + len(academic_results),
            "total_results": sum(r.get("sources_count", 0) for r in search_results + academic_results)
        }
    
    def _extract_keywords(self, objectives: List[Dict[str, Any]], domain: Dict[str, Any]) -> List[str]:
        """ëª©í‘œì™€ ë„ë©”ì¸ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ."""
        keywords = []
        
        # Objectivesì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
        for obj in objectives:
            description = obj.get('description', '')
            # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ì¶”ì¶œ (ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ NLP ì‚¬ìš©)
            words = description.lower().split()
            keywords.extend([w for w in words if len(w) > 3 and w not in ['the', 'and', 'for', 'with', 'from']])
        
        # Domainì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
        fields = domain.get('fields', [])
        keywords.extend(fields)
        
        # ì¤‘ë³µ ì œê±° ë° ë¹ˆë„ìˆœ ì •ë ¬
        from collections import Counter
        keyword_counts = Counter(keywords)
        return [kw for kw, count in keyword_counts.most_common(10)]
    
    async def _decompose_into_tasks(
        self,
        state: ResearchState,
        preliminary_research: Dict[str, Any],
        depth_config: Optional[Any] = None
    ) -> List[Dict[str, Any]]:
        """ë³µì¡ë„ ê¸°ë°˜ task ë¶„í•´ (9ë²ˆì§¸ í˜ì‹ : Adaptive Research Depth í†µí•© + ì¬ê·€ì  ë¶„í•´)."""
        logger.info("ğŸ“‹ Decomposing research into specific tasks (with recursive decomposition)")
        
        # complexityì™€ num_tasksë¥¼ í•¨ìˆ˜ ì‹œì‘ ë¶€ë¶„ì—ì„œ í•­ìƒ ì´ˆê¸°í™” (ìŠ¤ì½”í”„ ë¬¸ì œ ë°©ì§€)
        complexity_raw = state.get('complexity_score', 5.0)
        if isinstance(complexity_raw, dict):
            complexity = complexity_raw.get('score', complexity_raw.get('value', 5.0))
        elif isinstance(complexity_raw, (int, float)):
            complexity = float(complexity_raw)
        else:
            complexity = 5.0
        
        num_tasks = 5  # ê¸°ë³¸ê°’
        
        # 9ë²ˆì§¸ í˜ì‹ : depth_configê°€ ìˆìœ¼ë©´ ì‚¬ìš©
        if depth_config:
            planning_config = depth_config.planning.get("decompose", {})
            mode = planning_config.get("mode", "manual")
            
            if mode == "auto":
                # ìë™ ëª¨ë“œ: ë³µì¡ë„ ê¸°ë°˜
                # ë³µì¡ë„ì— ë”°ë¥¸ task ê°œìˆ˜ ê²°ì •
                if complexity <= 5:
                    num_tasks = 3 + int(complexity)  # 3-8ê°œ
                elif complexity <= 8:
                    num_tasks = 5 + int(complexity)  # 5-13ê°œ
                else:
                    num_tasks = 8 + int(complexity * 0.5)  # 8-13ê°œ
                
                # auto_max_subtopics ì œí•œ ì ìš©
                max_subtopics = planning_config.get("auto_max_subtopics", 8)
                num_tasks = min(num_tasks, max_subtopics)
            else:
                # ìˆ˜ë™ ëª¨ë“œ: í”„ë¦¬ì…‹ ì„¤ì • ì‚¬ìš©
                num_tasks = planning_config.get("initial_subtopics", 5)
                logger.info(f"ğŸ“Š Using preset subtopics: {num_tasks}")
        else:
            # ê¸°ì¡´ ë¡œì§ (depth_configê°€ ì—†ëŠ” ê²½ìš°)
            # ë³µì¡ë„ì— ë”°ë¥¸ task ê°œìˆ˜ ê²°ì •
            if complexity <= 5:
                num_tasks = 3 + int(complexity)  # 3-8ê°œ
            elif complexity <= 8:
                num_tasks = 5 + int(complexity)  # 5-13ê°œ
            else:
                num_tasks = 8 + int(complexity * 0.5)  # 8-13ê°œ
        
        logger.info(f"ğŸ“Š Target task count: {num_tasks}")
        
        # ì´ˆê¸° íƒœìŠ¤í¬ ìƒì„±
        initial_tasks = await self._create_initial_tasks(state, preliminary_research, num_tasks, complexity)
        
        # ì¬ê·€ì  ë¶„í•´ ì ìš©
        final_tasks = []
        max_recursion_depth = depth_config.planning.get("max_recursion_depth", 3) if depth_config else 3
        
        for task in initial_tasks:
            if await self._is_atomic_task(task, depth_config, complexity):
                final_tasks.append(task)
                logger.info(f"  âœ… Atomic task: {task.get('name', 'Unknown')} (no further decomposition needed)")
            else:
                # ì¬ê·€ì  ë¶„í•´
                logger.info(f"  ğŸ”„ Non-atomic task detected: {task.get('name', 'Unknown')} - starting recursive decomposition")
                subtasks = await self._recursive_decompose(
                    task, 
                    state, 
                    preliminary_research, 
                    depth_config, 
                    current_depth=0,
                    max_depth=max_recursion_depth
                )
                final_tasks.extend(subtasks)
                logger.info(f"  âœ… Decomposed into {len(subtasks)} subtasks")
        
        # Task ê²€ì¦ ë° ë¡œê¹…
        logger.info(f"ğŸ“‹ Final task count: {len(final_tasks)} (from {len(initial_tasks)} initial tasks)")
        for i, task in enumerate(final_tasks):
            logger.info(f"  Task {i+1}: {task.get('name', 'Unknown')} ({task.get('type', 'research')}) - {task.get('assigned_agent_type', 'unknown')} agent")
        
        return final_tasks
    
    async def _create_initial_tasks(
        self,
        state: ResearchState,
        preliminary_research: Dict[str, Any],
        num_tasks: int,
        complexity: float
    ) -> List[Dict[str, Any]]:
        """ì´ˆê¸° íƒœìŠ¤í¬ ìƒì„± (ê¸°ì¡´ ë¡œì§)."""
        # LLMìœ¼ë¡œ task ìƒì„± (ì‚¬ì „ ì¡°ì‚¬ ê²°ê³¼ í¬í•¨)
        decomposition_prompt = f"""
        Based on preliminary research, decompose the research into {num_tasks} specific, executable tasks:
        
        Research Request: {state.get('user_request', '')}
        Objectives: {state.get('analyzed_objectives', [])}
        Domain: {state.get('domain_analysis', {})}
        Complexity Score: {complexity}
        
        Preliminary Research:
        - Keywords: {preliminary_research.get('keywords', [])}
        - Search Results: {len(preliminary_research.get('search_results', []))} sources
        - Academic Results: {len(preliminary_research.get('academic_results', []))} sources
        
        For each task, provide the following structure:
        {{
            "task_id": "task_1",
            "name": "Specific task name",
            "description": "Detailed task description",
            "type": "academic|market|technical|data|synthesis",
            "assigned_agent_type": "academic_researcher|market_analyst|technical_researcher|data_collector|synthesis_specialist",
            "required_tools": ["g-search", "arxiv", "tavily"],
            "dependencies": ["task_0"],
            "estimated_complexity": 1-10,
            "priority": "high|medium|low",
            "success_criteria": ["specific measurable criteria"]
        }}
        
        Ensure tasks cover all research objectives and have logical dependencies.
        Return as JSON array of task objects.
        """
        
        result = await execute_llm_task(
            prompt=decomposition_prompt,
            task_type=TaskType.PLANNING,
            system_message="You are an expert research project manager with deep knowledge of task decomposition and resource allocation."
        )
        
        # Task ê²°ê³¼ íŒŒì‹±
        tasks = self._parse_tasks_result(result.content)
        return tasks
    
    async def _is_atomic_task(
        self,
        task: Dict[str, Any],
        depth_config: Optional[Any],
        complexity: float
    ) -> bool:
        """
        íƒœìŠ¤í¬ê°€ ì›ìì (atomic)ì¸ì§€ íŒë‹¨ (ROMAì˜ Atomizer ê°œë…).
        
        ì›ìì  íƒœìŠ¤í¬ëŠ” ì§ì ‘ ì‹¤í–‰ ê°€ëŠ¥í•œ íƒœìŠ¤í¬ë¡œ, ë” ì´ìƒ ë¶„í•´í•  í•„ìš”ê°€ ì—†ìŠµë‹ˆë‹¤.
        íŒë‹¨ ê¸°ì¤€:
        - ë³µì¡ë„ê°€ ë‚®ìŒ (estimated_complexity <= 5)
        - ì˜ì¡´ì„±ì´ ì ìŒ (<= 1)
        - ë„êµ¬ ìš”êµ¬ì‚¬í•­ì´ ì ìŒ (<= 2)
        - ëª…í™•í•œ ì„±ê³µ ê¸°ì¤€
        """
        # ë³µì¡ë„ ê¸°ë°˜ íŒë‹¨
        task_complexity = task.get('estimated_complexity', 5)
        if isinstance(task_complexity, dict):
            task_complexity = task_complexity.get('score', task_complexity.get('value', 5))
        elif not isinstance(task_complexity, (int, float)):
            task_complexity = 5
        
        # ë³µì¡ë„ê°€ ë§¤ìš° ë†’ìœ¼ë©´ (>= 8) ë¹„ì›ìì 
        if task_complexity >= 8:
            return False
        
        # ì˜ì¡´ì„± ì²´í¬: ì˜ì¡´ì„±ì´ ë§ìœ¼ë©´ (>= 2) ë¹„ì›ìì 
        dependencies = task.get('dependencies', [])
        if len(dependencies) >= 2:
            return False
        
        # ë„êµ¬ ìš”êµ¬ì‚¬í•­ ì²´í¬: ë„êµ¬ê°€ ë§ìœ¼ë©´ (>= 3) ë¹„ì›ìì 
        required_tools = task.get('required_tools', [])
        if len(required_tools) >= 3:
            return False
        
        # ë³µì¡ë„ê°€ ë‚®ìœ¼ë©´ (<= 5) ì›ìì 
        if task_complexity <= 5:
            return True
        
        # ë³µì¡ë„ê°€ ì¤‘ê°„ì´ë©´ (6-7) ì¶”ê°€ ì¡°ê±´ í™•ì¸
        # ì„±ê³µ ê¸°ì¤€ì´ ëª…í™•í•˜ê³ , ì˜ì¡´ì„±ì´ ì—†ê³ , ë„êµ¬ê°€ ì ìœ¼ë©´ ì›ìì 
        success_criteria = task.get('success_criteria', [])
        if len(success_criteria) >= 2 and len(dependencies) == 0 and len(required_tools) <= 2:
            return True
        
        # ê¸°ë³¸ê°’: ë³µì¡ë„ê°€ ì¤‘ê°„ ì´ìƒì´ë©´ ë¹„ì›ìì 
        return False
    
    async def _recursive_decompose(
        self,
        task: Dict[str, Any],
        state: ResearchState,
        preliminary_research: Dict[str, Any],
        depth_config: Optional[Any],
        current_depth: int,
        max_depth: int
    ) -> List[Dict[str, Any]]:
        """
        ë¹„ì›ì íƒœìŠ¤í¬ë¥¼ ì¬ê·€ì ìœ¼ë¡œ ë¶„í•´ (ROMAì˜ ì¬ê·€ì  ë¶„í•´ ê°œë…).
        
        ë³µì¡í•œ íƒœìŠ¤í¬ë¥¼ ë” ì‘ì€ í•˜ìœ„ íƒœìŠ¤í¬ë¡œ ë¶„í•´í•©ë‹ˆë‹¤.
        ìµœëŒ€ ì¬ê·€ ê¹Šì´ë¥¼ ì œí•œí•˜ì—¬ ë¬´í•œ ë£¨í”„ë¥¼ ë°©ì§€í•©ë‹ˆë‹¤.
        """
        if current_depth >= max_depth:
            logger.warning(f"  âš ï¸ Maximum recursion depth ({max_depth}) reached for task: {task.get('name', 'Unknown')}")
            # ìµœëŒ€ ê¹Šì´ì— ë„ë‹¬í•˜ë©´ ì›ìì  íƒœìŠ¤í¬ë¡œ ê°„ì£¼
            return [task]
        
        task_complexity = task.get('estimated_complexity', 5)
        if isinstance(task_complexity, dict):
            task_complexity = task_complexity.get('score', task_complexity.get('value', 5))
        elif not isinstance(task_complexity, (int, float)):
            task_complexity = 5
        
        # í•˜ìœ„ íƒœìŠ¤í¬ ê°œìˆ˜ ê²°ì • (ë³µì¡ë„ ê¸°ë°˜)
        num_subtasks = min(3 + int(task_complexity / 2), 5)  # ìµœëŒ€ 5ê°œ
        
        # parent_task_id ì¶”ì¶œ (í”„ë¡¬í”„íŠ¸ì—ì„œ ì‚¬ìš©í•˜ê¸° ì „ì— ì •ì˜)
        parent_task_id = task.get('task_id', 'unknown')
        
        logger.info(f"  ğŸ”„ Recursive decomposition (depth {current_depth + 1}/{max_depth}): {task.get('name', 'Unknown')} -> {num_subtasks} subtasks")
        
        # í•˜ìœ„ íƒœìŠ¤í¬ ìƒì„± í”„ë¡¬í”„íŠ¸
        decomposition_prompt = f"""
        Decompose the following complex task into {num_subtasks} smaller, more manageable subtasks:
        
        Parent Task:
        - Name: {task.get('name', '')}
        - Description: {task.get('description', '')}
        - Type: {task.get('type', 'research')}
        - Complexity: {task_complexity}
        - Required Tools: {task.get('required_tools', [])}
        
        Research Context:
        - Request: {state.get('user_request', '')}
        - Objectives: {state.get('analyzed_objectives', [])}
        
        Create subtasks that:
        1. Are more specific and focused than the parent task
        2. Can be executed independently or with minimal dependencies
        3. Together accomplish the parent task's goal
        4. Have lower complexity scores (target: 3-6 each)
        
        For each subtask, provide:
        {{
            "task_id": "subtask_{parent_task_id}_1",
            "name": "Specific subtask name",
            "description": "Detailed subtask description",
            "type": "{task.get('type', 'research')}",
            "assigned_agent_type": "{task.get('assigned_agent_type', 'academic_researcher')}",
            "required_tools": ["g-search", "arxiv"],
            "dependencies": [],
            "estimated_complexity": 3-6,
            "priority": "{task.get('priority', 'medium')}",
            "success_criteria": ["specific measurable criteria"],
            "parent_task_id": "{parent_task_id}"
        }}
        
        Return as JSON array of subtask objects.
        """
        
        result = await execute_llm_task(
            prompt=decomposition_prompt,
            task_type=TaskType.PLANNING,
            system_message="You are an expert at breaking down complex research tasks into manageable subtasks."
        )
        
        # í•˜ìœ„ íƒœìŠ¤í¬ íŒŒì‹±
        subtasks = self._parse_tasks_result(result.content)
        
        # í•˜ìœ„ íƒœìŠ¤í¬ì— parent_task_id ì¶”ê°€ (ì´ë¯¸ ìœ„ì—ì„œ ì •ì˜ë¨)
        for subtask in subtasks:
            subtask['parent_task_id'] = parent_task_id
            subtask['decomposition_depth'] = current_depth + 1
        
        # ê° í•˜ìœ„ íƒœìŠ¤í¬ì— ëŒ€í•´ ì¬ê·€ì ìœ¼ë¡œ ì›ìì„± í™•ì¸
        final_subtasks = []
        for subtask in subtasks:
            if await self._is_atomic_task(subtask, depth_config, task_complexity):
                final_subtasks.append(subtask)
            else:
                # ë” ê¹Šì´ ë¶„í•´
                deeper_subtasks = await self._recursive_decompose(
                    subtask,
                    state,
                    preliminary_research,
                    depth_config,
                    current_depth + 1,
                    max_depth
                )
                final_subtasks.extend(deeper_subtasks)
        
        return final_subtasks
    
    async def _assign_agents_dynamically(
        self,
        tasks: List[Dict[str, Any]],
        state: ResearchState
    ) -> Dict[str, List[str]]:
        """ë³µì¡ë„ ê¸°ë°˜ ë™ì  agent í• ë‹¹."""
        logger.info("ğŸ‘¥ Assigning agents dynamically based on task complexity")
        
        agent_assignments = {}
        available_researchers = state.get('allocated_researchers', 1)
        
        for task in tasks:
            task_id = task.get('task_id', 'unknown')
            complexity_raw = task.get('estimated_complexity', 5)
            # complexity íƒ€ì… ì²´í¬
            if isinstance(complexity_raw, dict):
                complexity = complexity_raw.get('score', complexity_raw.get('value', 5))
            elif isinstance(complexity_raw, (int, float)):
                complexity = int(complexity_raw)
            else:
                complexity = 5
            task_type = task.get('type', 'research')
            
            # ë³µì¡ë„ì— ë”°ë¥¸ agent ìˆ˜ ê²°ì •
            if complexity <= 3:
                num_agents = 1
            elif complexity <= 7:
                num_agents = min(2, available_researchers)
            else:
                num_agents = min(3, available_researchers)
            
            # Agent ìœ í˜• ê²°ì •
            agent_types = self._select_agent_types(task_type, num_agents)
            
            agent_assignments[task_id] = agent_types
            
            logger.info(f"  {task_id}: {num_agents} agents ({', '.join(agent_types)}) for complexity {complexity}")
        
        return agent_assignments
    
    def _select_agent_types(self, task_type: str, num_agents: int) -> List[str]:
        """Task ìœ í˜•ì— ë”°ë¥¸ agent ìœ í˜• ì„ íƒ."""
        agent_type_mapping = {
            "academic": ["academic_researcher"],
            "market": ["market_analyst"],
            "technical": ["technical_researcher"],
            "data": ["data_collector"],
            "synthesis": ["synthesis_specialist"],
            "research": ["academic_researcher", "technical_researcher"]
        }
        
        base_types = agent_type_mapping.get(task_type, ["academic_researcher"])
        
        # í•„ìš”í•œ ìˆ˜ë§Œí¼ agent ìœ í˜• ë°˜í™˜
        if num_agents <= len(base_types):
            return base_types[:num_agents]
        else:
            # ë¶€ì¡±í•œ ê²½ìš° ë‹¤ë¥¸ ìœ í˜• ì¶”ê°€
            additional_types = ["market_analyst", "technical_researcher", "data_collector", "synthesis_specialist"]
            result = base_types.copy()
            for agent_type in additional_types:
                if len(result) >= num_agents:
                    break
                if agent_type not in result:
                    result.append(agent_type)
            return result[:num_agents]
    
    async def _create_execution_plan(
        self,
        tasks: List[Dict[str, Any]],
        agent_assignments: Dict[str, List[str]]
    ) -> Dict[str, Any]:
        """ì‹¤í–‰ ì „ëµ ìˆ˜ë¦½."""
        logger.info("ğŸ“ˆ Creating execution plan")
        
        # ì˜ì¡´ì„± ë¶„ì„
        dependency_graph = self._build_dependency_graph(tasks)
        
        # ë³‘ë ¬ ê°€ëŠ¥í•œ task ê·¸ë£¹ ì‹ë³„
        parallel_groups = self._identify_parallel_groups(dependency_graph)
        
        # ì‹¤í–‰ ìˆœì„œ ê²°ì •
        execution_order = self._determine_execution_order(tasks, dependency_graph)
        
        # ì „ëµ ê²°ì •
        strategy = "hybrid" if parallel_groups else "sequential"
        
        # ì˜ˆìƒ ì‹œê°„ ê³„ì‚°
        estimated_total_time = sum(task.get('estimated_time', 30) for task in tasks)
        
        execution_plan = {
            "strategy": strategy,
            "parallel_groups": parallel_groups,
            "execution_order": execution_order,
            "estimated_total_time": estimated_total_time,
            "dependency_graph": dependency_graph,
            "task_count": len(tasks),
            "agent_count": len(set(agent for agents in agent_assignments.values() for agent in agents))
        }
        
        logger.info(f"ğŸ“Š Execution plan: {strategy} strategy, {len(parallel_groups)} parallel groups, {estimated_total_time}min total")
        
        return execution_plan
    
    def _build_dependency_graph(self, tasks: List[Dict[str, Any]]) -> Dict[str, List[str]]:
        """Task ì˜ì¡´ì„± ê·¸ë˜í”„ êµ¬ì¶•."""
        graph = {}
        
        for task in tasks:
            task_id = task.get('task_id', '')
            dependencies = task.get('dependencies', [])
            graph[task_id] = dependencies
        
        return graph
    
    def _identify_parallel_groups(self, dependency_graph: Dict[str, List[str]]) -> List[List[str]]:
        """ë³‘ë ¬ ì‹¤í–‰ ê°€ëŠ¥í•œ task ê·¸ë£¹ ì‹ë³„."""
        # ê°„ë‹¨í•œ êµ¬í˜„: ì˜ì¡´ì„±ì´ ì—†ëŠ” taskë“¤ì„ ê·¸ë£¹í™”
        parallel_groups = []
        processed = set()
        
        for task_id, dependencies in dependency_graph.items():
            if task_id in processed:
                continue
                
            if not dependencies:  # ì˜ì¡´ì„±ì´ ì—†ëŠ” task
                group = [task_id]
                # ë‹¤ë¥¸ ì˜ì¡´ì„± ì—†ëŠ” taskë“¤ ì°¾ê¸°
                for other_task, other_deps in dependency_graph.items():
                    if other_task != task_id and other_task not in processed and not other_deps:
                        group.append(other_task)
                        processed.add(other_task)
                
                if len(group) > 1:
                    parallel_groups.append(group)
                    processed.update(group)
        
        return parallel_groups
    
    def _determine_execution_order(self, tasks: List[Dict[str, Any]], dependency_graph: Dict[str, List[str]]) -> List[str]:
        """ì˜ì¡´ì„±ì„ ê³ ë ¤í•œ ì‹¤í–‰ ìˆœì„œ ê²°ì •."""
        # ìœ„ìƒ ì •ë ¬ì„ ì‚¬ìš©í•œ ì‹¤í–‰ ìˆœì„œ ê²°ì •
        in_degree = {task_id: 0 for task_id in dependency_graph.keys()}
        
        # ì§„ì… ì°¨ìˆ˜ ê³„ì‚°
        for task_id, dependencies in dependency_graph.items():
            for dep in dependencies:
                if dep in in_degree:
                    in_degree[task_id] += 1
        
        # ìœ„ìƒ ì •ë ¬
        queue = [task_id for task_id, degree in in_degree.items() if degree == 0]
        result = []
        
        while queue:
            current = queue.pop(0)
            result.append(current)
            
            # í˜„ì¬ taskì— ì˜ì¡´í•˜ëŠ” taskë“¤ì˜ ì§„ì… ì°¨ìˆ˜ ê°ì†Œ
            for task_id, dependencies in dependency_graph.items():
                if current in dependencies:
                    in_degree[task_id] -= 1
                    if in_degree[task_id] == 0:
                        queue.append(task_id)
        
        return result
    
    # ==================== Helper Methods ====================
    
    def _parse_analysis_result(self, content: str) -> Dict[str, Any]:
        """ë¶„ì„ ê²°ê³¼ íŒŒì‹± - ì¬ì‹œë„ ë¡œì§ í¬í•¨."""
        import json
        import re
        
        # ìµœëŒ€ 3íšŒ ì¬ì‹œë„
        for attempt in range(3):
            try:
                # Markdown ì½”ë“œ ë¸”ë¡ ì œê±°
                cleaned_content = content.strip()
                if '```json' in cleaned_content:
                    # ```json ... ``` íŒ¨í„´ ì¶”ì¶œ
                    match = re.search(r'```json\s*(.*?)\s*```', cleaned_content, re.DOTALL)
                    if match:
                        cleaned_content = match.group(1).strip()
                elif '```' in cleaned_content:
                    # ``` ... ``` íŒ¨í„´ ì¶”ì¶œ
                    match = re.search(r'```\s*(.*?)\s*```', cleaned_content, re.DOTALL)
                    if match:
                        cleaned_content = match.group(1).strip()
                
                # JSON íŒŒì‹± ì‹œë„
                if cleaned_content.startswith('{'):
                    return json.loads(cleaned_content)
                else:
                    # JSONì´ ì•„ë‹Œ ê²½ìš° ë¶€ë¶„ íŒŒì‹± ì‹œë„
                    if attempt < 2:  # ë§ˆì§€ë§‰ ì‹œë„ê°€ ì•„ë‹ˆë©´
                        logger.warning(f"âš ï¸ Attempt {attempt + 1}: Invalid JSON format, retrying...")
                        continue
                    else:
                        raise ValueError("Invalid JSON format in analysis result")
                        
            except json.JSONDecodeError as e:
                if attempt < 2:  # ë§ˆì§€ë§‰ ì‹œë„ê°€ ì•„ë‹ˆë©´
                    logger.warning(f"âš ï¸ Attempt {attempt + 1}: JSON decode error: {e}, retrying...")
                    continue
                else:
                    logger.error(f"âŒ Failed to parse analysis result after 3 attempts: {e}")
                    raise ValueError(f"Analysis parsing failed after 3 attempts: {e}")
            except Exception as e:
                if attempt < 2:  # ë§ˆì§€ë§‰ ì‹œë„ê°€ ì•„ë‹ˆë©´
                    logger.warning(f"âš ï¸ Attempt {attempt + 1}: Parse error: {e}, retrying...")
                    continue
                else:
                    logger.error(f"âŒ Failed to parse analysis result after 3 attempts: {e}")
                    raise ValueError(f"Analysis parsing failed after 3 attempts: {e}")
        
        # ì´ ì§€ì ì— ë„ë‹¬í•˜ë©´ ì•ˆ ë¨
        raise ValueError("Unexpected error in analysis parsing")
    
    def _parse_tasks_result(self, content: str) -> List[Dict[str, Any]]:
        """Task ë¶„í•´ ê²°ê³¼ íŒŒì‹± - ì¬ì‹œë„ ë¡œì§ í¬í•¨."""
        import json
        import re
        
        # ìµœëŒ€ 3íšŒ ì¬ì‹œë„
        for attempt in range(3):
            try:
                # Markdown ì½”ë“œ ë¸”ë¡ ì œê±°
                cleaned_content = content.strip()
                if '```json' in cleaned_content:
                    match = re.search(r'```json\s*(.*?)\s*```', cleaned_content, re.DOTALL)
                    if match:
                        cleaned_content = match.group(1).strip()
                elif '```' in cleaned_content:
                    match = re.search(r'```\s*(.*?)\s*```', cleaned_content, re.DOTALL)
                    if match:
                        cleaned_content = match.group(1).strip()
                
                # JSON ë°°ì—´ íŒŒì‹± ì‹œë„
                if cleaned_content.startswith('['):
                    return json.loads(cleaned_content)
                else:
                    if attempt < 2:
                        logger.warning(f"âš ï¸ Attempt {attempt + 1}: Invalid JSON array format, retrying...")
                        continue
                    else:
                        raise ValueError("Invalid JSON array format in task decomposition result")
                        
            except json.JSONDecodeError as e:
                if attempt < 2:
                    logger.warning(f"âš ï¸ Attempt {attempt + 1}: JSON decode error: {e}, retrying...")
                    continue
                else:
                    logger.error(f"âŒ Failed to parse tasks result after 3 attempts: {e}")
                    raise ValueError(f"Task parsing failed after 3 attempts: {e}")
            except Exception as e:
                if attempt < 2:
                    logger.warning(f"âš ï¸ Attempt {attempt + 1}: Parse error: {e}, retrying...")
                    continue
                else:
                    logger.error(f"âŒ Failed to parse tasks result after 3 attempts: {e}")
                    raise ValueError(f"Task parsing failed after 3 attempts: {e}")
        
        raise ValueError("Unexpected error in task parsing")
    
    def _parse_verification_result(self, content: str) -> Dict[str, Any]:
        """Plan ê²€ì¦ ê²°ê³¼ íŒŒì‹± - ì¬ì‹œë„ ë¡œì§ í¬í•¨. ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’ ë°˜í™˜í•˜ì—¬ ì—°êµ¬ ê³„ì† ì§„í–‰."""
        import json
        import re
        
        # ì•ˆì „ í•„í„° ì‘ë‹µ ê°ì§€
        if not content or "blocked by safety filters" in content.lower() or "Unable to extract content" in content:
            logger.warning("âš ï¸ Safety filter triggered or empty response. Using default verification result.")
            return {
                "approved": True,  # ê¸°ë³¸ì ìœ¼ë¡œ ìŠ¹ì¸í•˜ì—¬ ì—°êµ¬ ê³„ì† ì§„í–‰
                "confidence": 0.5,  # ë‚®ì€ ì‹ ë¢°ë„
                "feedback": "Verification skipped due to safety filter. Proceeding with plan.",
                "suggested_changes": [],
                "critical_issues": []
            }
        
        # ìµœëŒ€ 3íšŒ ì¬ì‹œë„
        for attempt in range(3):
            try:
                # Markdown ì½”ë“œ ë¸”ë¡ ì œê±°
                cleaned_content = content.strip()
                if '```json' in cleaned_content:
                    match = re.search(r'```json\s*(.*?)\s*```', cleaned_content, re.DOTALL)
                    if match:
                        cleaned_content = match.group(1).strip()
                elif '```' in cleaned_content:
                    match = re.search(r'```\s*(.*?)\s*```', cleaned_content, re.DOTALL)
                    if match:
                        cleaned_content = match.group(1).strip()
                
                # JSON íŒŒì‹± ì‹œë„
                if cleaned_content.startswith('{'):
                    parsed = json.loads(cleaned_content)
                    # í•„ìˆ˜ í•„ë“œ ê²€ì¦
                    if not isinstance(parsed, dict):
                        raise ValueError("Parsed result is not a dictionary")
                    return parsed
                else:
                    if attempt < 2:
                        logger.warning(f"âš ï¸ Attempt {attempt + 1}: Invalid JSON format, retrying...")
                        continue
                    else:
                        # ìµœì¢… ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’ ë°˜í™˜
                        logger.warning("âš ï¸ JSON parsing failed after 3 attempts. Using default verification result.")
                        return {
                            "approved": True,
                            "confidence": 0.6,
                            "feedback": "Verification parsing failed. Proceeding with plan.",
                            "suggested_changes": [],
                            "critical_issues": []
                        }
                        
            except json.JSONDecodeError as e:
                if attempt < 2:
                    logger.warning(f"âš ï¸ Attempt {attempt + 1}: JSON decode error: {e}, retrying...")
                    continue
                else:
                    logger.warning(f"âš ï¸ Failed to parse verification result after 3 attempts. Using default result.")
                    # ê¸°ë³¸ê°’ ë°˜í™˜í•˜ì—¬ ì—°êµ¬ ê³„ì† ì§„í–‰
                    return {
                        "approved": True,
                        "confidence": 0.6,
                        "feedback": f"Verification parsing failed: {str(e)}. Proceeding with plan.",
                        "suggested_changes": [],
                        "critical_issues": []
                    }
            except Exception as e:
                if attempt < 2:
                    logger.warning(f"âš ï¸ Attempt {attempt + 1}: Parse error: {e}, retrying...")
                    continue
                else:
                    logger.warning(f"âš ï¸ Verification parsing error: {e}. Using default result.")
                    # ê¸°ë³¸ê°’ ë°˜í™˜í•˜ì—¬ ì—°êµ¬ ê³„ì† ì§„í–‰
                    return {
                        "approved": True,
                        "confidence": 0.6,
                        "feedback": f"Verification error: {str(e)}. Proceeding with plan.",
                        "suggested_changes": [],
                        "critical_issues": []
                    }
        
        # ìµœì¢… fallback
        logger.warning("âš ï¸ Unexpected error in verification parsing. Using default result.")
        return {
            "approved": True,
            "confidence": 0.5,
            "feedback": "Verification parsing failed. Proceeding with plan.",
            "suggested_changes": [],
            "critical_issues": []
        }
    
    def _parse_evaluation_result(self, content: str) -> Dict[str, Any]:
        """í‰ê°€ ê²°ê³¼ íŒŒì‹± - ì¬ì‹œë„ ë¡œì§ ë° safety filter ì‘ë‹µ ì²˜ë¦¬ í¬í•¨."""
        import json
        import re
        
        # Safety filter ì‘ë‹µ ì²´í¬
        if not content or not isinstance(content, str):
            logger.warning("Evaluation result content is empty or invalid, using default")
            return self._get_default_evaluation_result()
        
        # Safety filterë¡œ ì°¨ë‹¨ëœ ì‘ë‹µ ì²´í¬
        safety_indicators = [
            "Content blocked by safety filters",
            "Unable to extract content",
            "[Content blocked",
            "safety filter",
            "finish_reason=2"
        ]
        
        content_lower = content.lower()
        if any(indicator.lower() in content_lower for indicator in safety_indicators):
            logger.warning("Evaluation result was blocked by safety filters, using default")
            return self._get_default_evaluation_result()
        
        # ìµœëŒ€ 3íšŒ ì¬ì‹œë„
        for attempt in range(3):
            try:
                # Markdown ì½”ë“œ ë¸”ë¡ ì œê±°
                cleaned_content = content.strip()
                if '```json' in cleaned_content:
                    match = re.search(r'```json\s*(.*?)\s*```', cleaned_content, re.DOTALL)
                    if match:
                        cleaned_content = match.group(1).strip()
                elif '```' in cleaned_content:
                    match = re.search(r'```\s*(.*?)\s*```', cleaned_content, re.DOTALL)
                    if match:
                        cleaned_content = match.group(1).strip()
                
                # JSON ê°ì²´ ì¶”ì¶œ ì‹œë„ (ì¤‘ê´„í˜¸ë¡œ ì‹œì‘í•˜ëŠ” ë¶€ë¶„ ì°¾ê¸°)
                if cleaned_content:
                    # JSON ê°ì²´ ì°¾ê¸°
                    json_match = re.search(r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}', cleaned_content, re.DOTALL)
                    if json_match:
                        cleaned_content = json_match.group(0)
                
                # JSON íŒŒì‹± ì‹œë„
                if cleaned_content.startswith('{'):
                    try:
                        result = json.loads(cleaned_content)
                        if isinstance(result, dict):
                            return result
                    except json.JSONDecodeError as je:
                        if attempt < 2:
                            logger.warning(f"âš ï¸ Attempt {attempt + 1}: JSON decode error: {je}, retrying...")
                            continue
                
                # JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’ ë°˜í™˜
                if attempt == 2:
                    logger.warning("Failed to parse evaluation result after 3 attempts, using default")
                    return self._get_default_evaluation_result()
                        
            except json.JSONDecodeError as e:
                if attempt < 2:
                    logger.warning(f"âš ï¸ Attempt {attempt + 1}: JSON decode error: {e}, retrying...")
                    continue
                else:
                    logger.warning(f"âŒ Failed to parse evaluation result after 3 attempts: {e}, using default")
                    return self._get_default_evaluation_result()
            except Exception as e:
                if attempt < 2:
                    logger.warning(f"âš ï¸ Attempt {attempt + 1}: Parse error: {e}, retrying...")
                    continue
                else:
                    logger.warning(f"âŒ Failed to parse evaluation result after 3 attempts: {e}, using default")
                    return self._get_default_evaluation_result()
        
        # ëª¨ë“  ì‹œë„ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’ ë°˜í™˜
        logger.warning("All parsing attempts failed, using default evaluation result")
        return self._get_default_evaluation_result()
    
    def _get_default_evaluation_result(self) -> Dict[str, Any]:
        """ê¸°ë³¸ í‰ê°€ ê²°ê³¼ ë°˜í™˜ (íŒŒì‹± ì‹¤íŒ¨ ë˜ëŠ” safety filter ì‘ë‹µ ì‹œ)."""
        return {
            "overall_score": 0.7,
            "objective_scores": {},
            "quality_metrics": {
                "completeness": 0.7,
                "accuracy": 0.7,
                "relevance": 0.7,
                "depth": 0.7
            },
            "improvement_areas": [],
            "needs_additional_work": False,
            "recommendations": ["Evaluation parsing failed, results may need manual review"],
            "parsing_failed": True,
            "safety_filter_blocked": True
        }
    
    def _create_priority_queue(self, state: ResearchState) -> List[Dict[str, Any]]:
        """ìš°ì„ ìˆœìœ„ í ìƒì„±."""
        tasks = state.get("planned_tasks", [])
        priority_queue = []
        
        for task in tasks:
            priority = 1 if task.get("priority") == "high" else 2 if task.get("priority") == "medium" else 3
            priority_queue.append({
                "task_id": task.get("task_id", ""),
                "priority": priority,
                "estimated_time": task.get("estimated_time", 30),
                "complexity": task.get("estimated_complexity", 5)
            })
        
        # ìš°ì„ ìˆœìœ„ë³„ë¡œ ì •ë ¬
        priority_queue.sort(key=lambda x: (x["priority"], x["complexity"]))
        return priority_queue
    
    def _generate_tool_parameters(self, task: Dict[str, Any], tool_name: str) -> Dict[str, Any]:
        """ë„êµ¬ ì‹¤í–‰ì„ ìœ„í•œ íŒŒë¼ë¯¸í„° ìë™ ìƒì„± ë° ê²€ì¦."""
        # ê¸°ì¡´ íŒŒë¼ë¯¸í„° ê°€ì ¸ì˜¤ê¸°
        parameters = task.get("parameters", {}).copy() if isinstance(task.get("parameters"), dict) else {}
        
        # task nameê³¼ descriptionì—ì„œ ê²€ìƒ‰ì–´ ì¶”ì¶œ
        task_name = task.get("name", "")
        task_description = task.get("description", "")
        combined_text = f"{task_name} {task_description}".strip()
        
        # ë„êµ¬ë³„ í•„ìˆ˜ íŒŒë¼ë¯¸í„° ë§¤í•‘
        tool_requirements = {
            # semantic_scholar ë„êµ¬ë“¤
            "semantic_scholar::papers-search-basic": {"query": True},
            "semantic_scholar::paper-search-advanced": {"query": True},
            "semantic_scholar::search-paper-title": {"title": True},
            "semantic_scholar::search-arxiv": {"query": True},
            "semantic_scholar::get-paper-abstract": {"paper_id": True},
            "semantic_scholar::papers-citations": {"paper_id": True},
            "semantic_scholar::papers-references": {"paper_id": True},
            # ê²€ìƒ‰ ë„êµ¬ë“¤
            "g-search": {"query": True},
            "tavily": {"query": True},
            "exa": {"query": True},
            "ddg_search::search": {"query": True},
            "tavily-mcp::tavily-search": {"query": True},
            "exa::web_search_exa": {"query": True},
            "WebSearch-MCP::web_search": {"query": True},
            # ë°ì´í„° ë„êµ¬ë“¤
            "fetch::fetch_url": {"url": True},
            "fetch::extract_elements": {"url": True, "selector": True},
            "fetch::get_page_metadata": {"url": True},
        }
        
        # ë„êµ¬ ì´ë¦„ í™•ì¸ (server::tool í˜•ì‹ ë˜ëŠ” ë‹¨ìˆœ ì´ë¦„)
        tool_key = tool_name
        if "::" not in tool_name:
            # ë‹¨ìˆœ ì´ë¦„ì¸ ê²½ìš° ë§¤í•‘ì—ì„œ ì°¾ê¸°
            for key in tool_requirements.keys():
                if key.endswith(f"::{tool_name}") or tool_name in key:
                    tool_key = key
                    break
        
        requirements = tool_requirements.get(tool_key, {})
        
        # í•„ìˆ˜ íŒŒë¼ë¯¸í„° ì²´í¬ ë° ìë™ ìƒì„±
        for param_name, is_required in requirements.items():
            if is_required and not parameters.get(param_name):
                # ê²€ìƒ‰ì–´ ìë™ ìƒì„±
                if param_name in ["query", "title"]:
                    if combined_text:
                        # task nameì—ì„œ í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ
                        # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ì¶”ì¶œ: ê¸´ ë¬¸ì¥ì„ ìš”ì•½í•˜ê±°ë‚˜ í•µì‹¬ í‚¤ì›Œë“œë§Œ ì‚¬ìš©
                        query = self._extract_search_query(combined_text)
                        parameters[param_name] = query
                        logger.info(f"âœ… Auto-generated {param_name} for {tool_name}: '{query}'")
                    else:
                        # task nameì´ ì—†ìœ¼ë©´ stateì—ì„œ user_request ì‚¬ìš©
                        # stateëŠ” taskì— ì§ì ‘ í¬í•¨ë˜ì–´ ìˆì§€ ì•Šìœ¼ë¯€ë¡œ, ê¸°ë³¸ê°’ìœ¼ë¡œ task name ì‚¬ìš©
                        fallback_text = task_name if task_name else "research"
                        query = self._extract_search_query(fallback_text)
                        parameters[param_name] = query
                        logger.info(f"âœ… Auto-generated {param_name} from fallback for {tool_name}: '{query}'")
        
        # ê¸°ë³¸ê°’ ì„¤ì •
        if "max_results" not in parameters and tool_key in ["g-search", "tavily", "exa", "ddg_search::search"]:
            parameters["max_results"] = 10
        if "num_results" not in parameters and "exa" in tool_key:
            parameters["num_results"] = parameters.get("max_results", 10)
        
        return parameters
    
    def _extract_search_query(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ì—ì„œ ê²€ìƒ‰ ì¿¼ë¦¬ ì¶”ì¶œ (ê°„ë‹¨í•œ í‚¤ì›Œë“œ ì¶”ì¶œ)."""
        if not text or not isinstance(text, str):
            return ""
        
        # í…ìŠ¤íŠ¸ ì •ë¦¬
        text = text.strip()
        if len(text) > 200:
            # ë„ˆë¬´ ê¸´ ê²½ìš° ì²« 200ìë§Œ ì‚¬ìš©
            text = text[:200]
        
        # ê¸°ë³¸ ê²€ìƒ‰ì–´ë¡œ ì‚¬ìš© (ë” ì •êµí•œ ì¶”ì¶œ í•„ìš” ì‹œ LLM ì‚¬ìš© ê°€ëŠ¥)
        # í˜„ì¬ëŠ” ê°„ë‹¨í•˜ê²Œ í…ìŠ¤íŠ¸ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ë˜ ë¶ˆí•„ìš”í•œ ë‹¨ì–´ ì œê±°
        import re
        # ì¼ë°˜ì ì¸ ë¶ˆí•„ìš”í•œ ë‹¨ì–´ ì œê±° (ì˜ë¬¸ ê¸°ì¤€)
        stop_words = ["the", "a", "an", "and", "or", "but", "in", "on", "at", "to", "for", "of", "with", "by"]
        words = re.findall(r'\b\w+\b', text.lower())
        filtered_words = [w for w in words if w not in stop_words and len(w) > 2]
        
        if filtered_words:
            # í•µì‹¬ í‚¤ì›Œë“œë§Œ ì‚¬ìš© (ìµœëŒ€ 10ê°œ)
            query = " ".join(filtered_words[:10])
            return query[:150]  # ìµœëŒ€ 150ì
        
        return text[:150]  # í•„í„°ë§ ì‹¤íŒ¨ ì‹œ ì›ë³¸ í…ìŠ¤íŠ¸ ì‚¬ìš©
    
    def _get_tool_category_for_task(self, task: Dict[str, Any]) -> ToolCategory:
        """ì‘ì—…ì— ì í•©í•œ ë„êµ¬ ì¹´í…Œê³ ë¦¬ ë°˜í™˜."""
        task_type = task.get("type", "research").lower()
        if "search" in task_type:
            return ToolCategory.SEARCH
        elif "academic" in task_type:
            return ToolCategory.ACADEMIC
        elif "data" in task_type:
            return ToolCategory.DATA
        else:
            return ToolCategory.SEARCH  # RESEARCH ëŒ€ì‹  SEARCH ì‚¬ìš©
    
    def _get_best_tool_for_category(self, category: ToolCategory) -> Optional[str]:
        """ì¹´í…Œê³ ë¦¬ì— ë§ëŠ” ìµœì ì˜ ë„êµ¬ ë°˜í™˜ - ì‹¤ì œ MCP ë„êµ¬ ì´ë¦„ ì‚¬ìš©."""
        tool_mapping = {
            ToolCategory.SEARCH: "g-search",  # ë¼ìš°íŒ…ë¨
            ToolCategory.DATA: "fetch::fetch_url",
            ToolCategory.CODE: "python_coder",
            ToolCategory.ACADEMIC: "semantic_scholar::papers-search-basic",
            ToolCategory.BUSINESS: "g-search"  # ë¹„ì¦ˆë‹ˆìŠ¤ ê²€ìƒ‰ë„ ì¼ë°˜ ê²€ìƒ‰ìœ¼ë¡œ
        }
        return tool_mapping.get(category, "g-search")  # ê¸°ë³¸ê°’ìœ¼ë¡œ g-search ì‚¬ìš©
    
    def _get_available_tools_for_category(self, category: ToolCategory) -> List[str]:
        """ì¹´í…Œê³ ë¦¬ë³„ ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ ëª©ë¡ (ìš°ì„ ìˆœìœ„ ìˆœ) - ì‹¤ì œ MCP ë„êµ¬ ì´ë¦„ ì‚¬ìš©."""
        tool_priorities = {
            ToolCategory.SEARCH: [
                "g-search",  # ë¼ìš°íŒ…ë¨
                "ddg_search::search",  # ì‹¤ì œ MCP ë„êµ¬
                "tavily-mcp::tavily-search",  # ì‹¤ì œ MCP ë„êµ¬
                "exa::web_search_exa",  # ì‹¤ì œ MCP ë„êµ¬
                "parallel-search",  # ì‹¤ì œ MCP ì„œë²„ (ë„êµ¬ ì´ë¦„ í™•ì¸ í•„ìš”)
                "WebSearch-MCP::web_search"  # ì‹¤ì œ MCP ë„êµ¬
            ],
            ToolCategory.ACADEMIC: [
                "arxiv::arxiv_search",  # arXiv MCP ì„œë²„ ìš°ì„ 
                "arxiv::arxiv_get_paper",  # arXiv MCP ì„œë²„
                "semantic_scholar::papers-search-basic",
                "semantic_scholar::paper-search-advanced",
                "semantic_scholar::search-paper-title",
                "semantic_scholar::search-arxiv",
                "arxiv"  # ë¡œì»¬ fallback
            ],
            ToolCategory.DATA: [
                "fetch::fetch_url",
                "fetch::extract_elements",
                "fetch::get_page_metadata",
                "ddg_search::fetch_content"
            ],
            ToolCategory.CODE: [
                "python_coder",
                "code_interpreter"
            ],
            ToolCategory.BUSINESS: [
                "g-search"  # ë¹„ì¦ˆë‹ˆìŠ¤ ê²€ìƒ‰ë„ ì¼ë°˜ ê²€ìƒ‰ìœ¼ë¡œ
            ]
        }
        return tool_priorities.get(category, ["g-search", "ddg_search::search"])
    
    def _validate_tool_result(self, tool_result: Dict[str, Any], task: Dict[str, Any]) -> bool:
        """ë„êµ¬ ì‹¤í–‰ ê²°ê³¼ ê²€ì¦."""
        if not tool_result.get("success", False):
            return False
        
        data = tool_result.get("data")
        if not data:
            return False
        
        # ê¸°ë³¸ ê²€ì¦: ë¹ˆ ë°ì´í„°ê°€ ì•„ë‹Œì§€ í™•ì¸
        if isinstance(data, str) and len(data.strip()) == 0:
            return False
        
        if isinstance(data, dict) and len(data) == 0:
            return False
        
        if isinstance(data, list) and len(data) == 0:
            return False
        
        # ê²€ìƒ‰ ê²°ê³¼ì˜ ê²½ìš° ìµœì†Œí•œì˜ ë‚´ìš©ì´ ìˆëŠ”ì§€ í™•ì¸
        if task.get("type") == "search":
            if isinstance(data, list) and len(data) > 0:
                # ê²€ìƒ‰ ê²°ê³¼ê°€ ìˆëŠ”ì§€ í™•ì¸
                return True
            elif isinstance(data, dict) and "results" in data:
                # êµ¬ì¡°í™”ëœ ê²€ìƒ‰ ê²°ê³¼ì¸ì§€ í™•ì¸
                return len(data["results"]) > 0
        
        # í•™ìˆ  ê²€ìƒ‰ì˜ ê²½ìš° ë…¼ë¬¸ ì •ë³´ê°€ ìˆëŠ”ì§€ í™•ì¸
        if task.get("type") == "academic":
            if isinstance(data, list) and len(data) > 0:
                return True
            elif isinstance(data, dict) and ("papers" in data or "entries" in data):
                return True
        
        # ê¸°ë³¸ì ìœ¼ë¡œ ë°ì´í„°ê°€ ìˆìœ¼ë©´ ìœ íš¨í•œ ê²ƒìœ¼ë¡œ ê°„ì£¼
        return True
    
    def _extract_text_for_similarity(self, data: Any) -> str:
        """ìœ ì‚¬ë„ ê³„ì‚°ì„ ìœ„í•œ í…ìŠ¤íŠ¸ ì¶”ì¶œ - íƒ€ì… ì•ˆì „ì„± ê°œì„ ."""
        try:
            # íƒ€ì… ê²€ì¦
            if data is None:
                return ""
            
            # ë¬¸ìì—´ì¸ ê²½ìš° ê·¸ëŒ€ë¡œ ë°˜í™˜
            if isinstance(data, str):
                return data.strip() if data.strip() else ""
            
            # ë”•ì…”ë„ˆë¦¬ê°€ ì•„ë‹Œ ê²½ìš° ë¬¸ìì—´ë¡œ ë³€í™˜
            if not isinstance(data, dict):
                return str(data).strip() if str(data).strip() else ""
            
            # ë”•ì…”ë„ˆë¦¬ ì²˜ë¦¬
            text_parts = []
            
            # ì£¼ìš” í…ìŠ¤íŠ¸ í•„ë“œë“¤ ì¶”ì¶œ
            text_fields = ["title", "content", "summary", "description", "abstract", "snippet"]
            for field in text_fields:
                if field in data and data[field]:
                    value = data[field]
                    if isinstance(value, str):
                        text_parts.append(value.strip())
                    else:
                        text_parts.append(str(value).strip())
            
            # ë”•ì…”ë„ˆë¦¬ ê°’ë“¤ ì¤‘ ë¬¸ìì—´ì¸ ê²ƒë“¤ ì¶”ì¶œ
            for key, value in data.items():
                if key not in text_fields and isinstance(value, str) and value.strip():
                    text_parts.append(value.strip())
                elif key not in text_fields and value is not None:
                    # ë¦¬ìŠ¤íŠ¸ë‚˜ ë‹¤ë¥¸ íƒ€ì…ë„ ë¬¸ìì—´ë¡œ ë³€í™˜ ì‹œë„
                    try:
                        str_value = str(value).strip()
                        if str_value and len(str_value) < 500:  # ë„ˆë¬´ ê¸´ ê°’ì€ ì œì™¸
                            text_parts.append(str_value)
                    except Exception:
                        pass
            
            result = " ".join(text_parts)
            return result if result.strip() else ""
        except Exception as e:
            logger.warning(f"Text extraction failed: {e}")
            return ""
    
    def _calculate_semantic_similarity(self, text1: str, text2: str) -> float:
        """Semantic similarity ê³„ì‚° (ê°„ë‹¨í•œ ë²„ì „)."""
        try:
            if not text1 or not text2:
                return 0.0
            
            # ë‹¨ì–´ ë‹¨ìœ„ë¡œ ë¶„í• 
            words1 = set(text1.lower().split())
            words2 = set(text2.lower().split())
            
            if not words1 or not words2:
                return 0.0
            
            # Jaccard similarity ê³„ì‚°
            intersection = len(words1.intersection(words2))
            union = len(words1.union(words2))
            
            jaccard_similarity = intersection / union if union > 0 else 0.0
            
            # ê³µí†µ ë‹¨ì–´ ë¹„ìœ¨ë„ ê³ ë ¤
            common_ratio = intersection / min(len(words1), len(words2))
            
            # ë‘ ì§€í‘œì˜ ê°€ì¤‘ í‰ê· 
            similarity = (jaccard_similarity * 0.6 + common_ratio * 0.4)
            
            return min(similarity, 1.0)
            
        except Exception as e:
            logger.warning(f"Similarity calculation failed: {e}")
            return 0.0
    
    async def _self_verification(self, result: Dict[str, Any]) -> float:
        """ìì²´ ê²€ì¦ - ì‹¤ì œ ë°ì´í„° í’ˆì§ˆ í‰ê°€."""
        try:
            data = result.get("compressed_data", {})
            if not data:
                return 0.0
            
            quality_score = 0.0
            
            # 1. ë°ì´í„° ì™„ì„±ë„ ê²€ì¦
            if isinstance(data, dict):
                non_empty_fields = len([v for v in data.values() if v and str(v).strip()])
                total_fields = len(data)
                completeness = non_empty_fields / max(total_fields, 1)
                quality_score += completeness * 0.25
                
                # í•„ìˆ˜ í•„ë“œ ì¡´ì¬ ì—¬ë¶€
                essential_fields = ["title", "content", "summary"]
                essential_present = sum(1 for field in essential_fields if field in data and data[field])
                essential_score = essential_present / len(essential_fields)
                quality_score += essential_score * 0.25
            
            # 2. ë°ì´í„° ì¼ê´€ì„± ê²€ì¦
            if isinstance(data, dict):
                consistency_score = 0.0
                
                # ì œëª©ê³¼ ë‚´ìš©ì˜ ì¼ê´€ì„±
                if "title" in data and "content" in data:
                    title = str(data["title"]).lower()
                    content = str(data["content"]).lower()
                    if title and content:
                        # ì œëª©ì˜ í‚¤ì›Œë“œê°€ ë‚´ìš©ì— í¬í•¨ë˜ëŠ”ì§€ í™•ì¸
                        title_words = set(title.split())
                        content_words = set(content.split())
                        if len(title_words) > 0:
                            overlap = len(title_words.intersection(content_words)) / len(title_words)
                            consistency_score += overlap * 0.5
                
                # ìš”ì•½ê³¼ ë‚´ìš©ì˜ ì¼ê´€ì„±
                if "summary" in data and "content" in data:
                    summary = str(data["summary"]).lower()
                    content = str(data["content"]).lower()
                    if summary and content:
                        summary_words = set(summary.split())
                        content_words = set(content.split())
                        if len(summary_words) > 0:
                            overlap = len(summary_words.intersection(content_words)) / len(summary_words)
                            consistency_score += overlap * 0.5
                
                quality_score += consistency_score * 0.25
            
            # 3. ì••ì¶• í’ˆì§ˆ ê²€ì¦
            compression_ratio = result.get("compression_ratio", 1.0)
            original_size = result.get("original_size", 0)
            compressed_size = result.get("compressed_size", 0)
            
            if original_size > 0 and compressed_size > 0:
                actual_ratio = compressed_size / original_size
                # ì ì ˆí•œ ì••ì¶•ë¥  (0.1 ~ 0.8)ì¼ ë•Œ ë†’ì€ ì ìˆ˜
                if 0.1 <= actual_ratio <= 0.8:
                    compression_score = 1.0
                elif actual_ratio < 0.1:
                    compression_score = 0.7  # ê³¼ë„í•œ ì••ì¶•
                else:
                    compression_score = 0.5  # ì••ì¶• ë¶€ì¡±
                
                quality_score += compression_score * 0.25
            
            return min(quality_score, 1.0)
        except Exception as e:
            logger.error(f"âŒ Self verification failed: {e}")
            return 0.0
    
    async def _cross_verification(self, result: Dict[str, Any], all_results: List[Dict[str, Any]]) -> float:
        """êµì°¨ ê²€ì¦ - Semantic Similarity ê¸°ë°˜."""
        try:
            if not all_results or len(all_results) < 2:
                return 0.5
            
            current_data = result.get("compressed_data", {})
            if not current_data:
                return 0.0
            
            # í˜„ì¬ ê²°ê³¼ì˜ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            current_text = self._extract_text_for_similarity(current_data)
            if not current_text:
                return 0.5
            
            similarity_scores = []
            
            for other_result in all_results:
                if other_result.get("task_id") == result.get("task_id"):
                    continue
                
                other_data = other_result.get("compressed_data", {})
                if not other_data:
                    continue
                
                other_text = self._extract_text_for_similarity(other_data)
                if not other_text:
                    continue
                
                # Semantic similarity ê³„ì‚°
                similarity = self._calculate_semantic_similarity(current_text, other_text)
                similarity_scores.append(similarity)
            
            if similarity_scores:
                # í‰ê·  ìœ ì‚¬ë„ ë°˜í™˜ (0.3-0.7 ë²”ìœ„ê°€ ì ì ˆ)
                avg_similarity = sum(similarity_scores) / len(similarity_scores)
                # ë„ˆë¬´ ë†’ê±°ë‚˜ ë‚®ì€ ìœ ì‚¬ë„ëŠ” ì¡°ì •
                if avg_similarity > 0.9:
                    return 0.8  # ë„ˆë¬´ ìœ ì‚¬í•˜ë©´ ì˜ì‹¬ìŠ¤ëŸ¬ì›€
                elif avg_similarity < 0.1:
                    return 0.3  # ë„ˆë¬´ ë‹¤ë¥´ë©´ ì¼ê´€ì„± ë¶€ì¡±
                else:
                    return avg_similarity
            else:
                return 0.5
                
        except Exception as e:
            logger.error(f"âŒ Cross verification failed: {e}")
            return 0.3
    
    async def _external_verification(self, result: Dict[str, Any]) -> float:
        """ì™¸ë¶€ ê²€ì¦."""
        try:
            # MCP ë„êµ¬ë¥¼ ì‚¬ìš©í•œ ì™¸ë¶€ ê²€ì¦
            task_id = result.get("task_id", "")
            data = result.get("compressed_data", {})
            
            if not data or not task_id:
                return 0.5
            
            # ê°„ë‹¨í•œ ì™¸ë¶€ ê²€ì¦ (ì‹¤ì œë¡œëŠ” MCP ë„êµ¬ í™œìš©)
            # ì—¬ê¸°ì„œëŠ” ê¸°ë³¸ì ì¸ ë°ì´í„° ìœ íš¨ì„±ë§Œ ê²€ì‚¬
            if isinstance(data, dict) and len(data) > 0:
                return 0.8
            elif isinstance(data, list) and len(data) > 0:
                return 0.7
            else:
                return 0.6
                
        except Exception as e:
            logger.error(f"âŒ External verification failed: {e}")
            return 0.0
    
    def _calculate_validation_score(self, state: ResearchState) -> float:
        """ê²€ì¦ ì ìˆ˜ ê³„ì‚°."""
        try:
            confidence_scores = state.get("confidence_scores", {})
            if not confidence_scores:
                return 0.0
            
            # í‰ê·  ì‹ ë¢°ë„ ì ìˆ˜ ê³„ì‚°
            total_score = sum(confidence_scores.values())
            avg_score = total_score / len(confidence_scores)
            
            # í’ˆì§ˆ ë©”íŠ¸ë¦­ ë°˜ì˜
            quality_metrics = state.get("quality_metrics", {})
            if quality_metrics:
                quality_score = quality_metrics.get("overall_quality", 0.8)
                avg_score = (avg_score + quality_score) / 2
            
            return min(avg_score, 1.0)
        except Exception as e:
            logger.error(f"âŒ Validation score calculation failed: {e}")
            return 0.0
    
    def _identify_missing_elements(self, state: ResearchState) -> List[str]:
        """ëˆ„ë½ëœ ìš”ì†Œ ì‹ë³„."""
        try:
            missing_elements = []
            
            # í•„ìˆ˜ í•„ë“œ ê²€ì‚¬
            required_fields = ["analyzed_objectives", "planned_tasks", "execution_results"]
            for field in required_fields:
                if not state.get(field):
                    missing_elements.append(f"Missing {field}")
            
            # ì‹¤í–‰ ê²°ê³¼ ê²€ì‚¬
            execution_results = state.get("execution_results", [])
            if not execution_results:
                missing_elements.append("No execution results found")
            
            # ì••ì¶• ê²°ê³¼ ê²€ì‚¬
            compression_results = state.get("compression_results", [])
            if not compression_results:
                missing_elements.append("No compression results found")
            
            # ê²€ì¦ ê²°ê³¼ ê²€ì‚¬
            verification_stages = state.get("verification_stages", [])
            if not verification_stages:
                missing_elements.append("No verification results found")
            
            return missing_elements
        except Exception as e:
            logger.error(f"âŒ Missing elements identification failed: {e}")
            return ["Error in missing elements analysis"]
    
    def _decide_next_step_based_on_context(self, state: ResearchState) -> str:
        """
        ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ë‹¤ìŒ ë‹¨ê³„ ìë™ ê²°ì • (ì¬ê·€ì  ì»¨í…ìŠ¤íŠ¸ ì‚¬ìš©).
        
        Args:
            state: í˜„ì¬ ìƒíƒœ
        
        Returns:
            ë‹¤ìŒ ë‹¨ê³„ ì´ë¦„
        """
        current_context = self.context_manager.get_current_context()
        
        if not current_context:
            # ì»¨í…ìŠ¤íŠ¸ê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ íë¦„
            return "compress"
        
        # ì»¨í…ìŠ¤íŠ¸ ì™„ì „ì„± í‰ê°€
        completeness = self.context_manager.evaluate_context_completeness(current_context.context_id)
        
        # ì‹¤í–‰ ê²°ê³¼ í™•ì¸
        execution_results = state.get("execution_results", [])
        successful_results = [r for r in execution_results if r.get("status") == "completed"]
        success_rate = len(successful_results) / max(len(execution_results), 1)
        
        logger.debug(f"Context completeness: {completeness:.2f}, Success rate: {success_rate:.2f}")
        
        if completeness < 0.5 or success_rate < 0.5:
            # ì»¨í…ìŠ¤íŠ¸ê°€ ë¶ˆì™„ì „í•˜ê±°ë‚˜ ì„±ê³µë¥ ì´ ë‚®ìœ¼ë©´ ì¶”ê°€ ì—°êµ¬
            logger.info("ğŸ”„ Context incomplete or low success rate - continuing research")
            return "continue_research"
        elif completeness < 0.8:
            # ì»¨í…ìŠ¤íŠ¸ê°€ ê±°ì˜ ì™„ì „í•˜ë©´ ì••ì¶• í›„ ê²€ì¦
            logger.info("ğŸ“¦ Context nearly complete - compressing")
            return "compress"
        else:
            # ì»¨í…ìŠ¤íŠ¸ê°€ ì™„ì „í•˜ë©´ ê²€ì¦
            logger.info("âœ… Context complete - verifying")
            return "verify"
    
    def _calculate_context_usage(self, state: ResearchState, content: str) -> Dict[str, Any]:
        """ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš° ì‚¬ìš©ëŸ‰ ê³„ì‚°."""
        try:
            # ê°„ë‹¨í•œ í† í° ìˆ˜ ì¶”ì • (ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ í† í°í™” í•„ìš”)
            estimated_tokens = len(content.split()) * 1.3  # ëŒ€ëµì ì¸ í† í° ìˆ˜
            
            # ìµœëŒ€ í† í° ìˆ˜ (ëª¨ë¸ë³„ë¡œ ë‹¤ë¦„)
            max_tokens = 100000  # ê¸°ë³¸ê°’
            
            usage_ratio = min(estimated_tokens / max_tokens, 1.0)
            
            return {
                "usage_ratio": usage_ratio,
                "tokens_used": int(estimated_tokens),
                "max_tokens": max_tokens,
                "efficiency": 1.0 - usage_ratio
            }
        except Exception as e:
            logger.error(f"âŒ Context usage calculation failed: {e}")
            return {
                "usage_ratio": 0.0,
                "tokens_used": 0,
                "max_tokens": 100000,
                "efficiency": 1.0
            }
    
    async def run_research(self, user_request: str, context: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """ì—°êµ¬ ì‹¤í–‰ (Production-Grade Reliability + ExecutionContext)."""
        logger.info(f"ğŸš€ Starting research with 8 core innovations: {user_request}")
        
        # ExecutionContext ì„¤ì • (ROMA ìŠ¤íƒ€ì¼)
        execution_id = f"exec_{int(datetime.now().timestamp())}"
        context_token = None
        try:
            from src.core.recursive_context_manager import ExecutionContext
            context_token = ExecutionContext.set(execution_id, self.context_manager)
            logger.debug(f"ExecutionContext set for execution: {execution_id}")
        except Exception as e:
            logger.debug(f"Failed to set ExecutionContext: {e}")
        
        # CLI ëª¨ë“œ ê°ì§€ ë° autopilot ëª¨ë“œ ì„¤ì •
        import sys
        is_cli_mode = (
            not hasattr(sys, 'ps1') and  # Interactive shellì´ ì•„ë‹˜
            'streamlit' not in sys.modules and  # Streamlitì´ ë¡œë“œë˜ì§€ ì•ŠìŒ
            not any('streamlit' in str(arg) for arg in sys.argv)  # Streamlit ì‹¤í–‰ ì¸ìê°€ ì—†ìŒ
        )
        
        # ì´ˆê¸° ìƒíƒœ ì„¤ì •
        initial_state = ResearchState(
            user_request=user_request,
            context=context or {},
            objective_id=execution_id,  # execution_id ì‚¬ìš©
            analyzed_objectives=[],
            intent_analysis={},
            domain_analysis={},
            scope_analysis={},
            # Planning Agent í•„ë“œ
            preliminary_research={},
            planned_tasks=[],
            agent_assignments={},
            execution_plan={},
            plan_approved=False,
            plan_feedback=None,
            plan_iteration=0,
            execution_results=[],
            agent_status={},
            execution_metadata={},
            streaming_data=[],
            compression_results=[],
            compression_metadata={},
            verification_results={},
            confidence_scores={},
            verification_stages=[],
            evaluation_results={},
            quality_metrics={},
            improvement_areas=[],
            validation_results={},
            validation_score=0.0,
            missing_elements=[],
            final_synthesis={},
            deliverable_path=None,
            synthesis_metadata={},
            context_window_usage={},
            pending_questions=[],
            user_responses={},
            clarification_context={},
            waiting_for_user=False,
            autopilot_mode=is_cli_mode,  # CLI ëª¨ë“œì´ë©´ autopilot í™œì„±í™”
            current_step="analyze_objectives",
            iteration=0,
            max_iterations=10,
            should_continue=True,
            error_message=None,
            innovation_stats={},
            messages=[]
        )
        
        if is_cli_mode:
            logger.info("ğŸ¤– CLI mode detected - Autopilot mode enabled (auto-selecting responses)")

        # LangGraph ì›Œí¬í”Œë¡œìš° ì‹¤í–‰
        logger.info("ğŸ”„ Executing LangGraph workflow with 8 core innovations")
        final_state = await self.graph.ainvoke(initial_state)
        
        # ê²°ê³¼ í¬ë§·íŒ…
        result = {
            "content": final_state.get("final_synthesis", {}).get("content", "Research completed"),
            "metadata": {
                "model_used": final_state.get("final_synthesis", {}).get("model_used", "unknown"),
                "execution_time": final_state.get("final_synthesis", {}).get("execution_time", 0.0),
                "cost": 0.0,
                "confidence": final_state.get("final_synthesis", {}).get("confidence", 0.9)
            },
            "synthesis_results": {
                "content": final_state.get("final_synthesis", {}).get("content", ""),
                "original_length": len(str(final_state.get("execution_results", []))),
                "compressed_length": len(str(final_state.get("compression_results", []))),
                "compression_ratio": final_state.get("compression_metadata", {}).get("overall_compression_ratio", 1.0)
            },
            "innovation_stats": final_state.get("innovation_stats", {}),
            "system_health": {"overall_status": "healthy", "health_score": 95},
            "detailed_results": {
                "analyzed_objectives": final_state.get("analyzed_objectives", []),
                "planned_tasks": final_state.get("planned_tasks", []),
                "execution_results": final_state.get("execution_results", []),
                "compression_results": final_state.get("compression_results", []),
                "verification_stages": final_state.get("verification_stages", []),
                "evaluation_results": final_state.get("evaluation_results", {}),
                "quality_metrics": final_state.get("quality_metrics", {})
            }
        }
        
        logger.info("âœ… Research completed successfully with 8 core innovations")
        
        # ExecutionContext ë° MCP Hub ì •ë¦¬ (ROMA ìŠ¤íƒ€ì¼)
        try:
            from src.core.recursive_context_manager import ExecutionContext
            if context_token:
                ExecutionContext.reset(context_token)
                logger.debug(f"ExecutionContext reset for execution: {execution_id}")
        except Exception as e:
            logger.debug(f"Failed to reset ExecutionContext: {e}")
        
        # MCP Hub ì‹¤í–‰ ì„¸ì…˜ ì •ë¦¬
        try:
            from src.core.mcp_integration import get_mcp_hub
            mcp_hub = get_mcp_hub()
            await mcp_hub.cleanup_execution(execution_id)
        except Exception as e:
            logger.debug(f"Failed to cleanup MCP Hub execution session: {e}")
        
        return result
    
    async def _search_similar_research(self, query: str, user_id: str) -> List[Dict[str, Any]]:
        """ìœ ì‚¬í•œ ê³¼ê±° ì—°êµ¬ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤."""
        try:
            # í•˜ì´ë¸Œë¦¬ë“œ ìŠ¤í† ë¦¬ì§€ì—ì„œ ìœ ì‚¬ ì—°êµ¬ ê²€ìƒ‰
            similar_research = await self.hybrid_storage.search_similar_research(
                query=query,
                user_id=user_id,
                limit=5,
                similarity_threshold=0.3
            )
            
            # ê²°ê³¼ í¬ë§·íŒ…
            formatted_results = []
            for research in similar_research:
                formatted_results.append({
                    'research_id': research.research_id,
                    'topic': research.metadata.get('topic', ''),
                    'summary': research.summary,
                    'similarity_score': research.similarity_score,
                    'timestamp': research.timestamp.isoformat(),
                    'confidence_score': research.metadata.get('confidence_score', 0.0)
                })
            
            logger.info(f"Found {len(formatted_results)} similar research results")
            return formatted_results
            
        except Exception as e:
            logger.error(f"Failed to search similar research: {e}")
            return []
    
    async def _overseer_initial_review(self, state: ResearchState) -> ResearchState:
        """Overseerì˜ ì´ˆê¸° ê²€í†  - Planning í›„ ìš”êµ¬ì‚¬í•­ ì •ì˜"""
        logger.info("=" * 80)
        logger.info("ğŸ” [OVERSEER] Initial Review - Defining Requirements")
        logger.info("=" * 80)
        
        try:
            from src.agents.greedy_overseer_agent import get_greedy_overseer_agent
            from src.core.researcher_config import load_config_from_env
            
            config = load_config_from_env()
            overseer_config = config.overseer if hasattr(config, 'overseer') else None
            
            if overseer_config and overseer_config.enabled:
                overseer = get_greedy_overseer_agent(
                    max_iterations=overseer_config.max_iterations,
                    completeness_threshold=overseer_config.completeness_threshold,
                    quality_threshold=overseer_config.quality_threshold,
                    min_academic_sources=overseer_config.min_academic_sources,
                    min_verified_sources=overseer_config.min_verified_sources,
                    require_cross_validation=overseer_config.require_cross_validation,
                    enable_human_loop=overseer_config.enable_human_loop
                )
            else:
                # Default configuration
                overseer = get_greedy_overseer_agent()
            
            state = await overseer.review_planning_output(state)
            
            logger.info(f"[OVERSEER] Requirements defined: {len(state.get('overseer_requirements', []))}")
            
        except Exception as e:
            logger.error(f"[OVERSEER] Initial review failed: {e}")
            # Continue with default requirements
            state['overseer_iterations'] = 0
            state['overseer_requirements'] = []
        
        return state
    
    async def _overseer_evaluation(self, state: ResearchState) -> ResearchState:
        """Overseer í‰ê°€ - Execution + Validation í›„ ê²°ê³¼ í‰ê°€"""
        logger.info("=" * 80)
        logger.info("ğŸ” [OVERSEER] Evaluating Execution Results")
        logger.info("=" * 80)
        
        try:
            from src.agents.greedy_overseer_agent import get_greedy_overseer_agent
            from src.core.researcher_config import load_config_from_env
            
            config = load_config_from_env()
            overseer_config = config.overseer if hasattr(config, 'overseer') else None
            
            if overseer_config and overseer_config.enabled:
                overseer = get_greedy_overseer_agent(
                    max_iterations=overseer_config.max_iterations,
                    completeness_threshold=overseer_config.completeness_threshold,
                    quality_threshold=overseer_config.quality_threshold,
                    min_academic_sources=overseer_config.min_academic_sources,
                    min_verified_sources=overseer_config.min_verified_sources,
                    require_cross_validation=overseer_config.require_cross_validation,
                    enable_human_loop=overseer_config.enable_human_loop
                )
            else:
                overseer = get_greedy_overseer_agent()
            
            state = await overseer.evaluate_execution_results(state)
            
            decision = state.get('overseer_decision', 'proceed')
            logger.info(f"[OVERSEER] Decision: {decision}")
            
        except Exception as e:
            logger.error(f"[OVERSEER] Evaluation failed: {e}")
            # Default to proceed on error
            state['overseer_decision'] = 'proceed'
        
        return state
    
    def _overseer_decision_router(self, state: ResearchState) -> str:
        """Overseer ê²°ì •ì— ë”°ë¥¸ ë¼ìš°íŒ…"""
        decision = state.get('overseer_decision', 'proceed')
        current_iteration = state.get('overseer_iterations', 0)
        max_iterations = 5  # Default max
        
        try:
            from src.core.researcher_config import load_config_from_env
            config = load_config_from_env()
            if hasattr(config, 'overseer') and config.overseer.enabled:
                max_iterations = config.overseer.max_iterations
        except:
            pass
        
        logger.info(f"[OVERSEER] Routing decision: {decision} (iteration {current_iteration}/{max_iterations})")
        
        if decision == 'retry' and current_iteration < max_iterations:
            logger.info(f"[OVERSEER] Retrying execution (iteration {current_iteration + 1})")
            return 'retry'
        elif decision == 'ask_user':
            logger.info("[OVERSEER] Requesting user clarification")
            return 'waiting_for_clarification'
        else:
            logger.info("[OVERSEER] Proceeding to evaluation")
            return 'proceed'
    
    async def _save_research_memory(self, state: ResearchState) -> bool:
        """ì—°êµ¬ ê²°ê³¼ë¥¼ ë©”ëª¨ë¦¬ì— ì €ì¥í•©ë‹ˆë‹¤."""
        try:
            from src.storage.vector_store import ResearchMemory
            
            # ì—°êµ¬ ë©”ëª¨ë¦¬ ìƒì„±
            memory = ResearchMemory(
                research_id=state['objective_id'],
                user_id=state.get('user_id', 'default_user'),
                topic=state['user_request'],
                timestamp=datetime.now(timezone.utc),
                embedding=[],  # í•˜ì´ë¸Œë¦¬ë“œ ìŠ¤í† ë¦¬ì§€ì—ì„œ ìƒì„±
                metadata={
                    'complexity_score': state.get('complexity_score', 0.0),
                    'objectives_count': len(state.get('analyzed_objectives', [])),
                    'execution_results': state.get('execution_results', []),
                    'verification_results': state.get('verification_results', {}),
                    'quality_metrics': state.get('quality_metrics', {})
                },
                results=state.get('final_synthesis', {}),
                content=state.get('final_synthesis', {}).get('content', ''),
                summary=state.get('final_synthesis', {}).get('summary', ''),
                keywords=state.get('final_synthesis', {}).get('keywords', []),
                confidence_score=state.get('final_synthesis', {}).get('confidence', 0.0),
                source_count=len(state.get('execution_results', [])),
                verification_status=state.get('verification_results', {}).get('status', 'unverified')
            )
            
            # í•˜ì´ë¸Œë¦¬ë“œ ìŠ¤í† ë¦¬ì§€ì— ì €ì¥
            success = await self.hybrid_storage.store_research(
                research_id=memory.research_id,
                user_id=memory.user_id,
                topic=memory.topic,
                content=memory.content,
                results=memory.results,
                metadata=memory.metadata,
                summary=memory.summary,
                keywords=memory.keywords
            )
            
            if success:
                logger.info(f"Research memory saved: {memory.research_id}")
            else:
                logger.warning(f"Failed to save research memory: {memory.research_id}")
            
            return success
            
        except Exception as e:
            logger.error(f"Failed to save research memory: {e}")
            return False


# Global orchestrator instance (lazy initialization)
_orchestrator = None

def get_orchestrator() -> 'AutonomousOrchestrator':
    """Get or initialize global orchestrator."""
    global _orchestrator
    if _orchestrator is None:
        _orchestrator = AutonomousOrchestrator()
    return _orchestrator

async def run_research(user_request: str, context: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
    """ì—°êµ¬ ì‹¤í–‰."""
    orchestrator = get_orchestrator()
    return await orchestrator.run_research(user_request, context)

