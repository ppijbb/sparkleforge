#!/usr/bin/env python3
"""
Parallel Agent Executor for Local Researcher Project

ë³‘ë ¬ agent ì‹¤í–‰ ê´€ë¦¬ ì‹œìŠ¤í…œ.
ì—¬ëŸ¬ agentë¥¼ ë™ì‹œì— ì‹¤í–‰í•˜ê³ , ì‘ì—…ì„ ë¶„í• í•˜ì—¬ ë³‘ë ¬ ì²˜ë¦¬í•˜ë©°, ê²°ê³¼ë¥¼ ìˆ˜ì§‘ í†µí•©í•©ë‹ˆë‹¤.
"""

import asyncio
import logging
from typing import Dict, Any, List, Optional, Callable
from datetime import datetime
from collections import defaultdict

from src.core.task_queue import TaskQueue, TaskQueueItem
from src.core.agent_pool import AgentPool
from src.core.researcher_config import get_agent_config, get_research_config, get_mcp_config
from src.core.mcp_integration import execute_tool
from src.core.streaming_manager import get_streaming_manager, EventType
from src.core.agent_result_sharing import SharedResultsManager, AgentDiscussionManager
from src.core.task_validator import TaskValidator, ValidationResult
from src.core.result_cache import get_result_cache
from src.core.error_handler import get_error_handler
from src.core.concurrency_manager import get_concurrency_manager

logger = logging.getLogger(__name__)


class ParallelAgentExecutor:
    """ë³‘ë ¬ agent ì‹¤í–‰ ê´€ë¦¬ì."""
    
    def __init__(self):
        """ì´ˆê¸°í™”."""
        self.agent_config = get_agent_config()
        self.research_config = get_research_config()
        self.mcp_config = get_mcp_config()
        self.concurrency_manager = get_concurrency_manager()
        # Use dynamic concurrency if available, otherwise use config value
        self.max_concurrent = self.concurrency_manager.get_current_concurrency() or self.agent_config.max_concurrent_research_units
        
        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.task_queue = TaskQueue()
        self.agent_pool = AgentPool(max_pool_size=self.max_concurrent * 2)
        self.streaming_manager = get_streaming_manager()
        self.task_validator = TaskValidator()
        self.result_cache = get_result_cache()
        self.error_handler = get_error_handler()
        
        # ì‹¤í–‰ ìƒíƒœ
        self.active_tasks: Dict[str, asyncio.Task] = {}
        self.completed_results: List[Dict[str, Any]] = []
        self.failed_tasks: List[Dict[str, Any]] = []
        
        # ê²°ê³¼ ê³µìœ  ë° í† ë¡  (objective_idê°€ ì„¤ì •ë˜ë©´ ì´ˆê¸°í™”)
        self.shared_results_manager: Optional[SharedResultsManager] = None
        self.discussion_manager: Optional[AgentDiscussionManager] = None
        
        logger.info(f"ParallelAgentExecutor initialized with max_concurrent={self.max_concurrent}")
        
        # Start concurrency monitoring (only if event loop is running)
        try:
            loop = asyncio.get_running_loop()
            asyncio.create_task(self.concurrency_manager.start_monitoring())
        except RuntimeError:
            # No event loop running, will be started later
            logger.debug("No event loop running, concurrency monitoring will start later")
    
    async def execute_parallel_tasks(
        self,
        tasks: List[Dict[str, Any]],
        agent_assignments: Dict[str, List[str]],
        execution_plan: Dict[str, Any],
        objective_id: str
    ) -> Dict[str, Any]:
        """ë³‘ë ¬ ì‘ì—… ì‹¤í–‰ (ì˜ì¡´ì„± ê·¸ë˜í”„ ê¸°ë°˜ ìŠ¤ë§ˆíŠ¸ ìŠ¤ì¼€ì¤„ë§)."""
        logger.info(f"Starting parallel execution of {len(tasks)} tasks (with dependency graph)")
        
        # ê²°ê³¼ ê³µìœ  ë° í† ë¡  ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        if self.agent_config.enable_agent_communication:
            self.shared_results_manager = SharedResultsManager(objective_id=objective_id)
            self.discussion_manager = AgentDiscussionManager(
                objective_id=objective_id,
                shared_results_manager=self.shared_results_manager
            )
            logger.info("âœ… Agent result sharing and discussion enabled")
        
        # ì˜ì¡´ì„± ê·¸ë˜í”„ êµ¬ì¶•
        try:
            from src.core.task_dependency_graph import TaskDependencyGraph
            dependency_graph = TaskDependencyGraph(tasks)
            logger.info(f"âœ… Dependency graph built: {dependency_graph.get_statistics()}")
            use_dependency_graph = True
            
            # DAG ì‹œê°í™” ì´ˆê¸°í™”
            try:
                from src.core.dag_visualizer import get_dag_visualizer
                visualizer = get_dag_visualizer()
                visualizer.initialize(tasks)
                logger.info("âœ… DAG Visualizer initialized")
            except Exception as e:
                logger.debug(f"DAG Visualizer initialization failed: {e}")
        except ImportError as e:
            logger.warning(f"âš ï¸ TaskDependencyGraph not available: {e}. Falling back to simple parallel groups.")
            use_dependency_graph = False
            dependency_graph = None
            # ì‘ì—… í ì´ˆê¸°í™” (fallback)
            self.task_queue.add_tasks(tasks)
            parallel_groups = execution_plan.get('parallel_groups', [])
            if parallel_groups:
                logger.info(f"Using {len(parallel_groups)} parallel groups from execution plan")
                self.task_queue.parallel_groups = parallel_groups
        
        # ìŠ¤íŠ¸ë¦¬ë° ì´ë²¤íŠ¸: ì‹¤í–‰ ì‹œì‘
        await self.streaming_manager.stream_event(
            event_type=EventType.WORKFLOW_START,
            agent_id="parallel_executor",
            workflow_id=objective_id,
            data={
                'stage': 'parallel_execution',
                'message': 'Starting parallel task execution',
                'total_tasks': len(tasks),
                'max_concurrent': self.max_concurrent,
                'strategy': execution_plan.get('strategy', 'sequential'),
                'use_dependency_graph': use_dependency_graph
            },
            priority=1
        )
        
        # ë³‘ë ¬ ì‹¤í–‰ ì‹œì‘
        execution_start = datetime.now()
        if use_dependency_graph:
            results = await self._execute_with_dependency_graph(
                dependency_graph,
                agent_assignments,
                execution_plan,
                objective_id
            )
        else:
            results = await self._execute_with_parallel_groups(
                agent_assignments,
                execution_plan,
                objective_id
            )
        execution_time = (datetime.now() - execution_start).total_seconds()
        
        # Record performance for concurrency optimization
        tasks_completed = len([r for r in results if r.get("status") == "completed"])
        self.concurrency_manager.record_performance(tasks_completed, execution_time)
        
        # ê²°ê³¼ í†µí•©
        final_results = await self._collect_results(results, objective_id)
        
        # ìŠ¤íŠ¸ë¦¬ë° ì´ë²¤íŠ¸: ì‹¤í–‰ ì™„ë£Œ
        await self.streaming_manager.stream_event(
            event_type=EventType.WORKFLOW_COMPLETE,
            agent_id="parallel_executor",
            workflow_id=objective_id,
            data={
                'stage': 'parallel_execution',
                'message': 'Parallel task execution completed',
                'total_tasks': len(tasks),
                'completed_tasks': len(final_results.get('execution_results', [])),
                'failed_tasks': len(self.failed_tasks),
                'execution_time': execution_time,
                'success_rate': len(final_results.get('execution_results', [])) / max(len(tasks), 1),
                'dependency_graph_stats': dependency_graph.get_statistics() if dependency_graph else None
            },
            priority=1
        )
        
        logger.info(f"Parallel execution completed: {len(final_results.get('execution_results', []))} tasks completed in {execution_time:.2f}s")
        
        # DAG ì‹œê°í™” ì¢…ë£Œ
        try:
            from src.core.dag_visualizer import get_dag_visualizer
            visualizer = get_dag_visualizer()
            visualizer.finalize()
            
            # DAG ìš”ì•½ì„ ìŠ¤íŠ¸ë¦¬ë° ì´ë²¤íŠ¸ë¡œ ì „ì†¡
            dag_summary = visualizer.get_dag_summary()
            await self.streaming_manager.stream_event(
                event_type=EventType.PROGRESS_UPDATE,
                agent_id="parallel_executor",
                workflow_id=objective_id,
                data={
                    'stage': 'dag_summary',
                    'message': 'DAG execution summary',
                    'summary': dag_summary,
                    'visualization_data': visualizer.get_visualization_data()
                },
                priority=1
            )
        except Exception as e:
            logger.debug(f"Failed to finalize DAG visualizer: {e}")
        
        return final_results
    
    async def _execute_with_dependency_graph(
        self,
        dependency_graph,
        agent_assignments: Dict[str, List[str]],
        execution_plan: Dict[str, Any],
        objective_id: str
    ) -> List[Dict[str, Any]]:
        """ì˜ì¡´ì„± ê·¸ë˜í”„ ê¸°ë°˜ ìŠ¤ë§ˆíŠ¸ ë³‘ë ¬ ì‹¤í–‰."""
        results = []
        
        # Get dynamic concurrency
        current_concurrency = self.concurrency_manager.get_current_concurrency()
        semaphore = asyncio.Semaphore(current_concurrency)
        
        # ì‹¤í–‰ ë ˆë²¨ë³„ë¡œ ì²˜ë¦¬
        execution_levels = dependency_graph.get_execution_levels()
        logger.info(f"ğŸ“Š Execution levels: {len(execution_levels)} levels")
        
        for level_idx, level_tasks in enumerate(execution_levels):
            logger.info(f"ğŸ“Š Processing level {level_idx + 1}/{len(execution_levels)}: {len(level_tasks)} tasks")
            
            # í˜„ì¬ ë ˆë²¨ì˜ ì‹¤í–‰ ê°€ëŠ¥í•œ íƒœìŠ¤í¬ë§Œ í•„í„°ë§ (ì˜ì¡´ì„±ì´ í•´ê²°ëœ íƒœìŠ¤í¬)
            ready_tasks = dependency_graph.get_ready_tasks()
            level_ready_tasks = [t for t in level_tasks if t in ready_tasks]
            
            if not level_ready_tasks:
                # ì˜ì¡´ì„±ì´ ì•„ì§ í•´ê²°ë˜ì§€ ì•Šì€ íƒœìŠ¤í¬ëŠ” ë‹¤ìŒ ì‚¬ì´í´ì—ì„œ ì²˜ë¦¬
                logger.debug(f"  No ready tasks in level {level_idx + 1}, waiting for dependencies...")
                continue
            
            # ë™ì  ë™ì‹œì„±ì— ë§ì¶° íƒœìŠ¤í¬ ê·¸ë£¹ ìƒì„±
            current_concurrency = self.concurrency_manager.get_current_concurrency()
            task_groups = []
            for i in range(0, len(level_ready_tasks), current_concurrency):
                task_groups.append(level_ready_tasks[i:i + current_concurrency])
            
            # ê° ê·¸ë£¹ì„ ìˆœì°¨ì ìœ¼ë¡œ ì‹¤í–‰ (ê·¸ë£¹ ë‚´ì—ì„œëŠ” ë³‘ë ¬)
            for group_idx, task_group in enumerate(task_groups):
                logger.info(f"  Executing group {group_idx + 1}/{len(task_groups)}: {len(task_group)} tasks")
                
                # ê·¸ë£¹ ë‚´ ì‘ì—…ë“¤ì„ ë³‘ë ¬ë¡œ ì‹¤í–‰
                group_tasks = []
                for task_id in task_group:
                    task = dependency_graph.get_task(task_id)
                    if task:
                        dependency_graph.mark_running(task_id)
                        group_tasks.append(
                            self._execute_single_task(
                                task_id,
                                task,
                                agent_assignments,
                                semaphore,
                                objective_id
                            )
                        )
                
                # ê·¸ë£¹ ì‹¤í–‰ ì™„ë£Œ ëŒ€ê¸°
                group_results = await asyncio.gather(*group_tasks, return_exceptions=True)
                
                # ê²°ê³¼ ì²˜ë¦¬
                for i, result in enumerate(group_results):
                    task_id = task_group[i]
                    if isinstance(result, Exception):
                        logger.error(f"Task {task_id} failed with exception: {result}")
                        self.failed_tasks.append({
                            'task_id': task_id,
                            'error': str(result)
                        })
                        dependency_graph.mark_completed(task_id)  # ì‹¤íŒ¨í•´ë„ ì™„ë£Œë¡œ í‘œì‹œ
                        
                        # DAG ì‹œê°í™” ì—…ë°ì´íŠ¸
                        try:
                            from src.core.dag_visualizer import get_dag_visualizer
                            visualizer = get_dag_visualizer()
                            visualizer.mark_task_failed(task_id, str(result))
                        except Exception as e:
                            logger.debug(f"Failed to update DAG visualizer: {e}")
                    else:
                        results.append(result)
                        dependency_graph.mark_completed(task_id)
                        logger.info(f"  âœ… Task {task_id} completed")
                        
                        # DAG ì‹œê°í™” ì—…ë°ì´íŠ¸
                        try:
                            from src.core.dag_visualizer import get_dag_visualizer
                            visualizer = get_dag_visualizer()
                            visualizer.mark_task_completed(task_id, result)
                        except Exception as e:
                            logger.debug(f"Failed to update DAG visualizer: {e}")
        
        # ë‚¨ì€ íƒœìŠ¤í¬ ì²˜ë¦¬ (ì˜ì¡´ì„± ë¬¸ì œë¡œ ë ˆë²¨ì— í¬í•¨ë˜ì§€ ì•Šì€ íƒœìŠ¤í¬)
        remaining_ready = dependency_graph.get_ready_tasks()
        if remaining_ready:
            logger.info(f"ğŸ“Š Processing remaining {len(remaining_ready)} ready tasks")
            current_concurrency = self.concurrency_manager.get_current_concurrency()
            semaphore = asyncio.Semaphore(current_concurrency)
            
            remaining_tasks = []
            for task_id in remaining_ready[:current_concurrency]:
                task = dependency_graph.get_task(task_id)
                if task:
                    dependency_graph.mark_running(task_id)
                    remaining_tasks.append(
                        self._execute_single_task(
                            task_id,
                            task,
                            agent_assignments,
                            semaphore,
                            objective_id
                        )
                    )
            
            if remaining_tasks:
                remaining_results = await asyncio.gather(*remaining_tasks, return_exceptions=True)
                for i, result in enumerate(remaining_results):
                    task_id = remaining_ready[i]
                    if isinstance(result, Exception):
                        logger.error(f"Task {task_id} failed: {result}")
                        self.failed_tasks.append({'task_id': task_id, 'error': str(result)})
                        dependency_graph.mark_completed(task_id)
                    else:
                        results.append(result)
                        dependency_graph.mark_completed(task_id)
        
        return results
    
    async def _execute_with_parallel_groups(
        self,
        agent_assignments: Dict[str, List[str]],
        execution_plan: Dict[str, Any],
        objective_id: str
    ) -> List[Dict[str, Any]]:
        """ë³‘ë ¬ ê·¸ë£¹ ê¸°ë°˜ ì‹¤í–‰ (fallback - ì˜ì¡´ì„± ê·¸ë˜í”„ ì—†ì„ ë•Œ)."""
        results = []
        
        # Get dynamic concurrency
        current_concurrency = self.concurrency_manager.get_current_concurrency()
        semaphore = asyncio.Semaphore(current_concurrency)
        
        while self.task_queue.has_pending_tasks():
            # ë‹¤ìŒ ì‘ì—… ê·¸ë£¹ ê°€ì ¸ì˜¤ê¸° (dynamic concurrency)
            current_concurrency = self.concurrency_manager.get_current_concurrency()
            task_group = self.task_queue.get_next_task_group(max_group_size=current_concurrency)
            
            if not task_group:
                # ë” ì´ìƒ ì‹¤í–‰ ê°€ëŠ¥í•œ ì‘ì—…ì´ ì—†ìœ¼ë©´ ëŒ€ê¸°
                await asyncio.sleep(0.1)
                continue
            
            logger.info(f"Executing task group with {len(task_group)} tasks")
            
            # ê·¸ë£¹ ë‚´ ì‘ì—…ë“¤ì„ ë³‘ë ¬ë¡œ ì‹¤í–‰
            group_tasks = []
            for task_id in task_group:
                task = self.task_queue.get_task(task_id)
                if task:
                    group_tasks.append(
                        self._execute_single_task(
                            task_id,
                            task,
                            agent_assignments,
                            semaphore,
                            objective_id
                        )
                    )
            
            # ê·¸ë£¹ ì‹¤í–‰ ì™„ë£Œ ëŒ€ê¸°
            group_results = await asyncio.gather(*group_tasks, return_exceptions=True)
            
            # ê²°ê³¼ ì²˜ë¦¬
            for i, result in enumerate(group_results):
                task_id = task_group[i]
                if isinstance(result, Exception):
                    logger.error(f"Task {task_id} failed with exception: {result}")
                    self.failed_tasks.append({
                        'task_id': task_id,
                        'error': str(result)
                    })
                    self.task_queue.mark_completed(task_id)
                else:
                    results.append(result)
                    self.task_queue.mark_completed(task_id)
        
        return results
    
    async def _execute_single_task(
        self,
        task_id: str,
        task: Dict[str, Any],
        agent_assignments: Dict[str, List[str]],
        semaphore: asyncio.Semaphore,
        objective_id: str
    ) -> Dict[str, Any]:
        """ë‹¨ì¼ ì‘ì—… ì‹¤í–‰."""
        # Track active tasks for concurrency manager
        self.concurrency_manager.increment_active_tasks()
        
        try:
            async with semaphore:
                task_start = datetime.now()
                
                # ìŠ¤íŠ¸ë¦¬ë° ì´ë²¤íŠ¸: ì‘ì—… ì‹œì‘
                await self.streaming_manager.stream_event(
                    event_type=EventType.AGENT_ACTION,
                    agent_id="parallel_executor",
                    workflow_id=objective_id,
                    data={
                        'action': 'task_started',
                        'task_id': task_id,
                        'task_name': task.get('name', ''),
                        'status': 'running'
                    },
                    priority=0
                )
                
                try:
                    # ì‚¬ì „ ê²€ì¦: ì‘ì—… ì‹¤í–‰ ì „ ê²€ì¦
                    tool_category = self._get_tool_category_for_task(task)
                    available_tools = self._get_available_tools_for_category(tool_category)
                    
                    pre_validation = await self.task_validator.validate_task_before_execution(
                        task=task,
                        task_id=task_id,
                        task_queue=self.task_queue,
                        available_tools=available_tools
                    )
                    
                    if not pre_validation.is_valid:
                        logger.warning(f"Task {task_id} failed pre-execution validation: {pre_validation.errors}")
                        # ê²€ì¦ ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰ (ê²½ê³ ë§Œ)
                        if pre_validation.confidence < 0.5:
                            # ì‹ ë¢°ë„ê°€ ë„ˆë¬´ ë‚®ìœ¼ë©´ ì‹¤íŒ¨ ì²˜ë¦¬
                            raise ValueError(f"Task validation failed: {', '.join(pre_validation.errors)}")
                    
                    if pre_validation.warnings:
                        logger.debug(f"Task {task_id} pre-execution validation warnings: {pre_validation.warnings}")
                    
                    tool_result = None
                    tool_attempts = []
                    
                    # ë„êµ¬ ìš°ì„ ìˆœìœ„ë³„ë¡œ ì‹œë„
                    for tool_name in available_tools:
                        try:
                            logger.debug(f"Task {task_id}: Attempting tool {tool_name}")
                            
                            # íŒŒë¼ë¯¸í„° ìƒì„±
                            tool_parameters = self._generate_tool_parameters(task, tool_name)
                            
                            # ìºì‹œ í™•ì¸
                            cached_result = await self.result_cache.get(
                                tool_name=tool_name,
                                parameters=tool_parameters,
                                task_id=task_id,
                                check_similarity=True
                            )
                            
                            if cached_result:
                                logger.info(f"Task {task_id}: Cache hit for tool {tool_name}")
                                tool_result = cached_result
                            else:
                                # ë„êµ¬ ì‹¤í–‰
                                tool_result = await execute_tool(
                                    tool_name,
                                    tool_parameters
                                )
                                
                                # ì„±ê³µí•œ ê²°ê³¼ë§Œ ìºì‹œì— ì €ì¥
                                if tool_result.get("success", False):
                                    # TTL ê²°ì •: ê²€ìƒ‰ ë„êµ¬ëŠ” 1ì‹œê°„, ë‹¤ë¥¸ ë„êµ¬ëŠ” 30ë¶„
                                    ttl = 3600 if 'search' in tool_name.lower() else 1800
                                    await self.result_cache.set(
                                        tool_name=tool_name,
                                        parameters=tool_parameters,
                                        value=tool_result,
                                        ttl=ttl,
                                        task_id=task_id
                                    )
                                    logger.debug(f"Task {task_id}: Cached result for tool {tool_name}")
                            
                            tool_attempts.append({
                                "tool": tool_name,
                                "success": tool_result.get("success", False),
                                "error": tool_result.get("error", ""),
                                "execution_time": tool_result.get("execution_time", 0.0)
                            })
                            
                            # ì‹¤í–‰ ì¤‘ ê²€ì¦
                            execution_time_so_far = (datetime.now() - task_start).total_seconds()
                            during_validation = await self.task_validator.validate_task_during_execution(
                                task_id=task_id,
                                intermediate_result=tool_result,
                                task=task,
                                execution_time=execution_time_so_far
                            )
                            
                            if during_validation.warnings:
                                logger.debug(f"Task {task_id} during-execution validation warnings: {during_validation.warnings}")
                            
                            # ê²°ê³¼ ê²€ì¦ (ê°•í™”ëœ ë²„ì „)
                            result_validation = await self.task_validator.validate_task_result(
                                tool_result=tool_result,
                                task=task
                            )
                            
                            # ì„±ê³µ ì¡°ê±´: ê¸°ë³¸ ì„±ê³µ + ê²€ì¦ í†µê³¼
                            is_success = (
                                tool_result.get("success", False) and
                                result_validation.is_valid and
                                result_validation.confidence >= 0.5
                            )
                            
                            if is_success:
                                logger.info(
                                    f"Task {task_id}: Tool {tool_name} executed successfully "
                                    f"(confidence: {result_validation.confidence:.2f})"
                                )
                                break
                            else:
                                if not tool_result.get("success", False):
                                    logger.warning(f"Task {task_id}: Tool {tool_name} execution failed")
                                elif not result_validation.is_valid:
                                    logger.warning(
                                        f"Task {task_id}: Tool {tool_name} result validation failed: "
                                        f"{result_validation.errors}"
                                    )
                                elif result_validation.confidence < 0.5:
                                    logger.warning(
                                        f"Task {task_id}: Tool {tool_name} result confidence too low: "
                                        f"{result_validation.confidence:.2f}"
                                    )
                                tool_result = None
                            
                        except Exception as tool_error:
                            logger.warning(f"Task {task_id}: Tool {tool_name} error: {tool_error}")
                            
                            # Try error handler recovery
                            try:
                                recovery_result, recovery_success = await self.error_handler.handle_error(
                                    tool_error,
                                    execute_tool,
                                    tool_name,
                                    tool_parameters
                                )
                                
                                if recovery_success and recovery_result:
                                    logger.info(f"Task {task_id}: Tool {tool_name} recovered from error")
                                    tool_result = recovery_result
                                    
                                    tool_attempts.append({
                                        "tool": tool_name,
                                        "success": True,
                                        "error": "",
                                        "execution_time": recovery_result.get("execution_time", 0.0),
                                        "recovered": True
                                    })
                                    
                                    # Continue with recovered result
                                    execution_time_so_far = (datetime.now() - task_start).total_seconds()
                                    during_validation = await self.task_validator.validate_task_during_execution(
                                        task_id=task_id,
                                        intermediate_result=tool_result,
                                        task=task,
                                        execution_time=execution_time_so_far
                                    )
                                    
                                    result_validation = await self.task_validator.validate_task_result(
                                        tool_result=tool_result,
                                        task=task
                                    )
                                    
                                    is_success = (
                                        tool_result.get("success", False) and
                                        result_validation.is_valid and
                                        result_validation.confidence >= 0.5
                                    )
                                    
                                    if is_success:
                                        logger.info(f"Task {task_id}: Tool {tool_name} recovered and validated successfully")
                                        break
                            except Exception as recovery_error:
                                logger.debug(f"Error recovery failed: {recovery_error}")
                            
                            # If recovery failed or not attempted, record failure
                            tool_attempts.append({
                                "tool": tool_name,
                                "success": False,
                                "error": str(tool_error),
                                "execution_time": 0.0
                            })
                            continue
                    
                    execution_time = (datetime.now() - task_start).total_seconds()
                    
                    # Agent ID ìƒì„± (ì‘ì—…ë³„ ê³ ìœ  agent)
                    agent_id = f"agent_{task_id}"
                    
                    # ê²°ê³¼ ìƒì„±
                    if tool_result and tool_result.get("success", False):
                        result_data = tool_result.get("data")
                        confidence = tool_result.get("confidence", 0.0)
                        
                        result = {
                        "task_id": task_id,
                        "task_name": task.get("name", ""),
                        "agent_id": agent_id,
                        "tool_used": tool_attempts[-1]["tool"] if tool_attempts else "none",
                        "result": result_data,
                        "execution_time": execution_time,
                        "confidence": confidence,
                        "attempts": len(tool_attempts),
                        "status": "completed"
                        }
                        
                        # ê²°ê³¼ ê³µìœ  (agent communicationì´ í™œì„±í™”ëœ ê²½ìš°)
                        if self.shared_results_manager:
                            result_id = await self.shared_results_manager.share_result(
                                task_id=task_id,
                                agent_id=agent_id,
                                result=result_data,
                                metadata={
                                    "tool_used": result["tool_used"],
                                    "execution_time": execution_time
                                },
                                confidence=confidence
                            )
                            result["shared_result_id"] = result_id
                            
                            # ë‹¤ë¥¸ agentë“¤ì˜ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸° (ë™ì¼í•œ ì‘ì—…ì— ëŒ€í•œ)
                            other_results = await self.shared_results_manager.get_shared_results(
                                task_id=task_id,
                                exclude_agent_id=agent_id
                            )
                            
                            # ë‹¤ë¥¸ agentë“¤ê³¼ í† ë¡ 
                            if other_results and self.discussion_manager:
                                discussion = await self.discussion_manager.agent_discuss_result(
                                    result_id=result_id,
                                    agent_id=agent_id,
                                    other_agent_results=other_results
                                )
                                if discussion:
                                    result["discussion"] = discussion
                                    logger.info(f"Agent {agent_id} discussed result with {len(other_results)} other agents")
                    else:
                        result = {
                            "task_id": task_id,
                            "task_name": task.get("name", ""),
                            "agent_id": agent_id,
                            "tool_used": "none",
                            "result": None,
                            "execution_time": execution_time,
                            "confidence": 0.0,
                            "attempts": len(tool_attempts),
                            "error": "All tools failed",
                            "tool_attempts": tool_attempts,
                            "status": "failed"
                        }
                    
                    # ìŠ¤íŠ¸ë¦¬ë° ì´ë²¤íŠ¸: ì‘ì—… ì™„ë£Œ
                    await self.streaming_manager.stream_event(
                        event_type=EventType.AGENT_ACTION,
                        agent_id="parallel_executor",
                        workflow_id=objective_id,
                        data={
                            'action': 'task_completed',
                            'task_id': task_id,
                            'status': result.get("status", "unknown"),
                            'execution_time': execution_time
                        },
                        priority=0
                    )
                    
                    return result
                    
                except Exception as e:
                    logger.error(f"Task {task_id} execution failed: {e}")
                    execution_time = (datetime.now() - task_start).total_seconds()
                    
                    return {
                        "task_id": task_id,
                        "task_name": task.get("name", ""),
                        "tool_used": "none",
                        "result": None,
                        "execution_time": execution_time,
                        "confidence": 0.0,
                        "attempts": 0,
                        "error": str(e),
                        "status": "failed"
                    }
        finally:
            # Always decrement active tasks
            self.concurrency_manager.decrement_active_tasks()
    
    async def _collect_results(
        self,
        results: List[Dict[str, Any]],
        objective_id: str
    ) -> Dict[str, Any]:
        """ê²°ê³¼ ìˆ˜ì§‘ ë° í†µí•©."""
        # ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸
        progress = self.task_queue.get_progress()
        
        # ê²°ê³¼ ê³µìœ  ìš”ì•½
        sharing_summary = None
        discussion_summary = None
        if self.shared_results_manager:
            sharing_summary = await self.shared_results_manager.get_result_summary()
            logger.info(f"âœ… Result sharing summary: {sharing_summary['total_results']} results shared by {sharing_summary['agents_count']} agents")
        
        if self.discussion_manager:
            discussion_summary = await self.discussion_manager.get_discussion_summary()
            logger.info(f"âœ… Discussion summary: {discussion_summary['total_topics']} discussion topics")
        
        # ìŠ¤íŠ¸ë¦¬ë° ì´ë²¤íŠ¸: ì§„í–‰ ìƒí™©
        await self.streaming_manager.stream_event(
            event_type=EventType.PROGRESS_UPDATE,
            agent_id="parallel_executor",
            workflow_id=objective_id,
            data={
                'progress': progress,
                'completed_results': len(results),
                'failed_tasks': len(self.failed_tasks),
                'result_sharing': sharing_summary,
                'discussions': discussion_summary
            },
            priority=0
        )
        
        return {
            "execution_results": results,
            "failed_tasks": self.failed_tasks,
            "progress": progress,
            "total_execution_time": sum(r.get("execution_time", 0.0) for r in results),
            "success_count": len([r for r in results if r.get("status") == "completed"]),
            "failure_count": len([r for r in results if r.get("status") == "failed"]) + len(self.failed_tasks),
            "result_sharing": sharing_summary,
            "discussions": discussion_summary
        }
    
    def _get_tool_category_for_task(self, task: Dict[str, Any]) -> str:
        """ì‘ì—…ì— ì í•©í•œ ë„êµ¬ ì¹´í…Œê³ ë¦¬ ê²°ì •."""
        task_type = task.get('task_type', 'general').lower()
        
        if 'search' in task_type or 'find' in task_type:
            return 'search'
        elif 'data' in task_type or 'analyze' in task_type:
            return 'data'
        elif 'code' in task_type or 'implement' in task_type:
            return 'code'
        elif 'academic' in task_type or 'paper' in task_type:
            return 'academic'
        elif 'business' in task_type or 'market' in task_type:
            return 'business'
        else:
            return 'utility'
    
    def _get_available_tools_for_category(self, category: str) -> List[str]:
        """ì¹´í…Œê³ ë¦¬ë³„ ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ ëª©ë¡."""
        from src.core.mcp_integration import UniversalMCPHub
        
        # ì„¤ì •ì—ì„œ ë„êµ¬ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
        config_tools = []
        if category == 'search':
            config_tools = self.mcp_config.search_tools
        elif category == 'data':
            config_tools = self.mcp_config.data_tools
        elif category == 'code':
            config_tools = self.mcp_config.code_tools
        elif category == 'academic':
            config_tools = self.mcp_config.academic_tools
        elif category == 'business':
            config_tools = self.mcp_config.business_tools
        
        # MCP Hubì—ì„œ ì‹¤ì œ ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ í™•ì¸
        available_tools = []
        try:
            mcp_hub = UniversalMCPHub()
            if hasattr(mcp_hub, 'mcp_tools_map') and mcp_hub.mcp_tools_map:
                # ì„¤ì •ì—ì„œ ì§€ì •ëœ ë„êµ¬ê°€ ì‹¤ì œë¡œ ì‚¬ìš© ê°€ëŠ¥í•œì§€ í™•ì¸
                for tool_name in config_tools:
                    # server_name::tool_name í˜•ì‹ì¸ì§€ í™•ì¸
                    if "::" in tool_name:
                        server_name, actual_tool_name = tool_name.split("::", 1)
                        if server_name in mcp_hub.mcp_tools_map:
                            if actual_tool_name in mcp_hub.mcp_tools_map[server_name]:
                                available_tools.append(tool_name)
                    else:
                        # tool_nameë§Œ ìˆëŠ” ê²½ìš°, ëª¨ë“  ì„œë²„ì—ì„œ ì°¾ê¸°
                        for server_name, server_tools in mcp_hub.mcp_tools_map.items():
                            if tool_name in server_tools:
                                available_tools.append(f"{server_name}::{tool_name}")
                                break
                
                # ì„¤ì •ëœ ë„êµ¬ê°€ ì—†ìœ¼ë©´ ì¹´í…Œê³ ë¦¬ ê¸°ë°˜ìœ¼ë¡œ ì°¾ê¸°
                if not available_tools:
                    for server_name, server_tools in mcp_hub.mcp_tools_map.items():
                        for tool_name in server_tools.keys():
                            tool_lower = tool_name.lower()
                            if category == 'search' and ('search' in tool_lower or 'query' in tool_lower):
                                available_tools.append(f"{server_name}::{tool_name}")
                            elif category == 'data' and 'data' in tool_lower:
                                available_tools.append(f"{server_name}::{tool_name}")
                            elif category == 'code' and ('code' in tool_lower or 'exec' in tool_lower):
                                available_tools.append(f"{server_name}::{tool_name}")
                            elif category == 'academic' and ('academic' in tool_lower or 'paper' in tool_lower):
                                available_tools.append(f"{server_name}::{tool_name}")
                            elif category == 'business' and ('business' in tool_lower or 'market' in tool_lower):
                                available_tools.append(f"{server_name}::{tool_name}")
        except Exception as e:
            logger.warning(f"Failed to get tools from MCP Hub: {e}")
        
        # ì„¤ì •ëœ ë„êµ¬ê°€ ìˆìœ¼ë©´ ë°˜í™˜, ì—†ìœ¼ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ (ì—ëŸ¬ ì²˜ë¦¬)
        return available_tools if available_tools else config_tools
    
    def _generate_tool_parameters(self, task: Dict[str, Any], tool_name: str) -> Dict[str, Any]:
        """ë„êµ¬ íŒŒë¼ë¯¸í„° ìƒì„±."""
        # ê°„ë‹¨í•œ êµ¬í˜„: ì‘ì—… ì •ë³´ë¥¼ ë„êµ¬ íŒŒë¼ë¯¸í„°ë¡œ ë³€í™˜
        task_description = task.get('description', task.get('name', ''))
        
        # task_type ê¸°ë°˜ íŒŒë¼ë¯¸í„° ìƒì„±
        task_type = task.get('task_type', 'general').lower()
        
        if 'search' in tool_name.lower() or 'search' in task_type:
            query = task.get('query', task_description)
            if not query:
                query = task.get('name', '')
            return {
                'query': query,
                'max_results': task.get('max_results', 10)
            }
        elif 'fetch' in tool_name.lower() or 'fetch' in task_type:
            return {
                'url': task.get('url', ''),
                'timeout': task.get('timeout', 30)
            }
        else:
            # ê¸°ë³¸ íŒŒë¼ë¯¸í„°
            query = task.get('query', task_description)
            if not query:
                query = task.get('name', '')
            return {
                'query': query,
                'task': task
            }
    

