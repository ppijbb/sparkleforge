"""
Iterative Deep-Research Paradigm

DeepResearch (Alibaba-NLP) ì˜ê°ì„ ë°›ì€ ë°˜ë³µì  ê¹Šì€ ì—°êµ¬ ì‹œìŠ¤í…œ.
ë¼ìš´ë“œ ê¸°ë°˜ Think/Report/Action íŒ¨í„´ìœ¼ë¡œ ë³µì¡í•œ ì£¼ì œë¥¼ ì ì§„ì ìœ¼ë¡œ íƒìƒ‰.

í•µì‹¬ íŠ¹ì§•:
- Round-based research with workspace reconstruction
- Think/Report/Action ë¶„ë¦¬ë¡œ context bloat ë°©ì§€
- Evolving Summary Report as central memory
- Quality threshold-based termination
"""

import asyncio
import logging
from dataclasses import dataclass, field
from typing import Dict, Any, List, Optional, Callable, TypedDict
from datetime import datetime
from enum import Enum

from pydantic import BaseModel, Field

logger = logging.getLogger(__name__)


class ResearchPhase(Enum):
    """ì—°êµ¬ ë¼ìš´ë“œì˜ í˜„ì¬ ë‹¨ê³„."""
    THINK = "think"
    REPORT = "report"
    ACTION = "action"
    RECONSTRUCT = "reconstruct"
    COMPLETE = "complete"


class QualityMetrics(BaseModel):
    """ì—°êµ¬ í’ˆì§ˆ ì¸¡ì • ì§€í‘œ."""
    completeness: float = Field(default=0.0, ge=0.0, le=1.0, description="ì£¼ì œ ì™„ì„±ë„")
    depth: float = Field(default=0.0, ge=0.0, le=1.0, description="ë¶„ì„ ê¹Šì´")
    source_diversity: float = Field(default=0.0, ge=0.0, le=1.0, description="ì†ŒìŠ¤ ë‹¤ì–‘ì„±")
    factual_accuracy: float = Field(default=0.0, ge=0.0, le=1.0, description="ì‚¬ì‹¤ì  ì •í™•ì„±")
    coherence: float = Field(default=0.0, ge=0.0, le=1.0, description="ì¼ê´€ì„±")
    
    @property
    def overall_score(self) -> float:
        """ì „ì²´ í’ˆì§ˆ ì ìˆ˜ (ê°€ì¤‘ í‰ê· )."""
        weights = {
            "completeness": 0.25,
            "depth": 0.25,
            "source_diversity": 0.15,
            "factual_accuracy": 0.20,
            "coherence": 0.15
        }
        return sum(
            getattr(self, k) * v 
            for k, v in weights.items()
        )


class ThinkOutput(BaseModel):
    """Think ë‹¨ê³„ì˜ ì¶œë ¥."""
    current_understanding: str = Field(description="í˜„ì¬ê¹Œì§€ì˜ ì´í•´")
    knowledge_gaps: List[str] = Field(default_factory=list, description="ì§€ì‹ ê³µë°±")
    next_research_directions: List[str] = Field(default_factory=list, description="ë‹¤ìŒ ì—°êµ¬ ë°©í–¥")
    hypotheses: List[str] = Field(default_factory=list, description="ê²€ì¦í•  ê°€ì„¤ë“¤")
    confidence_level: float = Field(default=0.0, ge=0.0, le=1.0, description="í˜„ì¬ ì‹ ë¢°ë„")


class ReportOutput(BaseModel):
    """Report ë‹¨ê³„ì˜ ì¶œë ¥ (Evolving Summary)."""
    round_number: int = Field(description="í˜„ì¬ ë¼ìš´ë“œ ë²ˆí˜¸")
    executive_summary: str = Field(description="í•µì‹¬ ìš”ì•½")
    key_findings: List[Dict[str, Any]] = Field(default_factory=list, description="ì£¼ìš” ë°œê²¬ì‚¬í•­")
    sources_used: List[Dict[str, str]] = Field(default_factory=list, description="ì‚¬ìš©ëœ ì†ŒìŠ¤")
    quality_metrics: QualityMetrics = Field(default_factory=QualityMetrics, description="í’ˆì§ˆ ì§€í‘œ")
    remaining_questions: List[str] = Field(default_factory=list, description="ë‚¨ì€ ì§ˆë¬¸ë“¤")


class ActionOutput(BaseModel):
    """Action ë‹¨ê³„ì˜ ì¶œë ¥."""
    actions_taken: List[Dict[str, Any]] = Field(default_factory=list, description="ìˆ˜í–‰í•œ ì•¡ì…˜ë“¤")
    new_information: List[Dict[str, Any]] = Field(default_factory=list, description="ìƒˆë¡œ íšë“í•œ ì •ë³´")
    tool_calls: List[Dict[str, Any]] = Field(default_factory=list, description="ë„êµ¬ í˜¸ì¶œ ê¸°ë¡")
    errors_encountered: List[str] = Field(default_factory=list, description="ë°œìƒí•œ ì˜¤ë¥˜ë“¤")


class RoundState(BaseModel):
    """ê°œë³„ ë¼ìš´ë“œì˜ ìƒíƒœ."""
    round_number: int
    phase: ResearchPhase = ResearchPhase.THINK
    think_output: Optional[ThinkOutput] = None
    report_output: Optional[ReportOutput] = None
    action_output: Optional[ActionOutput] = None
    started_at: datetime = Field(default_factory=datetime.now)
    completed_at: Optional[datetime] = None
    
    class Config:
        arbitrary_types_allowed = True


class IterativeResearchState(BaseModel):
    """ë°˜ë³µ ì—°êµ¬ ì „ì²´ ìƒíƒœ."""
    query: str = Field(description="ì›ë³¸ ì—°êµ¬ ì§ˆë¬¸")
    session_id: str = Field(description="ì„¸ì…˜ ID")
    current_round: int = Field(default=1, description="í˜„ì¬ ë¼ìš´ë“œ")
    max_rounds: int = Field(default=5, description="ìµœëŒ€ ë¼ìš´ë“œ ìˆ˜")
    quality_threshold: float = Field(default=0.8, ge=0.0, le=1.0, description="ì¢…ë£Œ í’ˆì§ˆ ì„ê³„ê°’")
    
    # Evolving Summary Report (ì¤‘ì•™ ë©”ëª¨ë¦¬)
    evolving_summary: str = Field(default="", description="ì§„í™”í•˜ëŠ” ìš”ì•½ ë³´ê³ ì„œ")
    accumulated_findings: List[Dict[str, Any]] = Field(default_factory=list, description="ëˆ„ì  ë°œê²¬ì‚¬í•­")
    all_sources: List[Dict[str, str]] = Field(default_factory=list, description="ëª¨ë“  ì‚¬ìš© ì†ŒìŠ¤")
    
    # ë¼ìš´ë“œ íˆìŠ¤í† ë¦¬ (Think ì œì™¸ - context bloat ë°©ì§€)
    round_reports: List[ReportOutput] = Field(default_factory=list, description="ë¼ìš´ë“œë³„ ë³´ê³ ì„œ")
    
    # ì¢…ë£Œ ì¡°ê±´
    is_complete: bool = Field(default=False, description="ì—°êµ¬ ì™„ë£Œ ì—¬ë¶€")
    termination_reason: Optional[str] = Field(default=None, description="ì¢…ë£Œ ì´ìœ ")
    
    # í˜„ì¬ ë¼ìš´ë“œ ìƒíƒœ
    current_round_state: Optional[RoundState] = None
    
    class Config:
        arbitrary_types_allowed = True


@dataclass
class WorkspaceContext:
    """ë¼ìš´ë“œ ê°„ ì „ë‹¬ë˜ëŠ” lean workspace context."""
    evolving_summary: str
    last_report: Optional[ReportOutput]
    knowledge_gaps: List[str]
    remaining_questions: List[str]
    quality_score: float
    round_number: int


class WorkspaceReconstructor:
    """
    ë¼ìš´ë“œ ê°„ workspace ì¬êµ¬ì„±ê¸°.
    
    Think ì¶œë ¥ì„ ë‹¤ìŒ ë¼ìš´ë“œë¡œ ì „ë‹¬í•˜ì§€ ì•Šê³ ,
    Reportì˜ í•µì‹¬ ì •ë³´ë§Œ ì¶”ì¶œí•˜ì—¬ lean context êµ¬ì„±.
    """
    
    def __init__(self, max_summary_tokens: int = 4000):
        self.max_summary_tokens = max_summary_tokens
    
    def reconstruct(
        self,
        state: IterativeResearchState,
        new_report: ReportOutput
    ) -> WorkspaceContext:
        """
        ìƒˆ ë¼ìš´ë“œë¥¼ ìœ„í•œ workspace ì¬êµ¬ì„±.
        
        Args:
            state: í˜„ì¬ ì—°êµ¬ ìƒíƒœ
            new_report: ìƒˆë¡œ ìƒì„±ëœ ë³´ê³ ì„œ
            
        Returns:
            ë‹¤ìŒ ë¼ìš´ë“œë¥¼ ìœ„í•œ lean workspace context
        """
        # ì§„í™”í•˜ëŠ” ìš”ì•½ ì—…ë°ì´íŠ¸
        updated_summary = self._merge_summaries(
            state.evolving_summary,
            new_report.executive_summary
        )
        
        # Knowledge gapsì™€ remaining questions ì¶”ì¶œ
        knowledge_gaps = new_report.remaining_questions[:5]  # ìƒìœ„ 5ê°œë§Œ
        
        return WorkspaceContext(
            evolving_summary=updated_summary,
            last_report=new_report,
            knowledge_gaps=knowledge_gaps,
            remaining_questions=new_report.remaining_questions,
            quality_score=new_report.quality_metrics.overall_score,
            round_number=state.current_round
        )
    
    def _merge_summaries(self, existing: str, new: str) -> str:
        """ê¸°ì¡´ ìš”ì•½ê³¼ ìƒˆ ìš”ì•½ ë³‘í•©."""
        if not existing:
            return new
        
        # ê°„ë‹¨í•œ ë³‘í•© (ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” LLM ì‚¬ìš©)
        merged = f"{existing}\n\n[Round Update]\n{new}"
        
        # í† í° ì œí•œ ì ìš© (ê°„ë‹¨í•œ ë¬¸ì ê¸°ë°˜)
        max_chars = self.max_summary_tokens * 4  # ëŒ€ëµì  ì¶”ì •
        if len(merged) > max_chars:
            # ì˜¤ë˜ëœ ë¶€ë¶„ ì••ì¶•
            merged = merged[-max_chars:]
            
        return merged


class IterativeResearchEngine:
    """
    ë°˜ë³µì  ê¹Šì€ ì—°êµ¬ ì—”ì§„.
    
    Think â†’ Report â†’ Action â†’ Reconstruct ì‚¬ì´í´ì„ ê´€ë¦¬.
    """
    
    def __init__(
        self,
        max_rounds: int = 5,
        quality_threshold: float = 0.8,
        min_improvement_threshold: float = 0.05,
        workspace_reconstructor: Optional[WorkspaceReconstructor] = None
    ):
        self.max_rounds = max_rounds
        self.quality_threshold = quality_threshold
        self.min_improvement_threshold = min_improvement_threshold
        self.workspace_reconstructor = workspace_reconstructor or WorkspaceReconstructor()
        
        # Callbacks
        self.on_round_start: Optional[Callable] = None
        self.on_think_complete: Optional[Callable] = None
        self.on_report_complete: Optional[Callable] = None
        self.on_action_complete: Optional[Callable] = None
        self.on_round_complete: Optional[Callable] = None
        
        logger.info(
            f"IterativeResearchEngine initialized: "
            f"max_rounds={max_rounds}, quality_threshold={quality_threshold}"
        )
    
    async def run(
        self,
        query: str,
        session_id: str,
        think_fn: Callable,
        report_fn: Callable,
        action_fn: Callable,
        initial_context: Optional[Dict[str, Any]] = None
    ) -> IterativeResearchState:
        """
        ë°˜ë³µì  ì—°êµ¬ ì‹¤í–‰.
        
        Args:
            query: ì—°êµ¬ ì§ˆë¬¸
            session_id: ì„¸ì…˜ ID
            think_fn: Think ë‹¨ê³„ í•¨ìˆ˜ (async)
            report_fn: Report ë‹¨ê³„ í•¨ìˆ˜ (async)
            action_fn: Action ë‹¨ê³„ í•¨ìˆ˜ (async)
            initial_context: ì´ˆê¸° ì»¨í…ìŠ¤íŠ¸
            
        Returns:
            ìµœì¢… ì—°êµ¬ ìƒíƒœ
        """
        # ìƒíƒœ ì´ˆê¸°í™”
        state = IterativeResearchState(
            query=query,
            session_id=session_id,
            max_rounds=self.max_rounds,
            quality_threshold=self.quality_threshold
        )
        
        previous_quality = 0.0
        stagnation_count = 0
        max_stagnation = 2  # ì—°ì† 2ë¼ìš´ë“œ ê°œì„  ì—†ìœ¼ë©´ ì¢…ë£Œ
        
        logger.info(f"ğŸ”„ Starting iterative research for query: {query[:100]}...")
        
        while not state.is_complete and state.current_round <= self.max_rounds:
            round_start = datetime.now()
            
            # ë¼ìš´ë“œ ì‹œì‘ ì½œë°±
            if self.on_round_start:
                await self._safe_callback(self.on_round_start, state)
            
            logger.info(f"ğŸ“ Round {state.current_round}/{self.max_rounds} starting...")
            
            # ë¼ìš´ë“œ ìƒíƒœ ì´ˆê¸°í™”
            state.current_round_state = RoundState(round_number=state.current_round)
            
            # Workspace context ì¤€ë¹„
            workspace = self._prepare_workspace(state)
            
            try:
                # 1. THINK ë‹¨ê³„
                state.current_round_state.phase = ResearchPhase.THINK
                think_output = await think_fn(
                    query=query,
                    workspace=workspace,
                    round_number=state.current_round
                )
                state.current_round_state.think_output = think_output
                
                if self.on_think_complete:
                    await self._safe_callback(self.on_think_complete, state, think_output)
                
                logger.info(f"ğŸ’­ Think complete: {len(think_output.knowledge_gaps)} gaps identified")
                
                # 2. ACTION ë‹¨ê³„ (Think ê¸°ë°˜ìœ¼ë¡œ ì •ë³´ ìˆ˜ì§‘)
                state.current_round_state.phase = ResearchPhase.ACTION
                action_output = await action_fn(
                    query=query,
                    think_output=think_output,
                    workspace=workspace
                )
                state.current_round_state.action_output = action_output
                
                if self.on_action_complete:
                    await self._safe_callback(self.on_action_complete, state, action_output)
                
                logger.info(f"âš¡ Action complete: {len(action_output.new_information)} new items")
                
                # 3. REPORT ë‹¨ê³„ (Think + Action ê²°ê³¼ ì¢…í•©)
                state.current_round_state.phase = ResearchPhase.REPORT
                report_output = await report_fn(
                    query=query,
                    think_output=think_output,
                    action_output=action_output,
                    workspace=workspace,
                    round_number=state.current_round
                )
                state.current_round_state.report_output = report_output
                
                if self.on_report_complete:
                    await self._safe_callback(self.on_report_complete, state, report_output)
                
                logger.info(
                    f"ğŸ“Š Report complete: quality={report_output.quality_metrics.overall_score:.2f}"
                )
                
                # 4. RECONSTRUCT ë‹¨ê³„
                state.current_round_state.phase = ResearchPhase.RECONSTRUCT
                new_workspace = self.workspace_reconstructor.reconstruct(state, report_output)
                
                # ìƒíƒœ ì—…ë°ì´íŠ¸ (Think ì¶œë ¥ì€ ì €ì¥í•˜ì§€ ì•ŠìŒ - context bloat ë°©ì§€)
                state.evolving_summary = new_workspace.evolving_summary
                state.round_reports.append(report_output)
                state.accumulated_findings.extend(report_output.key_findings)
                state.all_sources.extend(report_output.sources_used)
                
                # ì¢…ë£Œ ì¡°ê±´ í™•ì¸
                current_quality = report_output.quality_metrics.overall_score
                
                # í’ˆì§ˆ ì„ê³„ê°’ ë„ë‹¬
                if current_quality >= self.quality_threshold:
                    state.is_complete = True
                    state.termination_reason = f"Quality threshold reached: {current_quality:.2f} >= {self.quality_threshold}"
                    logger.info(f"âœ… {state.termination_reason}")
                
                # ê°œì„  ì •ì²´ í™•ì¸
                improvement = current_quality - previous_quality
                if improvement < self.min_improvement_threshold:
                    stagnation_count += 1
                    if stagnation_count >= max_stagnation:
                        state.is_complete = True
                        state.termination_reason = f"Improvement stagnated for {stagnation_count} rounds"
                        logger.info(f"âš ï¸ {state.termination_reason}")
                else:
                    stagnation_count = 0
                
                previous_quality = current_quality
                
                # ë¼ìš´ë“œ ì™„ë£Œ
                state.current_round_state.phase = ResearchPhase.COMPLETE
                state.current_round_state.completed_at = datetime.now()
                
                if self.on_round_complete:
                    await self._safe_callback(self.on_round_complete, state)
                
                round_duration = (datetime.now() - round_start).total_seconds()
                logger.info(
                    f"ğŸ”„ Round {state.current_round} complete in {round_duration:.1f}s "
                    f"(quality: {current_quality:.2f})"
                )
                
                state.current_round += 1
                
            except Exception as e:
                logger.error(f"âŒ Round {state.current_round} failed: {e}")
                state.termination_reason = f"Error in round {state.current_round}: {str(e)}"
                # ì˜¤ë¥˜ ë°œìƒí•´ë„ ì´ì „ ê²°ê³¼ëŠ” ìœ ì§€í•˜ê³  ì¢…ë£Œ
                if state.round_reports:
                    state.is_complete = True
                else:
                    raise
        
        # ìµœëŒ€ ë¼ìš´ë“œ ë„ë‹¬
        if not state.is_complete:
            state.is_complete = True
            state.termination_reason = f"Max rounds ({self.max_rounds}) reached"
            logger.info(f"ğŸ“ {state.termination_reason}")
        
        return state
    
    def _prepare_workspace(self, state: IterativeResearchState) -> WorkspaceContext:
        """í˜„ì¬ ìƒíƒœì—ì„œ workspace context ì¤€ë¹„."""
        last_report = state.round_reports[-1] if state.round_reports else None
        
        return WorkspaceContext(
            evolving_summary=state.evolving_summary,
            last_report=last_report,
            knowledge_gaps=last_report.remaining_questions[:5] if last_report else [],
            remaining_questions=last_report.remaining_questions if last_report else [],
            quality_score=last_report.quality_metrics.overall_score if last_report else 0.0,
            round_number=state.current_round
        )
    
    async def _safe_callback(self, callback: Callable, *args, **kwargs):
        """ì•ˆì „í•œ ì½œë°± ì‹¤í–‰."""
        try:
            if asyncio.iscoroutinefunction(callback):
                await callback(*args, **kwargs)
            else:
                callback(*args, **kwargs)
        except Exception as e:
            logger.warning(f"Callback error (non-fatal): {e}")


class IterativeResearchNode:
    """
    LangGraph í†µí•©ì„ ìœ„í•œ Iterative Research ë…¸ë“œ.
    
    ê¸°ì¡´ agent_orchestrator.pyì˜ ì›Œí¬í”Œë¡œìš°ì— í†µí•© ê°€ëŠ¥.
    """
    
    def __init__(
        self,
        engine: Optional[IterativeResearchEngine] = None,
        llm_task_executor: Optional[Callable] = None
    ):
        self.engine = engine or IterativeResearchEngine()
        self.llm_task_executor = llm_task_executor
    
    async def think(
        self,
        query: str,
        workspace: WorkspaceContext,
        round_number: int
    ) -> ThinkOutput:
        """Think ë‹¨ê³„ ì‹¤í–‰."""
        from src.core.llm_manager import execute_llm_task, TaskType
        
        prompt = self._build_think_prompt(query, workspace, round_number)
        
        result = await execute_llm_task(
            prompt=prompt,
            task_type=TaskType.DEEP_REASONING,
            temperature=0.7
        )
        
        return self._parse_think_output(result)
    
    async def report(
        self,
        query: str,
        think_output: ThinkOutput,
        action_output: ActionOutput,
        workspace: WorkspaceContext,
        round_number: int
    ) -> ReportOutput:
        """Report ë‹¨ê³„ ì‹¤í–‰."""
        from src.core.llm_manager import execute_llm_task, TaskType
        
        prompt = self._build_report_prompt(
            query, think_output, action_output, workspace, round_number
        )
        
        result = await execute_llm_task(
            prompt=prompt,
            task_type=TaskType.SYNTHESIS,
            temperature=0.3
        )
        
        return self._parse_report_output(result, round_number)
    
    async def action(
        self,
        query: str,
        think_output: ThinkOutput,
        workspace: WorkspaceContext
    ) -> ActionOutput:
        """Action ë‹¨ê³„ ì‹¤í–‰ (ë„êµ¬ í˜¸ì¶œ í¬í•¨)."""
        actions_taken = []
        new_information = []
        tool_calls = []
        errors = []
        
        # Knowledge gaps ê¸°ë°˜ìœ¼ë¡œ ì—°êµ¬ ë°©í–¥ ê²°ì •
        for direction in think_output.next_research_directions[:3]:  # ìƒìœ„ 3ê°œ ë°©í–¥
            try:
                # ì—¬ê¸°ì„œ ì‹¤ì œ MCP ë„êµ¬ í˜¸ì¶œ (ì˜ˆ: search, fetch ë“±)
                # ì‹¤ì œ êµ¬í˜„ì€ agent_orchestratorì™€ í†µí•© ì‹œ MCP ë„êµ¬ ì‚¬ìš©
                action_record = {
                    "direction": direction,
                    "type": "research",
                    "timestamp": datetime.now().isoformat()
                }
                actions_taken.append(action_record)
                
                # Placeholder for actual tool execution
                # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” MCP ë„êµ¬ë¥¼ í†µí•´ ì •ë³´ ìˆ˜ì§‘
                
            except Exception as e:
                errors.append(f"Action failed for direction '{direction}': {str(e)}")
        
        return ActionOutput(
            actions_taken=actions_taken,
            new_information=new_information,
            tool_calls=tool_calls,
            errors_encountered=errors
        )
    
    def _build_think_prompt(
        self,
        query: str,
        workspace: WorkspaceContext,
        round_number: int
    ) -> str:
        """Think ë‹¨ê³„ í”„ë¡¬í”„íŠ¸ ìƒì„±."""
        context_section = ""
        if workspace.evolving_summary:
            context_section = f"""
## Previous Research Summary
{workspace.evolving_summary}

## Known Knowledge Gaps
{chr(10).join(f"- {gap}" for gap in workspace.knowledge_gaps) if workspace.knowledge_gaps else "None identified yet"}

## Current Quality Score: {workspace.quality_score:.2f}
"""
        
        return f"""# Deep Research Think Phase (Round {round_number})

## Original Query
{query}

{context_section}

## Your Task
Analyze the current state of research and identify:
1. Your current understanding of the topic
2. Knowledge gaps that need to be filled
3. Next research directions to pursue
4. Hypotheses to verify
5. Your confidence level (0-1)

Respond in a structured format:

### Current Understanding
[Your synthesis of what is known]

### Knowledge Gaps
- [Gap 1]
- [Gap 2]
...

### Next Research Directions
- [Direction 1]
- [Direction 2]
...

### Hypotheses to Verify
- [Hypothesis 1]
- [Hypothesis 2]
...

### Confidence Level
[0.0 - 1.0]
"""
    
    def _build_report_prompt(
        self,
        query: str,
        think_output: ThinkOutput,
        action_output: ActionOutput,
        workspace: WorkspaceContext,
        round_number: int
    ) -> str:
        """Report ë‹¨ê³„ í”„ë¡¬í”„íŠ¸ ìƒì„±."""
        new_info = "\n".join(
            f"- {info.get('content', str(info))}" 
            for info in action_output.new_information
        ) if action_output.new_information else "No new information gathered in this round."
        
        return f"""# Deep Research Report Phase (Round {round_number})

## Original Query
{query}

## Previous Summary
{workspace.evolving_summary if workspace.evolving_summary else "First round - no previous summary"}

## Current Understanding (from Think phase)
{think_output.current_understanding}

## New Information Gathered
{new_info}

## Your Task
Create an evolving summary report that:
1. Synthesizes all information gathered so far
2. Highlights key findings with evidence
3. Assesses quality metrics (completeness, depth, accuracy, etc.)
4. Identifies remaining questions

Respond in a structured format:

### Executive Summary
[Comprehensive summary of research findings]

### Key Findings
1. [Finding 1] - Evidence: [source]
2. [Finding 2] - Evidence: [source]
...

### Quality Assessment
- Completeness: [0.0-1.0]
- Depth: [0.0-1.0]
- Source Diversity: [0.0-1.0]
- Factual Accuracy: [0.0-1.0]
- Coherence: [0.0-1.0]

### Remaining Questions
- [Question 1]
- [Question 2]
...
"""
    
    def _parse_think_output(self, result: str) -> ThinkOutput:
        """Think ê²°ê³¼ íŒŒì‹±."""
        # ê°„ë‹¨í•œ íŒŒì‹± (ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ë” ì •êµí•œ íŒŒì‹± í•„ìš”)
        lines = result.split('\n')
        
        current_understanding = ""
        knowledge_gaps = []
        directions = []
        hypotheses = []
        confidence = 0.5
        
        current_section = None
        
        for line in lines:
            line = line.strip()
            if "Current Understanding" in line:
                current_section = "understanding"
            elif "Knowledge Gaps" in line:
                current_section = "gaps"
            elif "Research Directions" in line or "Next Research" in line:
                current_section = "directions"
            elif "Hypotheses" in line:
                current_section = "hypotheses"
            elif "Confidence" in line:
                current_section = "confidence"
            elif line.startswith("- ") or line.startswith("* "):
                item = line[2:].strip()
                if current_section == "gaps":
                    knowledge_gaps.append(item)
                elif current_section == "directions":
                    directions.append(item)
                elif current_section == "hypotheses":
                    hypotheses.append(item)
            elif current_section == "understanding" and line:
                current_understanding += line + " "
            elif current_section == "confidence":
                try:
                    # ìˆ«ì ì¶”ì¶œ ì‹œë„
                    import re
                    match = re.search(r'([0-9.]+)', line)
                    if match:
                        confidence = min(1.0, max(0.0, float(match.group(1))))
                except:
                    pass
        
        return ThinkOutput(
            current_understanding=current_understanding.strip(),
            knowledge_gaps=knowledge_gaps,
            next_research_directions=directions,
            hypotheses=hypotheses,
            confidence_level=confidence
        )
    
    def _parse_report_output(self, result: str, round_number: int) -> ReportOutput:
        """Report ê²°ê³¼ íŒŒì‹±."""
        lines = result.split('\n')
        
        executive_summary = ""
        key_findings = []
        remaining_questions = []
        quality_metrics = QualityMetrics()
        
        current_section = None
        
        for line in lines:
            line = line.strip()
            if "Executive Summary" in line:
                current_section = "summary"
            elif "Key Findings" in line:
                current_section = "findings"
            elif "Quality Assessment" in line:
                current_section = "quality"
            elif "Remaining Questions" in line:
                current_section = "questions"
            elif current_section == "summary" and line and not line.startswith("#"):
                executive_summary += line + " "
            elif current_section == "findings" and (line.startswith("- ") or line.startswith("* ") or line[0:2].isdigit()):
                key_findings.append({"content": line.lstrip("0123456789.-* ").strip()})
            elif current_section == "questions" and (line.startswith("- ") or line.startswith("* ")):
                remaining_questions.append(line[2:].strip())
            elif current_section == "quality":
                try:
                    import re
                    if "Completeness" in line:
                        match = re.search(r'([0-9.]+)', line)
                        if match:
                            quality_metrics.completeness = float(match.group(1))
                    elif "Depth" in line:
                        match = re.search(r'([0-9.]+)', line)
                        if match:
                            quality_metrics.depth = float(match.group(1))
                    elif "Source Diversity" in line:
                        match = re.search(r'([0-9.]+)', line)
                        if match:
                            quality_metrics.source_diversity = float(match.group(1))
                    elif "Factual Accuracy" in line or "Accuracy" in line:
                        match = re.search(r'([0-9.]+)', line)
                        if match:
                            quality_metrics.factual_accuracy = float(match.group(1))
                    elif "Coherence" in line:
                        match = re.search(r'([0-9.]+)', line)
                        if match:
                            quality_metrics.coherence = float(match.group(1))
                except:
                    pass
        
        return ReportOutput(
            round_number=round_number,
            executive_summary=executive_summary.strip(),
            key_findings=key_findings,
            sources_used=[],
            quality_metrics=quality_metrics,
            remaining_questions=remaining_questions
        )


# Singleton instance
_iterative_research_engine: Optional[IterativeResearchEngine] = None


def get_iterative_research_engine(
    max_rounds: int = 5,
    quality_threshold: float = 0.8
) -> IterativeResearchEngine:
    """IterativeResearchEngine ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜."""
    global _iterative_research_engine
    
    if _iterative_research_engine is None:
        _iterative_research_engine = IterativeResearchEngine(
            max_rounds=max_rounds,
            quality_threshold=quality_threshold
        )
    
    return _iterative_research_engine
