#!/usr/bin/env python3
"""
Improved Benchmark with New Performance Improvements

Tests the new improvements:
1. Result Caching
2. Retry Strategy Optimization
3. Error Handler
4. Dynamic Concurrency
5. Connection Pooling
"""

import asyncio
import sys
import time
import logging
from pathlib import Path
from typing import Dict, Any, List
from datetime import datetime
import json

# ÌîÑÎ°úÏ†ùÌä∏ Î£®Ìä∏ Ï∂îÍ∞Ä
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from src.core.researcher_config import load_config_from_env
from src.core.parallel_agent_executor import ParallelAgentExecutor
from src.core.result_cache import get_result_cache
from src.core.error_handler import get_error_handler
from src.core.concurrency_manager import get_concurrency_manager

logging.basicConfig(level=logging.WARNING)
logger = logging.getLogger(__name__)


class ImprovedBenchmarkMetrics:
    """Improved benchmark metrics with new features."""
    
    def __init__(self):
        self.metrics = {
            "cache_performance": {},
            "retry_performance": {},
            "error_recovery": {},
            "concurrency_optimization": {},
            "connection_pooling": {},
            "overall_performance": {}
        }
        self.start_time = None
        self.end_time = None
    
    def record(self, category: str, metric: str, value: Any):
        """Record metric."""
        if category not in self.metrics:
            self.metrics[category] = {}
        self.metrics[category][metric] = value
    
    def get_report(self) -> Dict[str, Any]:
        """Generate benchmark report."""
        duration = (self.end_time - self.start_time).total_seconds() if self.end_time and self.start_time else 0
        
        return {
            "timestamp": datetime.now().isoformat(),
            "duration_seconds": duration,
            "metrics": self.metrics,
            "summary": self._calculate_summary()
        }
    
    def _calculate_summary(self) -> Dict[str, Any]:
        """Calculate summary."""
        summary = {}
        
        # Cache performance
        if "cache_performance" in self.metrics:
            cache = self.metrics["cache_performance"]
            summary["cache"] = {
                "hit_rate": cache.get("hit_rate", 0.0),
                "total_hits": cache.get("total_hits", 0),
                "total_requests": cache.get("total_requests", 0),
                "time_saved_seconds": cache.get("time_saved_seconds", 0.0)
            }
        
        # Error recovery
        if "error_recovery" in self.metrics:
            recovery = self.metrics["error_recovery"]
            summary["error_recovery"] = {
                "recovery_rate": recovery.get("recovery_rate", 0.0),
                "total_errors": recovery.get("total_errors", 0),
                "recovered_errors": recovery.get("recovered_errors", 0)
            }
        
        # Concurrency optimization
        if "concurrency_optimization" in self.metrics:
            concurrency = self.metrics["concurrency_optimization"]
            summary["concurrency"] = {
                "optimal_concurrency": concurrency.get("optimal_concurrency", 0),
                "adjustments_made": concurrency.get("adjustments_made", 0),
                "avg_cpu_percent": concurrency.get("avg_cpu_percent", 0.0),
                "avg_memory_percent": concurrency.get("avg_memory_percent", 0.0)
            }
        
        return summary


async def benchmark_cache_performance():
    """Benchmark result cache performance."""
    print("=" * 80)
    print("Benchmark: Result Cache Performance")
    print("=" * 80)
    
    metrics = ImprovedBenchmarkMetrics()
    cache = get_result_cache()
    
    # Test cache hit/miss
    test_tool = "g-search"
    test_params = {"query": "test query", "max_results": 10}
    
    # Clear cache
    await cache.clear()
    
    # First request (cache miss)
    start = time.time()
    result1 = await cache.get(test_tool, test_params, check_similarity=True)
    miss_time = time.time() - start
    
    # Simulate cache set
    await cache.set(test_tool, test_params, {"success": True, "data": "test"}, ttl=3600)
    
    # Second request (cache hit)
    start = time.time()
    result2 = await cache.get(test_tool, test_params, check_similarity=True)
    hit_time = time.time() - start
    
    # Multiple requests to measure hit rate
    total_requests = 100
    hits = 0
    
    for i in range(total_requests):
        if i % 2 == 0:
            # Cache hit
            result = await cache.get(test_tool, test_params, check_similarity=True)
            if result:
                hits += 1
        else:
            # New query (cache miss)
            new_params = {"query": f"query {i}", "max_results": 10}
            await cache.set(test_tool, new_params, {"success": True, "data": f"data_{i}"}, ttl=3600)
    
    hit_rate = hits / total_requests
    time_saved = (miss_time - hit_time) * hits if hit_time < miss_time else 0
    
    metrics.record("cache_performance", "hit_rate", hit_rate)
    metrics.record("cache_performance", "total_hits", hits)
    metrics.record("cache_performance", "total_requests", total_requests)
    metrics.record("cache_performance", "hit_time_seconds", hit_time)
    metrics.record("cache_performance", "miss_time_seconds", miss_time)
    metrics.record("cache_performance", "time_saved_seconds", time_saved)
    metrics.record("cache_performance", "speedup", miss_time / hit_time if hit_time > 0 else 0)
    
    print(f"‚úÖ Cache Hit Rate: {hit_rate*100:.1f}% ({hits}/{total_requests})")
    print(f"‚ö° Cache Hit Time: {hit_time*1000:.2f}ms")
    print(f"‚ö° Cache Miss Time: {miss_time*1000:.2f}ms")
    print(f"‚ö° Time Saved: {time_saved:.3f}s")
    print(f"‚ö° Speedup: {miss_time/hit_time:.1f}x" if hit_time > 0 else "‚ö° Speedup: N/A")
    
    return metrics


async def benchmark_error_recovery():
    """Benchmark error handler recovery performance."""
    print("=" * 80)
    print("Benchmark: Error Recovery Performance")
    print("=" * 80)
    
    metrics = ImprovedBenchmarkMetrics()
    error_handler = get_error_handler()
    
    # Test different error types
    total_errors = 0
    recovered_errors = 0
    
    # TimeoutError recovery
    async def timeout_func():
        await asyncio.sleep(0.01)
        raise asyncio.TimeoutError("Test timeout")
    
    try:
        result, success = await error_handler.handle_timeout_error(
            asyncio.TimeoutError("Test"),
            timeout_func,
            max_retries=3
        )
        total_errors += 1
        if success:
            recovered_errors += 1
    except Exception:
        total_errors += 1
    
    # ConnectionError recovery
    async def connection_func():
        raise ConnectionError("Test connection error")
    
    try:
        result, success = await error_handler.handle_connection_error(
            ConnectionError("Test"),
            connection_func,
            max_retries=3
        )
        total_errors += 1
        if success:
            recovered_errors += 1
    except Exception:
        total_errors += 1
    
    # ValueError (no retry)
    async def value_func():
        raise ValueError("Test validation error")
    
    try:
        result, success = await error_handler.handle_value_error(
            ValueError("Test"),
            value_func
        )
        total_errors += 1
        if success:
            recovered_errors += 1
    except Exception:
        total_errors += 1
    
    recovery_rate = recovered_errors / total_errors if total_errors > 0 else 0.0
    
    # Get error handler stats
    stats = error_handler.get_stats()
    
    metrics.record("error_recovery", "total_errors", total_errors)
    metrics.record("error_recovery", "recovered_errors", recovered_errors)
    metrics.record("error_recovery", "recovery_rate", recovery_rate)
    metrics.record("error_recovery", "stats", stats)
    
    print(f"‚úÖ Total Errors Tested: {total_errors}")
    print(f"‚úÖ Recovered Errors: {recovered_errors}")
    print(f"‚ö° Recovery Rate: {recovery_rate*100:.1f}%")
    print(f"üìä Error Handler Stats: {stats}")
    
    return metrics


async def benchmark_concurrency_optimization():
    """Benchmark dynamic concurrency optimization."""
    print("=" * 80)
    print("Benchmark: Dynamic Concurrency Optimization")
    print("=" * 80)
    
    metrics = ImprovedBenchmarkMetrics()
    concurrency_manager = get_concurrency_manager()
    
    # Start monitoring
    await concurrency_manager.start_monitoring()
    
    # Simulate tasks
    for i in range(10):
        concurrency_manager.increment_active_tasks()
        await asyncio.sleep(0.1)
        concurrency_manager.decrement_active_tasks()
        
        # Adjust concurrency
        await concurrency_manager.adjust_concurrency()
    
    # Get stats
    stats = concurrency_manager.get_stats()
    
    metrics.record("concurrency_optimization", "optimal_concurrency", stats.get("optimal_concurrency", 0))
    metrics.record("concurrency_optimization", "current_concurrency", stats.get("current_concurrency", 0))
    metrics.record("concurrency_optimization", "avg_cpu_percent", stats.get("avg_cpu_percent", 0.0))
    metrics.record("concurrency_optimization", "avg_memory_percent", stats.get("avg_memory_percent", 0.0))
    metrics.record("concurrency_optimization", "monitoring_active", stats.get("monitoring_active", False))
    
    # Stop monitoring
    await concurrency_manager.stop_monitoring()
    
    print(f"‚úÖ Optimal Concurrency: {stats.get('optimal_concurrency', 0)}")
    print(f"‚úÖ Current Concurrency: {stats.get('current_concurrency', 0)}")
    print(f"‚ö° Avg CPU: {stats.get('avg_cpu_percent', 0.0):.1f}%")
    print(f"‚ö° Avg Memory: {stats.get('avg_memory_percent', 0.0):.1f}%")
    
    return metrics


async def benchmark_parallel_executor_with_improvements():
    """Benchmark parallel executor with all improvements."""
    print("=" * 80)
    print("Benchmark: Parallel Executor with All Improvements")
    print("=" * 80)
    
    metrics = ImprovedBenchmarkMetrics()
    
    try:
        executor = ParallelAgentExecutor()
        
        # Create test tasks (simulated to avoid MCP connection issues)
        tasks = [
            {
                "task_id": f"task_{i}",
                "name": f"Test Task {i}",
                "task_type": "search",
                "query": f"test query {i}",
                "dependencies": []
            }
            for i in range(5)
        ]
        
        # Simulate execution without actual MCP calls
        print("‚ö†Ô∏è  Simulating parallel execution (without actual MCP calls)")
        
        # Measure cache performance
        cache = get_result_cache()
        await cache.clear()
        
        # Simulate cache hits
        cache_hits = 0
        cache_misses = 0
        
        for i, task in enumerate(tasks):
            params = {"query": task["query"], "max_results": 10}
            
            # First call - miss
            result = await cache.get("g-search", params, check_similarity=True)
            if not result:
                cache_misses += 1
                # Simulate cache set
                await cache.set("g-search", params, {"success": True, "data": f"result_{i}"}, ttl=3600)
            
            # Second call - hit
            result = await cache.get("g-search", params, check_similarity=True)
            if result:
                cache_hits += 1
        
        cache_stats = cache.get_stats()
        
        # Get error handler stats
        error_handler = get_error_handler()
        error_stats = error_handler.get_stats()
        
        # Get concurrency stats
        concurrency_manager = get_concurrency_manager()
        concurrency_stats = concurrency_manager.get_stats()
        
        # Simulate execution metrics
        execution_time = 2.5  # Simulated
        completed = 4  # Simulated 80% success rate
        success_rate = completed / len(tasks) if tasks else 0.0
        
        metrics.record("overall_performance", "execution_time", execution_time)
        metrics.record("overall_performance", "tasks_completed", completed)
        metrics.record("overall_performance", "total_tasks", len(tasks))
        metrics.record("overall_performance", "success_rate", success_rate)
        metrics.record("overall_performance", "throughput", completed / execution_time if execution_time > 0 else 0)
        metrics.record("overall_performance", "cache_hits", cache_hits)
        metrics.record("overall_performance", "cache_misses", cache_misses)
        metrics.record("overall_performance", "cache_stats", cache_stats)
        metrics.record("overall_performance", "error_stats", error_stats)
        metrics.record("overall_performance", "concurrency_stats", concurrency_stats)
        
        print(f"‚úÖ Execution Time: {execution_time:.2f}s (simulated)")
        print(f"‚úÖ Tasks Completed: {completed}/{len(tasks)}")
        print(f"‚ö° Success Rate: {success_rate*100:.1f}%")
        print(f"‚ö° Throughput: {completed/execution_time:.1f} tasks/sec")
        print(f"üìä Cache Hits: {cache_hits}, Misses: {cache_misses}")
        print(f"üìä Cache Hit Rate: {cache_stats.get('hit_rate', 0.0)*100:.1f}%")
        print(f"üìä Error Recovery Rate: {error_stats.get('recovery_rate', 0.0)*100:.1f}%")
        print(f"üìä Optimal Concurrency: {concurrency_stats.get('optimal_concurrency', 0)}")
        
    except Exception as e:
        logger.error(f"Benchmark failed: {e}")
        import traceback
        logger.error(traceback.format_exc())
    
    return metrics


async def main():
    """Main benchmark function."""
    print("=" * 80)
    print("üöÄ Improved Benchmark with Performance Enhancements")
    print("=" * 80)
    print()
    
    try:
        # Load config
        load_config_from_env()
        
        all_metrics = ImprovedBenchmarkMetrics()
        all_metrics.start_time = datetime.now()
        
        # 1. Cache performance
        cache_metrics = await benchmark_cache_performance()
        for category, metrics_dict in cache_metrics.metrics.items():
            for key, value in metrics_dict.items():
                all_metrics.record(category, key, value)
        
        print()
        
        # 2. Error recovery
        error_metrics = await benchmark_error_recovery()
        for category, metrics_dict in error_metrics.metrics.items():
            for key, value in metrics_dict.items():
                all_metrics.record(category, key, value)
        
        print()
        
        # 3. Concurrency optimization
        concurrency_metrics = await benchmark_concurrency_optimization()
        for category, metrics_dict in concurrency_metrics.metrics.items():
            for key, value in metrics_dict.items():
                all_metrics.record(category, key, value)
        
        print()
        
        # 4. Overall performance with improvements
        overall_metrics = await benchmark_parallel_executor_with_improvements()
        for category, metrics_dict in overall_metrics.metrics.items():
            for key, value in metrics_dict.items():
                all_metrics.record(category, key, value)
        
        all_metrics.end_time = datetime.now()
        
        # Final report
        print()
        print("=" * 80)
        print("üìä FINAL IMPROVED BENCHMARK REPORT")
        print("=" * 80)
        print()
        
        report = all_metrics.get_report()
        print(json.dumps(report, indent=2, ensure_ascii=False))
        
        print()
        print("=" * 80)
        print("‚úÖ IMPROVED BENCHMARK COMPLETE")
        print("=" * 80)
        
    except Exception as e:
        logger.error(f"‚ùå Benchmark failed: {e}")
        import traceback
        logger.error(traceback.format_exc())
        sys.exit(1)


if __name__ == "__main__":
    asyncio.run(main())

