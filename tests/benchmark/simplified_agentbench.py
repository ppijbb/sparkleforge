#!/usr/bin/env python3
"""
ê°„ì†Œí™”ëœ AgentBench ì ìˆ˜ ì¸¡ì • - ë¹ ë¥¸ ì‹¤í–‰ê³¼ ëª…í™•í•œ ì¸¡ì •
"""

import sys
import json
import asyncio
from pathlib import Path
from typing import Dict, List
from collections import defaultdict

project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(Path(__file__).parent))

from src.core.researcher_config import load_config_from_env
from src.core.autonomous_orchestrator import AutonomousOrchestrator


async def measure_single_task(query: str, category: str) -> Dict:
    """ë‹¨ì¼ íƒœìŠ¤í¬ ì‹¤í–‰ ë° ì ìˆ˜ ì¸¡ì •"""
    try:
        load_config_from_env()
        orchestrator = AutonomousOrchestrator()
        
        # ì‹¤ì œ ì‹¤í–‰
        result = await orchestrator.run_research(query)
        
        if not result:
            return {
                'category': category,
                'success': False,
                'score': 0.0,
                'error': 'No result returned'
            }
        
        # ê²°ê³¼ ë¶„ì„
        execution_results = result.get('execution_results', [])
        sources = result.get('sources', []) or []
        planned_tasks = result.get('planned_tasks', []) or []
        agent_status = result.get('agent_status', {}) or {}
        
        # ì¹´í…Œê³ ë¦¬ë³„ ì ìˆ˜ ê³„ì‚°
        if category == 'WebNavigation':
            # Web Navigation: ì†ŒìŠ¤ ìˆ˜ì™€ ê²€ìƒ‰ ì„±ê³µ
            sources_score = min(1.0, len(sources) / 8.0)
            execution_score = 1.0 if len(execution_results) > 0 else 0.0
            score = sources_score * 0.7 + execution_score * 0.3
            
        elif category == 'ToolUsage':
            # Tool Usage: ë„êµ¬ ì‚¬ìš© ì„±ê³µ
            tools_used = set()
            for exec_result in execution_results:
                if exec_result.get('tool_used'):
                    tools_used.add(exec_result['tool_used'])
            tools_score = min(1.0, len(tools_used) / 5.0)
            execution_score = 1.0 if len(execution_results) > 0 else 0.0
            score = tools_score * 0.6 + execution_score * 0.4
            
        elif category == 'MultiAgent':
            # Multi-Agent: ë³‘ë ¬ ì‹¤í–‰ ë° ì—ì´ì „íŠ¸ í˜‘ì—…
            parallel_score = 1.0 if any(exec_result.get('parallel_execution', False) 
                                       for exec_result in execution_results) else 0.0
            collaboration_score = 1.0 if len(agent_status) > 1 else 0.5
            score = parallel_score * 0.5 + collaboration_score * 0.5
            
        elif category == 'Reasoning':
            # Reasoning: ë…¼ë¦¬ì  ë¶„ì„ ë‹¨ê³„
            planning_score = min(1.0, len(planned_tasks) / 5.0)
            analysis_score = 1.0 if result.get('analyzed_objectives') else 0.0
            score = planning_score * 0.6 + analysis_score * 0.4
            
        else:
            score = 0.0
        
        return {
            'category': category,
            'success': True,
            'score': score * 100,  # Convert to percentage
            'sources_count': len(sources),
            'execution_results_count': len(execution_results),
            'planned_tasks_count': len(planned_tasks),
            'agents_count': len(agent_status)
        }
        
    except Exception as e:
        return {
            'category': category,
            'success': False,
            'score': 0.0,
            'error': str(e)
        }


async def measure_agentbench_scores() -> Dict[str, float]:
    """AgentBench ì ìˆ˜ ì¸¡ì •"""
    print("=" * 80)
    print("ğŸš€ AgentBench ì ìˆ˜ ì¸¡ì •")
    print("=" * 80)
    print()
    
    # ì¹´í…Œê³ ë¦¬ë³„ í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬
    test_queries = {
        'WebNavigation': 'Latest AI developments in 2025',
        'ToolUsage': 'Analyze remote work productivity trends',
        'MultiAgent': 'Innovation in education and learning methods',
        'Reasoning': 'Logical analysis of AI ethics implications'
    }
    
    results = {}
    category_scores = defaultdict(list)
    
    for category, query in test_queries.items():
        print(f"ğŸ“Š {category}: {query}")
        try:
            result = await measure_single_task(query, category)
            results[category] = result
            category_scores[category].append(result['score'])
            status = "âœ…" if result['success'] else "âŒ"
            print(f"   {status} Score: {result['score']:.1f}%")
            if not result['success']:
                print(f"      Error: {result.get('error', 'Unknown')}")
        except Exception as e:
            print(f"   âŒ Error: {e}")
            results[category] = {'category': category, 'success': False, 'score': 0.0, 'error': str(e)}
        print()
    
    # ì¹´í…Œê³ ë¦¬ë³„ í‰ê·  ê³„ì‚°
    scores = {}
    category_names = {
        'WebNavigation': 'Web Navigation',
        'ToolUsage': 'Tool Usage',
        'MultiAgent': 'Multi-Agent',
        'Reasoning': 'Reasoning'
    }
    
    for category, score_list in category_scores.items():
        if score_list:
            avg = sum(score_list) / len(score_list)
            scores[category_names[category]] = avg
        else:
            scores[category_names[category]] = 0.0
    
    # Overall ê³„ì‚°
    if scores:
        overall = sum(scores.values()) / len(scores)
        scores['Overall'] = overall
    
    return scores


def main():
    """ë©”ì¸ ì‹¤í–‰"""
    scores = asyncio.run(measure_agentbench_scores())
    
    print("=" * 80)
    print("ğŸ“Š AgentBench Scores")
    print("=" * 80)
    print()
    
    for category, score in scores.items():
        print(f"  {category}: {score:.1f}%")
    
    print()
    print("=" * 80)
    
    # ê²°ê³¼ ì €ì¥
    output_file = project_root / "results" / "agentbench_scores.json"
    output_file.parent.mkdir(parents=True, exist_ok=True)
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(scores, f, indent=2)
    
    print(f"ğŸ’¾ ì €ì¥ë¨: {output_file}")
    
    # README ì—…ë°ì´íŠ¸ ì œì•ˆ
    print()
    print("ğŸ’¡ README ì—…ë°ì´íŠ¸:")
    print("   ë‹¤ìŒ ê°’ë“¤ë¡œ READMEë¥¼ ì—…ë°ì´íŠ¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:")
    for category, score in scores.items():
        if category != 'Overall':
            print(f"   {category}: {score:.1f}%")


if __name__ == "__main__":
    main()

