#!/usr/bin/env python3
"""
AgentBench ì¸¡ì • ë°©ë²• ê°œì„  - ì‹¤ì œ ê²°ê³¼ êµ¬ì¡°ì— ë§ê²Œ ìˆ˜ì •
"""

import sys
import json
from pathlib import Path
from typing import Dict, Any

project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(Path(__file__).parent))

from benchmark_runner import BenchmarkRunner


def analyze_result_structure():
    """ì‹¤ì œ ì‹¤í–‰ ê²°ê³¼ êµ¬ì¡° ë¶„ì„"""
    print("=" * 80)
    print("ğŸ” ê²°ê³¼ êµ¬ì¡° ë¶„ì„")
    print("=" * 80)
    print()
    
    runner = BenchmarkRunner(
        str(project_root),
        str(project_root / "tests" / "benchmark" / "benchmark_config.yaml"),
        str(project_root / "tests" / "benchmark" / "benchmark_thresholds.yaml")
    )
    
    # ì²« ë²ˆì§¸ íƒœìŠ¤í¬ ì‹¤í–‰
    agent_tasks = runner.config.get('agent_tasks', [])
    if not agent_tasks:
        print("âŒ No agent tasks found")
        return
    
    test_task = agent_tasks[0]
    print(f"ğŸ“Š í…ŒìŠ¤íŠ¸: {test_task.get('id')} - {test_task.get('query')}")
    print()
    
    # CLI ì‹¤í–‰
    print("â³ CLI ì‹¤í–‰ ì¤‘...")
    cli_result = runner.cli_executor.execute_research(test_task['query'])
    
    print(f"âœ… ì‹¤í–‰ ì™„ë£Œ: {cli_result.success}")
    print(f"   Execution Time: {cli_result.execution_time:.2f}s")
    print()
    
    if cli_result.parsed_output:
        print("ğŸ“‹ ê²°ê³¼ êµ¬ì¡°:")
        print(json.dumps(cli_result.parsed_output, indent=2, default=str)[:1000])
        print("...")
        print()
        
        # ë©”íŠ¸ë¦­ ì¶”ì¶œ í…ŒìŠ¤íŠ¸
        print("ğŸ” ë©”íŠ¸ë¦­ ì¶”ì¶œ í…ŒìŠ¤íŠ¸:")
        extracted = runner._extract_agent_metrics_from_output(cli_result.parsed_output)
        print(json.dumps(extracted, indent=2, default=str))
        print()
        
        # ë©”íŠ¸ë¦­ ìˆ˜ì§‘ í…ŒìŠ¤íŠ¸
        print("ğŸ“Š ë©”íŠ¸ë¦­ ìˆ˜ì§‘ í…ŒìŠ¤íŠ¸:")
        metrics = runner._collect_agent_metrics_for_task(test_task, extracted, 0.0)
        print(f"   Collected {len(metrics)} metrics")
        for metric in metrics[:5]:
            print(f"   - {metric.name}: {metric.value:.2%} (category: {metric.category})")
        print()
        
        # ì ìˆ˜ ê³„ì‚° í…ŒìŠ¤íŠ¸
        if metrics:
            score = runner._calculate_test_score(metrics)
            print(f"ğŸ“Š Overall Score: {score:.2%}")
    else:
        print("âŒ No parsed output")
        if cli_result.error_message:
            print(f"   Error: {cli_result.error_message}")
        if cli_result.stdout:
            print(f"   Stdout: {cli_result.stdout[:500]}")
        if cli_result.stderr:
            print(f"   Stderr: {cli_result.stderr[:500]}")


def fix_metric_extraction():
    """ë©”íŠ¸ë¦­ ì¶”ì¶œ ë¡œì§ ê°œì„  ì œì•ˆ"""
    print()
    print("=" * 80)
    print("ğŸ’¡ ë©”íŠ¸ë¦­ ì¶”ì¶œ ê°œì„  ì œì•ˆ")
    print("=" * 80)
    print()
    print("ë¬¸ì œì :")
    print("1. CLI ì‹¤í–‰ ê²°ê³¼ê°€ ê¸°ëŒ€í•˜ëŠ” êµ¬ì¡°ì™€ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ")
    print("2. navigation_log, tool_usage_log ë“±ì´ ì‹¤ì œ ê²°ê³¼ì— ì—†ì„ ìˆ˜ ìˆìŒ")
    print("3. execution_results, sources ë“±ì˜ ì‹¤ì œ í•„ë“œë¥¼ ì‚¬ìš©í•´ì•¼ í•¨")
    print()
    print("ê°œì„  ë°©ì•ˆ:")
    print("1. ì‹¤ì œ ê²°ê³¼ êµ¬ì¡°ì— ë§ê²Œ ë©”íŠ¸ë¦­ ì¶”ì¶œ ë¡œì§ ìˆ˜ì •")
    print("2. execution_resultsì—ì„œ ë„êµ¬ ì‚¬ìš©, ë³‘ë ¬ ì‹¤í–‰ ë“± ì¶”ì¶œ")
    print("3. sourcesì—ì„œ ì›¹ ë„¤ë¹„ê²Œì´ì…˜ ì„±ê³µ ì—¬ë¶€ ì¶”ì¶œ")
    print("4. planned_tasksì—ì„œ ë…¼ë¦¬ì  ê³„íš ì¶”ì¶œ")
    print()


if __name__ == "__main__":
    analyze_result_structure()
    fix_metric_extraction()

