#!/usr/bin/env python3
"""
Quick AgentBench score measurement - ì‹¤ì œ ì—ì´ì „íŠ¸ ì‹¤í–‰ìœ¼ë¡œ ì„±ëŠ¥ ì¸¡ì •
"""

import sys
import json
import time
from pathlib import Path
from typing import Dict, List, Any

# Add project root to path
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(Path(__file__).parent))

from src.core.researcher_config import load_config_from_env
from src.core.autonomous_orchestrator import AutonomousOrchestrator

async def measure_agent_performance(query: str, category: str) -> Dict[str, Any]:
    """ì‹¤ì œ ì—ì´ì „íŠ¸ ì‹¤í–‰ìœ¼ë¡œ ì„±ëŠ¥ ì¸¡ì •"""
    try:
        load_config_from_env()
        orchestrator = AutonomousOrchestrator()
        
        start_time = time.time()
        result = await orchestrator.run_research(query)
        execution_time = time.time() - start_time
        
        # ê²°ê³¼ì—ì„œ ì„±ê³µ ì—¬ë¶€ íŒë‹¨
        success = result is not None
        execution_results = result.get('execution_results', []) if result else []
        sources_count = len(result.get('sources', [])) if result else 0
        
        # ì‹¤í–‰ ê²°ê³¼ì—ì„œ ë„êµ¬ ì‚¬ìš©, ë³‘ë ¬ ì‹¤í–‰ ë“± ì¶”ì¶œ
        tools_used = []
        parallel_executed = False
        reasoning_steps = []
        
        if execution_results:
            for exec_result in execution_results:
                if exec_result.get('tool_used'):
                    tools_used.append(exec_result['tool_used'])
                if exec_result.get('parallel_execution'):
                    parallel_executed = True
                if exec_result.get('reasoning'):
                    reasoning_steps.append(exec_result['reasoning'])
        
        # ì¹´í…Œê³ ë¦¬ë³„ ì ìˆ˜ ê³„ì‚°
        if category == 'WebNavigation':
            # Web Navigation ì ìˆ˜: ì†ŒìŠ¤ ìˆ˜ì™€ ì„±ê³µ ì—¬ë¶€ ê¸°ë°˜
            score = min(1.0, (sources_count / 5.0) * 0.8 + (1.0 if success else 0.0) * 0.2)
        elif category == 'ToolUsage':
            # Tool Usage ì ìˆ˜: ì‹¤í–‰ ì„±ê³µê³¼ ë„êµ¬ ì‚¬ìš© ê¸°ë°˜
            tools_count = len(tools_used)
            score = min(1.0, (1.0 if success else 0.0) * 0.6 + min(1.0, tools_count / 3.0) * 0.4)
        elif category == 'MultiAgent':
            # Multi-Agent ì ìˆ˜: ë³‘ë ¬ ì‹¤í–‰ ì„±ê³µ ê¸°ë°˜
            score = min(1.0, (1.0 if success else 0.0) * 0.7 + (1.0 if parallel_executed else 0.0) * 0.3)
        elif category == 'Reasoning':
            # Reasoning ì ìˆ˜: ë…¼ë¦¬ì  ì¼ê´€ì„±ê³¼ ì„±ê³µ ê¸°ë°˜
            reasoning_count = len(reasoning_steps)
            score = min(1.0, (1.0 if success else 0.0) * 0.5 + min(1.0, reasoning_count / 5.0) * 0.5)
        else:
            score = 1.0 if success else 0.0
        
        return {
            'success': success,
            'score': score,
            'execution_time': execution_time,
            'sources_count': sources_count,
            'tools_used': tools_used,
            'parallel_executed': parallel_executed,
            'reasoning_steps': reasoning_steps,
            'result': result
        }
    except Exception as e:
        print(f"Error measuring {category}: {e}")
        return {
            'success': False,
            'score': 0.0,
            'execution_time': 0.0,
            'sources_count': 0,
            'error': str(e)
        }


async def calculate_agentbench_scores() -> Dict[str, float]:
    """ì‹¤ì œ ì‹¤í–‰ìœ¼ë¡œ AgentBench ì ìˆ˜ ê³„ì‚°"""
    print("=" * 80)
    print("ğŸš€ Measuring AgentBench Performance")
    print("=" * 80)
    print()
    
    # ê° ì¹´í…Œê³ ë¦¬ë³„ í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬
    test_queries = {
        'WebNavigation': 'Latest AI developments in 2025',
        'ToolUsage': 'Analyze remote work productivity trends',
        'MultiAgent': 'Innovation in education and learning methods',
        'Reasoning': 'Logical analysis of AI ethics implications'
    }
    
    scores = {}
    results = {}
    
    for category, query in test_queries.items():
        print(f"ğŸ“Š Testing {category}: {query}")
        result = await measure_agent_performance(query, category)
        scores[category] = result['score'] * 100  # Convert to percentage
        results[category] = result
        print(f"   Score: {scores[category]:.1f}% (Success: {result['success']}, Time: {result['execution_time']:.1f}s)")
        print()
    
    # Overall score ê³„ì‚°
    if scores:
        overall = sum(scores.values()) / len(scores)
        scores['Overall'] = overall
        print(f"ğŸ“Š Overall Score: {overall:.1f}%")
    
    return scores


if __name__ == "__main__":
    import asyncio
    scores = asyncio.run(calculate_agentbench_scores())
    
    print("=" * 80)
    print("ğŸ“Š AgentBench Scores")
    print("=" * 80)
    print()
    
    category_names = {
        'WebNavigation': 'Web Navigation',
        'ToolUsage': 'Tool Usage',
        'MultiAgent': 'Multi-Agent',
        'Reasoning': 'Reasoning',
        'Overall': 'Overall Score'
    }
    
    for key, value in scores.items():
        name = category_names.get(key, key)
        print(f"  {name}: {value:.1f}%")
    
    print()
    print("=" * 80)
    
    # Save results
    output_file = project_root / "results" / "agentbench_scores.json"
    output_file.parent.mkdir(parents=True, exist_ok=True)
    
    formatted_scores = {category_names.get(k, k): v for k, v in scores.items()}
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(formatted_scores, f, indent=2)
    
    print(f"ğŸ’¾ Scores saved to: {output_file}")

