#!/usr/bin/env python3
"""
Autonomous Multi-Agent Research System - Main Entry Point
Implements 9 Core Innovations: Production-Grade Reliability, Universal MCP Hub, Streaming Pipeline

MCP agent ë¼ì´ë¸ŒëŸ¬ë¦¬ ê¸°ë°˜ì˜ ììœ¨ ë¦¬ì„œì²˜ ì‹œìŠ¤í…œ.
ëª¨ë“  í•˜ë“œì½”ë”©, fallback, mock ì½”ë“œë¥¼ ì œê±°í•˜ê³  ì‹¤ì œ MCP agentë¥¼ ì‚¬ìš©.

í˜„ì¬ ìƒíƒœ: Production Level ê°œë°œ ì§„í–‰ ì¤‘ ğŸš§

Usage:
    python main.py --request "ì—°êµ¬ ì£¼ì œ"                    # CLI ëª¨ë“œ
    python main.py --web                                    # ì›¹ ëª¨ë“œ
    python main.py --mcp-server                            # MCP ì„œë²„ ëª¨ë“œ
    python main.py --streaming                             # ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œ
"""

import asyncio
import sys
import argparse
import subprocess
import signal
import logging
from pathlib import Path
from typing import Dict, Any, Optional, List
from datetime import datetime
import json
import os
import time
# Add project root to path
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from src.core.researcher_config import load_config_from_env

# CRITICAL: Load configuration BEFORE importing any modules that depend on it
config = load_config_from_env()

# Use new AgentOrchestrator for multi-agent orchestration
from src.core.agent_orchestrator import AgentOrchestrator as NewAgentOrchestrator
from src.core.autonomous_orchestrator import AutonomousOrchestrator
from src.monitoring.system_monitor import HealthMonitor

# Configure logging for production-grade reliability
# Advanced logging setup: setup logger manually to ensure logs directory exists and avoid issues with logging.basicConfig (per best practices)
log_dir = project_root / "logs"
log_dir.mkdir(parents=True, exist_ok=True)
log_file = log_dir / "researcher.log"

# Streamlit ê²½ê³  í•„í„°ë§ (CLI ëª¨ë“œì—ì„œ streamlitì´ importë  ë•Œ ë°œìƒí•˜ëŠ” ê²½ê³  ë¬´ì‹œ)
logging.getLogger("streamlit.runtime.scriptrunner_utils.script_run_context").setLevel(logging.ERROR)
logging.getLogger("streamlit").setLevel(logging.ERROR)

# HTTP ì—ëŸ¬ ë©”ì‹œì§€ í•„í„°ë§ í´ë˜ìŠ¤
class HTTPErrorFilter(logging.Filter):
    """HTML ì—ëŸ¬ ì‘ë‹µì„ í•„í„°ë§í•˜ì—¬ ê°„ë‹¨í•œ ë©”ì‹œì§€ë§Œ ì¶œë ¥"""
    def filter(self, record):
        message = record.getMessage()
        
        # HTML ì—ëŸ¬ í˜ì´ì§€ ê°ì§€ ë° í•„í„°ë§
        if '<!DOCTYPE html>' in message or '<html' in message.lower():
            # HTMLì—ì„œ ì—ëŸ¬ ë©”ì‹œì§€ ì¶”ì¶œ ì‹œë„
            import re
            
            # HTTP ìƒíƒœ ì½”ë“œ ì¶”ì¶œ
            status_match = re.search(r'HTTP (\d{3})', message)
            status_code = status_match.group(1) if status_match else "Unknown"
            
            # ì—ëŸ¬ ì œëª© ì¶”ì¶œ ì‹œë„
            title_match = re.search(r'<title>([^<]+)</title>', message, re.IGNORECASE)
            error_title = title_match.group(1).strip() if title_match else None
            
            # ê°„ë‹¨í•œ ì—ëŸ¬ ë©”ì‹œì§€ ìƒì„±
            if error_title:
                record.msg = f"HTTP {status_code}: {error_title}"
            else:
                # ìƒíƒœ ì½”ë“œì— ë”°ë¥¸ ê¸°ë³¸ ë©”ì‹œì§€
                if status_code == "502":
                    record.msg = f"HTTP {status_code}: Bad Gateway - Server temporarily unavailable"
                elif status_code == "504":
                    record.msg = f"HTTP {status_code}: Gateway Timeout - Server response timeout"
                elif status_code == "503":
                    record.msg = f"HTTP {status_code}: Service Unavailable - Server temporarily unavailable"
                elif status_code == "401":
                    record.msg = f"HTTP {status_code}: Unauthorized - Authentication failed"
                elif status_code == "404":
                    record.msg = f"HTTP {status_code}: Not Found"
                elif status_code == "500":
                    record.msg = f"HTTP {status_code}: Internal Server Error"
                else:
                    record.msg = f"HTTP {status_code}: Server Error"
            
            record.args = ()  # args ì´ˆê¸°í™”
        
        return True

logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

# Remove existing handlers to avoid duplicate logs on reloads (e.g., under some app servers or noteboks)
if logger.hasHandlers():
    logger.handlers.clear()

# File handler
file_handler = logging.FileHandler(log_file, encoding='utf-8')
file_handler.setLevel(logging.INFO)
file_handler.setFormatter(logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s'))
logger.addHandler(file_handler)

# Console handler
console_handler = logging.StreamHandler()
console_handler.setLevel(logging.INFO)
console_handler.setFormatter(logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s'))
console_handler.addFilter(HTTPErrorFilter())  # HTTP ì—ëŸ¬ í•„í„° ì¶”ê°€
logger.addHandler(console_handler)

# ============================================================================
# MCP / Runner / HTTP ê´€ë ¨ ë¡œê±° ì–µì œ (ê³¼ë„í•œ ë¡œê·¸ ì¶œë ¥ ë°©ì§€)
# ============================================================================
# FastMCP Runner ë¡œê±° - WARNING ì´ìƒë§Œ ì¶œë ¥
runner_logger = logging.getLogger("Runner")
runner_logger.setLevel(logging.WARNING)
runner_logger.propagate = False  # ìƒìœ„ë¡œ ì „íŒŒ ì°¨ë‹¨

# MCP ê´€ë ¨ ë¡œê±°ë“¤
logging.getLogger("mcp").setLevel(logging.WARNING)
logging.getLogger("fastmcp").setLevel(logging.WARNING)
logging.getLogger("mcp.client").setLevel(logging.WARNING)
logging.getLogger("mcp.server").setLevel(logging.WARNING)

# HTTP í´ë¼ì´ì–¸íŠ¸ ë¡œê±°ë“¤
logging.getLogger("httpx").setLevel(logging.WARNING)
logging.getLogger("httpcore").setLevel(logging.WARNING)
logging.getLogger("urllib3").setLevel(logging.WARNING)
logging.getLogger("requests").setLevel(logging.WARNING)

# asyncio ê´€ë ¨ ë¡œê±°
logging.getLogger("asyncio").setLevel(logging.WARNING)

# Root loggerì—ë„ í•„í„° ì¶”ê°€
root_logger = logging.getLogger()
for handler in root_logger.handlers:
    if isinstance(handler, logging.StreamHandler) and not any(isinstance(f, HTTPErrorFilter) for f in handler.filters):
        handler.addFilter(HTTPErrorFilter())



class WebAppManager:
    """ì›¹ ì•± ê´€ë¦¬ì - Streaming Pipeline (Innovation 5) ì§€ì›"""
    
    def __init__(self):
        self.project_root = project_root
        self.health_monitor = HealthMonitor()
        
    def start_web_app(self):
        """ì›¹ ì•± ì‹œì‘ - Production-Grade Reliability ì ìš©"""
        try:
            streamlit_app_path = self.project_root / "src" / "web" / "streamlit_app.py"
            
            if not streamlit_app_path.exists():
                logger.error(f"Streamlit app not found at {streamlit_app_path}")
                return False
            
            # Get port from environment variable, default to 8501
            port = os.getenv("STREAMLIT_PORT", "8501")
            address = os.getenv("STREAMLIT_ADDRESS", "0.0.0.0")
            
            logger.info("ğŸŒ Starting Local Researcher Web Application with Streaming Pipeline...")
            logger.info(f"App will be available at: http://{address}:{port}")
            logger.info("Features: Real-time streaming, Progressive reporting, Incremental save")
            logger.info("Press Ctrl+C to stop the application")
            
            # Create logs directory if it doesn't exist
            logs_dir = self.project_root / "logs"
            logs_dir.mkdir(exist_ok=True)
            
            cmd = [
                sys.executable, "-m", "streamlit", "run",
                str(streamlit_app_path),
                "--server.port", port,
                "--server.address", address,
                "--browser.gatherUsageStats", "false",
                "--server.enableCORS", "false",
                "--server.enableXsrfProtection", "false"
            ]
            
            # Start health monitoring
            asyncio.create_task(self.health_monitor.start_monitoring())
            
            subprocess.run(cmd, cwd=str(self.project_root))
            return True
            
        except KeyboardInterrupt:
            logger.info("Application stopped by user")
            return True
        except Exception as e:
            logger.error(f"Error running web application: {e}")
            return False
    
    async def get_web_app_health(self) -> Dict[str, Any]:
        """Get web application health status."""
        port = int(os.getenv("STREAMLIT_PORT", "8501"))
        return {
            'status': 'running',
            'port': port,
            'streaming_enabled': True,
            'progressive_reporting': True,
            'incremental_save': True,
            'timestamp': datetime.now().isoformat()
        }


class AutonomousResearchSystem:
    """ììœ¨ ë¦¬ì„œì²˜ ì‹œìŠ¤í…œ - 9ê°€ì§€ í•µì‹¬ í˜ì‹  í†µí•© ë©”ì¸ í´ë˜ìŠ¤"""
    
    def __init__(self):
        # Load configurations from environment - ALL REQUIRED, NO DEFAULTS
        try:
            self.config = load_config_from_env()
            logger.info("âœ… Configuration loaded successfully from environment variables")
            
            # Validate ChromaDB availability (optional)
            try:
                import chromadb  # type: ignore
                logger.info("âœ… ChromaDB module available")
            except ImportError:
                logger.warning("âš ï¸ ChromaDB not installed - vector search will be disabled")
                logger.info("   Install with: pip install chromadb")
            
        except ValueError as e:
            logger.error(f"âŒ Configuration loading failed: {e}")
            logger.error("Please check your .env file and ensure all required variables are set")
            logger.info("\nRequired environment variables:")
            logger.info("  - LLM_MODEL: LLM model identifier (e.g., google/gemini-2.5-flash-lite)")
            logger.info("  - GOOGLE_API_KEY: Your Google or Vertex AI API key")
            logger.info("  - LLM_PROVIDER: Provider name (e.g., google)")
            raise
        
        # Initialize components with 8 innovations
        logger.info("ğŸ”§ Initializing system components...")
        try:
            # Use new multi-agent orchestrator (no fallback - fail clearly)
            self.orchestrator = NewAgentOrchestrator()
            logger.info("âœ… Multi-Agent Orchestrator initialized (no fallback mode)")
            logger.info("âœ… Autonomous Orchestrator initialized")
        except Exception as e:
            logger.error(f"âŒ Orchestrator initialization failed: {e}")
            raise
        
        try:
            from src.core.mcp_integration import UniversalMCPHub
            self.mcp_hub = UniversalMCPHub()
            logger.info("âœ… MCP Hub initialized")
        except Exception as e:
            logger.error(f"âŒ MCP Hub initialization failed: {e}")
            raise
        
        # ìƒˆë¡œìš´ ê¸°ëŠ¥ ëª¨ë“ˆ ê¸°ë³¸ í™œì„±í™” (ëª¨ë“  ê¸°ëŠ¥ ê¸°ë³¸ ON)
        try:
            from src.core.feature_flags import FeatureFlags
            FeatureFlags.log_status()
            
            # MCP ì•ˆì •ì„± ì„œë¹„ìŠ¤ (ê¸°ë³¸ í™œì„±í™”)
            if FeatureFlags.ENABLE_MCP_STABILITY:
                from src.core.mcp_stability_service import MCPStabilityService
                self.mcp_stability_service = MCPStabilityService()
                logger.info("âœ… MCP Stability Service enabled (default)")
            else:
                self.mcp_stability_service = None
                logger.info("âš ï¸ MCP Stability Service disabled (via DISABLE_MCP_STABILITY)")
            
            # MCP ë°±ê·¸ë¼ìš´ë“œ í—¬ìŠ¤ì²´í¬ (ê¸°ë³¸ í™œì„±í™”)
            if FeatureFlags.ENABLE_MCP_HEALTH_BACKGROUND:
                from src.core.mcp_health_background import MCPHealthBackgroundService
                self.mcp_health_service = MCPHealthBackgroundService(self.mcp_hub, interval=60)
                logger.info("âœ… MCP Health Background Service enabled (default, will start on first execution)")
            else:
                self.mcp_health_service = None
                logger.info("âš ï¸ MCP Health Background Service disabled (via DISABLE_MCP_HEALTH_BACKGROUND)")
            
            # Guardrails ê²€ì¦ (ê¸°ë³¸ í™œì„±í™”)
            if FeatureFlags.ENABLE_GUARDRAILS:
                from src.core.guardrails_validator import GuardrailsValidator
                self.guardrails_validator = GuardrailsValidator()
                logger.info("âœ… Guardrails Validator enabled (default)")
            else:
                self.guardrails_validator = None
                logger.info("âš ï¸ Guardrails Validator disabled (via DISABLE_GUARDRAILS)")
            
            # Agent Tool Wrapper (ê¸°ë³¸ í™œì„±í™”)
            if FeatureFlags.ENABLE_AGENT_TOOLS:
                from src.core.agent_tool_wrapper import AgentToolWrapper
                # ì—ì´ì „íŠ¸ëŠ” ë‚˜ì¤‘ì— í• ë‹¹ (execute ì‹œì )
                self.agent_tool_wrapper = None
                logger.info("âœ… Agent Tool Wrapper enabled (default, will be initialized on first execution)")
            else:
                self.agent_tool_wrapper = None
                logger.info("âš ï¸ Agent Tool Wrapper disabled (via DISABLE_AGENT_TOOLS)")
            
            # YAML ì„¤ì • ë¡œë” (ê¸°ë³¸ í™œì„±í™”)
            if FeatureFlags.ENABLE_YAML_CONFIG:
                from src.core.yaml_config_loader import YAMLConfigLoader
                self.yaml_config_loader = YAMLConfigLoader()
                logger.info("âœ… YAML Config Loader enabled (default)")
            else:
                self.yaml_config_loader = None
                logger.info("âš ï¸ YAML Config Loader disabled (via DISABLE_YAML_CONFIG)")
        except Exception as e:
            logger.warning(f"âš ï¸ Feature initialization failed: {e} - continuing with core features only")
            # ê¸°ë³¸ê°’: ëª¨ë“  ê¸°ëŠ¥ None (ì—ëŸ¬ ë°œìƒ ì‹œ)
            self.mcp_stability_service = None
            self.mcp_health_service = None
            self.guardrails_validator = None
            self.agent_tool_wrapper = None
            self.yaml_config_loader = None
        
        try:
            self.web_manager = WebAppManager()
            logger.info("âœ… Web Manager initialized")
        except Exception as e:
            logger.error(f"âŒ Web Manager initialization failed: {e}")
            raise
        
        try:
            self.health_monitor = HealthMonitor()
            logger.info("âœ… Health Monitor initialized")
        except Exception as e:
            logger.error(f"âŒ Health Monitor initialization failed: {e}")
            raise
        
        # ProcessManager ì´ˆê¸°í™” (í”„ë¡œì„¸ìŠ¤ ì¶”ì  ë° ì¢…ë£Œ ê´€ë¦¬)
        try:
            from src.core.process_manager import get_process_manager
            pm = get_process_manager()
            # ProcessManagerì˜ ì‹œê·¸ë„ í•¸ë“¤ëŸ¬ëŠ” ì„¤ì¹˜í•˜ì§€ ì•ŠìŒ (main.pyì˜ í•¸ë“¤ëŸ¬ ì‚¬ìš©)
            # pm.initialize()ëŠ” í˜¸ì¶œí•˜ì§€ ì•ŠìŒ - ì‹œê·¸ë„ í•¸ë“¤ëŸ¬ ì¶©ëŒ ë°©ì§€
            logger.info("âœ… ProcessManager initialized (signal handlers managed by main.py)")
        except Exception as e:
            logger.warning(f"âš ï¸ ProcessManager initialization failed: {e} - continuing without process tracking")
        
        # Initialize signal handlers for graceful shutdown
        signal.signal(signal.SIGINT, self._signal_handler)
        signal.signal(signal.SIGTERM, self._signal_handler)
        
        # Shutdown flag ì´ˆê¸°í™”
        self._shutdown_requested = False
        
        logger.info("âœ… AutonomousResearchSystem initialized successfully")
        
    def _signal_handler(self, signum, frame):
        """Handle shutdown signals gracefully."""
        import sys
        import os
        
        # Reentrant call ë°©ì§€ë¥¼ ìœ„í•´ os.write ì‚¬ìš© (print ëŒ€ì‹ )
        try:
            msg = f"Received signal {signum}, initiating graceful shutdown...\n"
            os.write(sys.stderr.fileno(), msg.encode('utf-8'))
        except (OSError, AttributeError):
            # stderrì— ì“¸ ìˆ˜ ì—†ìœ¼ë©´ ë¬´ì‹œ (ì´ë¯¸ ì¢…ë£Œ ì¤‘ì¼ ìˆ˜ ìˆìŒ)
            pass
        
        # Shutdown í”Œë˜ê·¸ ì„¤ì • (ì¤‘ë³µ ë°©ì§€)
        if hasattr(self, '_shutdown_requested') and self._shutdown_requested:
            # ì´ë¯¸ ì¢…ë£Œ ì¤‘ì´ë©´ ì¬ì§„ì… ë°©ì§€ë§Œ ìˆ˜í–‰
            try:
                msg = "Shutdown already in progress; ignoring additional signal\n"
                os.write(sys.stderr.fileno(), msg.encode('utf-8'))
            except (OSError, AttributeError):
                pass
            return
        
        self._shutdown_requested = True
        
        # ì‹¤í–‰ ì¤‘ì¸ ì´ë²¤íŠ¸ ë£¨í”„ê°€ ìˆëŠ”ì§€ í™•ì¸í•˜ê³  shutdown ì‘ì—… ìŠ¤ì¼€ì¤„ë§
        try:
            loop = asyncio.get_running_loop()
            # ì¤‘ë³µ ìƒì„± ë°©ì§€: ì´ë¯¸ ìŠ¤ì¼€ì¤„ëœ ì‘ì—…ì´ ìˆìœ¼ë©´ ì¬ìƒì„±í•˜ì§€ ì•ŠìŒ
            if not hasattr(self, '_shutdown_task') or self._shutdown_task is None or self._shutdown_task.done():
                def _schedule():
                    self._shutdown_task = asyncio.create_task(self._graceful_shutdown())
                loop.call_soon_threadsafe(_schedule)
            else:
                # loggerëŠ” ì‹œê·¸ë„ í•¸ë“¤ëŸ¬ì—ì„œ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ (reentrant call ìœ„í—˜)
                pass
        except RuntimeError:
            # ì´ë²¤íŠ¸ ë£¨í”„ê°€ ì—†ìœ¼ë©´ ê°•ì œ ì¢…ë£Œ
            try:
                msg = "No event loop available, forcing exit\n"
                os.write(sys.stderr.fileno(), msg.encode('utf-8'))
            except (OSError, AttributeError):
                pass
            os._exit(1)  # sys.exit ëŒ€ì‹  os._exit ì‚¬ìš© (ë” ì•ˆì „)
    
    async def _graceful_shutdown(self):
        """Graceful shutdown with state persistence."""
        import sys
        import signal
        
        try:
            logger.info("Performing graceful shutdown...")
            
            # ProcessManagerë¥¼ í†µí•œ í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ (ìš°ì„  ì²˜ë¦¬)
            try:
                from src.core.process_manager import get_process_manager
                pm = get_process_manager()
                pm.abort()  # ëª¨ë“  ì‘ì—… ì¤‘ë‹¨ í”Œë˜ê·¸ ì„¤ì •
                killed = pm.kill_all()  # ëª¨ë“  ë“±ë¡ëœ í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ
                if killed > 0:
                    logger.info(f"Killed {killed} registered processes")
            except Exception as e:
                logger.debug(f"Error killing processes: {e}")
            
            # MCP Hub cleanup (íƒ€ì„ì•„ì›ƒ ë‹¨ì¶•: 10ì´ˆ -> 3ì´ˆ)
            if self.config.mcp.enabled and self.mcp_hub:
                try:
                    # ì‹ ê·œ ì—°ê²° ì°¨ë‹¨ (ì¦‰ì‹œ)
                    if hasattr(self.mcp_hub, 'start_shutdown'):
                        self.mcp_hub.start_shutdown()
                    # cleanupì€ CancelledErrorë¥¼ ë°œìƒì‹œí‚¬ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ë¬´ì‹œ
                    try:
                        await asyncio.wait_for(self.mcp_hub.cleanup(), timeout=3.0)  # íƒ€ì„ì•„ì›ƒ ë‹¨ì¶•
                    except asyncio.CancelledError:
                        logger.debug("MCP Hub cleanup was cancelled (normal during shutdown)")
                    except asyncio.TimeoutError:
                        logger.warning("MCP Hub cleanup timed out (continuing shutdown)")
                except asyncio.CancelledError:
                    logger.debug("MCP Hub cleanup setup was cancelled (normal during shutdown)")
                except Exception as e:
                    logger.warning(f"Error cleaning up MCP Hub: {e}")
            
            # Health monitor ì •ì§€ (íƒ€ì„ì•„ì›ƒ ë‹¨ì¶•: 5ì´ˆ -> 2ì´ˆ)
            try:
                await asyncio.wait_for(self.health_monitor.stop_monitoring(), timeout=2.0)
            except asyncio.TimeoutError:
                logger.warning("Health monitor stop timed out (continuing shutdown)")
            except Exception as e:
                logger.debug(f"Error stopping health monitor: {e}")
            
            logger.info("âœ… Graceful shutdown completed")
            
        except Exception as e:
            logger.error(f"Error during graceful shutdown: {e}")
        finally:
            # ìµœì¢… ì¢…ë£Œ ì¤€ë¹„
            logger.info("Exiting...")
            # ì™¸ë¶€ ë¼ì´ë¸ŒëŸ¬ë¦¬ íƒœìŠ¤í¬ëŠ” ê°œë³„ ë§¤ë‹ˆì €ê°€ ì •ë¦¬í•¨. ì¼ê´„ ì·¨ì†ŒëŠ” í•˜ì§€ ì•ŠìŒ
            
            # sys.exit(0)ì€ í˜¸ì¶œí•˜ì§€ ì•ŠìŒ - asyncio.run()ì´ ìë™ìœ¼ë¡œ ì²˜ë¦¬
            # ëŒ€ì‹  ë£¨í”„ì—ì„œ ë‚˜ê°€ë„ë¡ í•¨
    
    def _detect_output_format_from_content(self, content: str, request: str) -> str:
        """ìƒì„±ëœ ë‚´ìš©ì„ ë³´ê³  íŒŒì¼ í˜•ì‹ ê²°ì • (ìµœì†Œí•œì˜ íŒ¨í„´ ë§¤ì¹­ë§Œ)."""
        if not content:
            return "md"
        
        # ì½”ë“œ ë¸”ë¡ ì œê±° í›„ ë‚´ìš© í™•ì¸
        content_clean = content.replace("```", "").strip()
        
        # Python ì½”ë“œ íŒ¨í„´
        if ("def " in content_clean[:1000] and "import " in content_clean[:1000]) or content.startswith("```python"):
            return "py"
        
        # Java ì½”ë“œ íŒ¨í„´
        if ("public class" in content_clean[:1000] or "public static void" in content_clean[:1000]) or content.startswith("```java"):
            return "java"
        
        # JavaScript ì½”ë“œ íŒ¨í„´
        if (("const " in content_clean[:1000] or "function " in content_clean[:1000]) and "console" in content_clean[:1000]) or content.startswith("```javascript"):
            return "js"
        
        # HTML íŒ¨í„´
        if content.strip().startswith("<!DOCTYPE") or content.strip().startswith("<html"):
            return "html"
        
        # ê¸°ë³¸: Markdown
        return "md"
    
    async def run_research(self, request: str, output_path: Optional[str] = None, 
                          streaming: bool = False, output_format: Optional[str] = None) -> Dict[str, Any]:
        """ì—°êµ¬ ì‹¤í–‰ - 9ê°€ì§€ í•µì‹¬ í˜ì‹  ì ìš©"""
        logger.info("ğŸ¤– Starting Autonomous Research System with 9 Core Innovations")
        logger.info("=" * 80)
        logger.info(f"Request: {request}")
        logger.info(f"Primary LLM: {self.config.llm.primary_model}")
        logger.info(f"Planning Model: {self.config.llm.planning_model}")
        logger.info(f"Reasoning Model: {self.config.llm.reasoning_model}")
        logger.info(f"Verification Model: {self.config.llm.verification_model}")
        logger.info(f"Self-planning: {self.config.agent.enable_self_planning}")
        logger.info(f"Agent Communication: {self.config.agent.enable_agent_communication}")
        logger.info(f"MCP Enabled: {self.config.mcp.enabled}")
        logger.info(f"Streaming Pipeline: {streaming}")
        logger.info(f"Adaptive Supervisor: {self.config.agent.max_concurrent_research_units}")
        logger.info(f"Hierarchical Compression: {self.config.compression.enabled}")
        logger.info(f"Continuous Verification: {self.config.verification.enabled}")
        logger.info(f"Adaptive Context Window: {self.config.context_window.enabled}")
        logger.info("=" * 80)
        
        try:
            # Start health monitoring
            await self.health_monitor.start_monitoring()
            
            # Initialize MCP client if enabled
            if self.config.mcp.enabled:
                try:
                    await self.mcp_hub.initialize_mcp()
                    
                    # MCP ë°±ê·¸ë¼ìš´ë“œ í—¬ìŠ¤ì²´í¬ ì‹œì‘ (ì„ íƒì )
                    if hasattr(self, 'mcp_health_service') and self.mcp_health_service:
                        try:
                            await self.mcp_health_service.start()
                            logger.info("âœ… MCP Health Background Service started")
                        except Exception as e:
                            logger.warning(f"âš ï¸ Failed to start MCP health service: {e}")
                except asyncio.CancelledError:
                    # ì´ˆê¸°í™” ì¤‘ ì·¨ì†Œëœ ê²½ìš° - ìƒìœ„ë¡œ ì „íŒŒí•˜ì—¬ ì¢…ë£Œ
                    logger.warning("MCP initialization was cancelled")
                    raise
            
            # Run research with production-grade reliability
            if streaming:
                result = await self._run_streaming_research(request)
            else:
                # Guardrails Input ê²€ì¦ (ì„ íƒì )
                if hasattr(self, 'guardrails_validator') and self.guardrails_validator:
                    try:
                        from src.core.agent_orchestrator import AgentState
                        initial_state: AgentState = {
                            'messages': [],
                            'user_query': request,
                            'research_plan': None,
                            'research_tasks': [],
                            'research_results': [],
                            'verified_results': [],
                            'final_report': None,
                            'current_agent': None,
                            'iteration': 0,
                            'session_id': None,
                            'research_failed': False,
                            'verification_failed': False,
                            'report_failed': False,
                            'error': None
                        }
                        validated_state = await self.guardrails_validator.validate_input(initial_state)
                        # ê²€ì¦ í†µê³¼ ì‹œ request ê·¸ëŒ€ë¡œ ì‚¬ìš© (ê¸°ì¡´ ì½”ë“œ ìˆ˜ì • ì—†ìŒ)
                        request = validated_state.get('user_query', request)
                    except ValueError as e:
                        logger.error(f"âŒ Input validation failed: {e}")
                        return {
                            'success': False,
                            'error': str(e),
                            'final_report': f"Input validation failed: {str(e)}"
                        }
                    except Exception as e:
                        logger.warning(f"âš ï¸ Guardrails validation error: {e} - continuing without validation")
                
                # Use new multi-agent orchestrator (no fallback - fail clearly)
                workflow_result = await self.orchestrator.execute(request)
                
                # Guardrails Output ê²€ì¦ (ì„ íƒì )
                if hasattr(self, 'guardrails_validator') and self.guardrails_validator:
                    try:
                        validated_result = await self.guardrails_validator.validate_output(workflow_result)
                        workflow_result = validated_result
                    except ValueError as e:
                        logger.error(f"âŒ Output validation failed: {e}")
                        workflow_result['error'] = str(e)
                        workflow_result['final_report'] = f"Output validation failed: {str(e)}"
                    except Exception as e:
                        logger.warning(f"âš ï¸ Guardrails validation error: {e} - continuing without validation")
                
                # ì‹¤íŒ¨ ìƒíƒœ í™•ì¸ - fallback ì—†ì´ ëª…í™•í•œ ì˜¤ë¥˜ ë°˜í™˜
                research_failed = workflow_result.get('research_failed', False)
                verification_failed = workflow_result.get('verification_failed', False)
                report_failed = workflow_result.get('report_failed', False)
                final_report = workflow_result.get("final_report", "")
                
                # ì‹¤íŒ¨ ë³´ê³ ì„œë„ ì‹¤íŒ¨ë¡œ ì²˜ë¦¬ (ë‚´ìš©ì´ "ì—°êµ¬ ì‹¤íŒ¨" ë˜ëŠ” "ì—°êµ¬ ì™„ë£Œ ë¶ˆê°€" í¬í•¨)
                is_failure_report = (
                    final_report and 
                    ("ì—°êµ¬ ì‹¤íŒ¨" in final_report or "ì—°êµ¬ ì™„ë£Œ ë¶ˆê°€" in final_report or "âŒ" in final_report)
                )
                
                if research_failed or verification_failed or report_failed or is_failure_report:
                    error_msg = workflow_result.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')
                    if not error_msg or error_msg == 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜':
                        # ì‹¤íŒ¨ ë³´ê³ ì„œì—ì„œ ì˜¤ë¥˜ ë©”ì‹œì§€ ì¶”ì¶œ
                        if final_report and "ì˜¤ë¥˜ ë‚´ìš©" in final_report:
                            lines = final_report.split("\n")
                            for i, line in enumerate(lines):
                                if "ì˜¤ë¥˜ ë‚´ìš©" in line and i + 1 < len(lines):
                                    error_msg = lines[i + 1].strip()
                                    break
                        elif final_report and "âŒ" in final_report:
                            # ì‹¤íŒ¨ ë³´ê³ ì„œì—ì„œ ê°„ë‹¨íˆ ì¶”ì¶œ
                            if "ì—°êµ¬ ì‹¤í–‰ ì‹¤íŒ¨" in final_report:
                                error_msg = "ì—°êµ¬ ì‹¤í–‰ì´ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤"
                            elif "ë³´ê³ ì„œ ìƒì„± ì‹¤íŒ¨" in final_report:
                                error_msg = "ë³´ê³ ì„œ ìƒì„±ì´ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤"
                            else:
                                error_msg = "ì—°êµ¬ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤"
                    
                    failed_agent = workflow_result.get('current_agent', 'unknown')
                    session_id = workflow_result.get("session_id", 'N/A')
                    
                    logger.error(f"âŒ Research failed at {failed_agent}: {error_msg}")
                    
                    # ì‹¤íŒ¨ ê²°ê³¼ ë°˜í™˜ (ì‚¬ìš©ìê°€ ì¬ì‹œë„í•  ìˆ˜ ìˆë„ë¡)
                    result = {
                        "success": False,
                        "query": request,
                        "error": error_msg,
                        "failed_agent": failed_agent,
                        "session_id": session_id,
                        "content": final_report or f"ì—°êµ¬ ì‹¤íŒ¨: {error_msg}",
                        "timestamp": datetime.now().isoformat(),
                        "metadata": {
                            "model_used": "multi-agent",
                            "execution_time": 0.0,
                            "cost": 0.0,
                            "confidence": 0.0,
                            "failed": True
                        },
                        "synthesis_results": {
                            "content": final_report or "",
                            "failed": True
                        },
                        "sources": [],
                        "innovation_stats": {"multi_agent_orchestration": "enabled"},
                        "system_health": {"overall_status": "unhealthy", "error": error_msg},
                        "retry_available": True
                    }
                    
                    logger.warning("âš ï¸ Research completed with errors - user can retry")
                else:
                    # ì„±ê³µ ê²°ê³¼
                    final_report = workflow_result.get("final_report", "")
                    if not final_report:
                        # ë³´ê³ ì„œê°€ ì—†ìœ¼ë©´ ì‹¤íŒ¨ë¡œ ì²˜ë¦¬
                        result = {
                            "success": False,
                            "query": request,
                            "error": "ë³´ê³ ì„œ ìƒì„± ì‹¤íŒ¨: ìµœì¢… ë³´ê³ ì„œê°€ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤",
                            "failed_agent": "generator",
                            "session_id": workflow_result.get("session_id", "N/A"),
                            "content": "ì—°êµ¬ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
                            "timestamp": datetime.now().isoformat(),
                            "metadata": {"failed": True, "confidence": 0.0},
                            "retry_available": True
                        }
                    else:
                        result = {
                            "success": True,
                            "query": request,
                            "content": final_report,
                            "timestamp": datetime.now().isoformat(),
                            "metadata": {
                                "model_used": "multi-agent",
                                "execution_time": 0.0,
                                "cost": 0.0,
                                "confidence": 0.9
                            },
                            "synthesis_results": {
                                "content": final_report
                            },
                            "sources": self._extract_sources_from_workflow(workflow_result),
                            "innovation_stats": {"multi_agent_orchestration": "enabled"},
                            "system_health": {"overall_status": "healthy"},
                            "session_id": workflow_result.get("session_id")
                        }
            
            # Apply hierarchical compression if enabled
            # Commented out to avoid serialization errors
            # if self.config.compression.enabled:
            #     result = await self._apply_hierarchical_compression(result)
            
            # Save results - LLMì´ ìƒì„±í•œ ë‚´ìš©ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©
            content = result.get('content', '') or result.get('synthesis_results', {}).get('content', '')
            
            if output_path:
                # ì‚¬ìš©ìê°€ ì§€ì •í•œ ê²½ë¡œ ì‚¬ìš©
                final_path = await self._save_content_as_file(content, output_path, result)
                logger.info(f"ğŸ“„ Results saved to: {final_path}")
            else:
                # ìƒì„±ëœ ë‚´ìš©ì„ ë³´ê³  í˜•ì‹ ê²°ì •
                detected_format = self._detect_output_format_from_content(content, request)
                if output_format is None:
                    output_format = detected_format
                
                output_dir = project_root / "output"
                output_dir.mkdir(exist_ok=True)
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                
                # í™•ì¥ì ê²°ì • (ê°„ë‹¨í•˜ê²Œ)
                if output_format.startswith("."):
                    ext = output_format
                else:
                    ext_map = {"py": ".py", "java": ".java", "js": ".js", "html": ".html", "md": ".md", "pdf": ".pdf"}
                    ext = ext_map.get(output_format, ".md")
                default_output = output_dir / f"research_{timestamp}{ext}"
                
                final_path = await self._save_content_as_file(content, str(default_output), result)
                logger.info(f"ğŸ“„ Results saved to default location: {final_path} (format: {output_format})")
                self._display_results(result)
            
            # Get final health status
            health_status = self.health_monitor.get_system_health()
            result['system_health'] = health_status
            
            logger.info("âœ… Research completed successfully with 9 Core Innovations")
            return result
            
        except Exception as e:
            logger.error(f"Research failed with exception: {e}")
            import traceback
            logger.error(traceback.format_exc())
            
            # Get error health status
            error_health = self.health_monitor.get_system_health()
            logger.error(f"System health at failure: {error_health}")
            
            # ì‹¤íŒ¨ ê²°ê³¼ ë°˜í™˜ (ì˜ˆì™¸ ëŒ€ì‹ )
            result = {
                "success": False,
                "query": request,
                "error": f"ì‹œìŠ¤í…œ ì˜¤ë¥˜: {str(e)}",
                "failed_agent": "system",
                "content": f"ì—°êµ¬ ì‹¤í–‰ ì¤‘ ì‹œìŠ¤í…œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                "timestamp": datetime.now().isoformat(),
                "metadata": {
                    "model_used": "multi-agent",
                    "execution_time": 0.0,
                    "cost": 0.0,
                    "confidence": 0.0,
                    "failed": True
                },
                "synthesis_results": {"content": "", "failed": True},
                "sources": [],
                "innovation_stats": {},
                "system_health": error_health,
                "retry_available": True
            }
            
            # ì‹¤íŒ¨ ê²°ê³¼ë„ ì €ì¥
            if output_path:
                await self._save_results_incrementally(result, output_path, output_format)
            else:
                output_dir = project_root / "output"
                output_dir.mkdir(exist_ok=True)
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                default_output = output_dir / f"research_failed_{timestamp}.{output_format}"
                await self._save_results_incrementally(result, str(default_output), output_format)
                self._display_results(result)
            
            # ì˜ˆì™¸ë¥¼ ë°œìƒì‹œí‚¤ì§€ ì•Šê³  ê²°ê³¼ ë°˜í™˜
            return result
    
    async def _run_streaming_research(self, request: str) -> Dict[str, Any]:
        """Run research with streaming pipeline (Innovation 5)."""
        logger.info("ğŸŒŠ Starting streaming research pipeline...")
        
        # Create streaming callback
        async def streaming_callback(partial_result: Dict[str, Any]):
            logger.info(f"ğŸ“Š Streaming partial result: {partial_result.get('type', 'unknown')}")
            # In a real implementation, this would send to web interface
            print(f"ğŸ“Š Partial Result: {partial_result.get('summary', 'Processing...')}")
        
        # Use standard run_research but with streaming callback simulation
        result = await self.orchestrator.run_research(user_request=request, context={})
        
        # Extract and format result
        final_synthesis = result.get("synthesis_results", {}).get("content", "")
        if not final_synthesis:
            final_synthesis = result.get("content", "")
        
        formatted_result = {
            "query": request,
            "content": final_synthesis or "Research completed",
            "timestamp": datetime.now().isoformat(),
            "metadata": result.get("metadata", {}),
            "synthesis_results": result.get("synthesis_results", {}),
            "sources": self._extract_sources(result),
            "innovation_stats": result.get("innovation_stats", {}),
            "system_health": result.get("system_health", {})
        }
        
        logger.info("âœ… Streaming research completed")
        return formatted_result
    
    def _extract_sources_from_workflow(self, workflow_result: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Extract sources from workflow results."""
        sources = []
        seen_urls = set()
        
        # Extract from verified results (ìš°ì„ )
        verified_results = workflow_result.get("verified_results", [])
        for result in verified_results:
            if isinstance(result, dict):
                url = result.get('url', '')
                if url and url not in seen_urls:
                    seen_urls.add(url)
                    sources.append({
                        "title": result.get('title', ''),
                        "url": url,
                        "snippet": result.get('snippet', '')
                    })
        
        # Extract from research results (ë°±ì—…)
        research_results = workflow_result.get("research_results", [])
        for result in research_results:
            if isinstance(result, dict):
                url = result.get('url', '')
                if url and url not in seen_urls:
                    seen_urls.add(url)
                    sources.append({
                        "title": result.get('title', ''),
                        "url": url,
                        "snippet": result.get('snippet', '')
                    })
            elif isinstance(result, str) and "Source:" in result:
                # Extract URL from result string (ë ˆê±°ì‹œ)
                parts = result.split("Source:")
                if len(parts) > 1:
                    url = parts[1].strip()
                    if url and url not in seen_urls:
                        seen_urls.add(url)
                        title = parts[0].split(":")[-1].strip() if ":" in parts[0] else ""
                        sources.append({
                            "title": title,
                            "url": url,
                            "snippet": ""
                        })
        
        return sources[:20]  # Limit to 20 sources
    
    def _extract_sources(self, result: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Extract sources from research results (legacy method)."""
        sources = []
        
        # Try to extract from execution results
        execution_results = result.get("detailed_results", {}).get("execution_results", [])
        for exec_result in execution_results:
            if isinstance(exec_result, dict):
                # Look for search results
                if "results" in exec_result:
                    sources.extend(exec_result["results"])
                elif "sources" in exec_result:
                    sources.extend(exec_result["sources"])
                elif "url" in exec_result:
                    sources.append(exec_result)
        
        # Deduplicate by URL
        seen_urls = set()
        unique_sources = []
        for source in sources:
            url = source.get("url") or source.get("link")
            if url and url not in seen_urls:
                seen_urls.add(url)
                unique_sources.append({
                    "title": source.get("title", source.get("name", "")),
                    "url": url,
                    "snippet": source.get("snippet", source.get("summary", ""))
                })
        
        return unique_sources[:20]  # Limit to 20 sources
    
    async def _apply_hierarchical_compression(self, result: Dict[str, Any]) -> Dict[str, Any]:
        """Apply hierarchical compression (Innovation 2)."""
        if not self.config.compression.enabled:
            return result
        
        logger.info("ğŸ—œï¸ Applying hierarchical compression...")
        
        # Import compression module
        from src.core.compression import compress_data
        
        # Compress large text fields
        if 'synthesis_results' in result:
            compressed_synthesis = await compress_data(
                result['synthesis_results']
            )
            result['synthesis_results_compressed'] = compressed_synthesis
        
        logger.info("âœ… Hierarchical compression applied")
        return result
    
    async def _save_content_as_file(self, content: str, output_path: str, result: Dict[str, Any]) -> str:
        """LLMì´ ìƒì„±í•œ ë‚´ìš©ì„ ê·¸ëŒ€ë¡œ íŒŒì¼ë¡œ ì €ì¥ (í•˜ë“œì½”ë”© ì—†ì´)."""
        output_file = Path(output_path)
        output_file.parent.mkdir(parents=True, exist_ok=True)
        
        # LLMì´ ìƒì„±í•œ ë‚´ìš©ì„ ê·¸ëŒ€ë¡œ ì €ì¥ (ì¶”ê°€ í…œí”Œë¦¿ ì—†ì´)
        # ì†ŒìŠ¤ ì •ë³´ê°€ ìˆìœ¼ë©´ ë§ˆì§€ë§‰ì— ì¶”ê°€
        if result.get('sources'):
            sources_text = "\n\n## ì°¸ê³  ë¬¸í—Œ\n\n"
            for i, source in enumerate(result.get('sources', []), 1):
                sources_text += f"{i}. [{source.get('title', 'N/A')}]({source.get('url', '')})\n"
                if source.get('snippet'):
                    sources_text += f"   {source.get('snippet', '')[:200]}...\n"
                sources_text += "\n"
            content = content + sources_text
        
        output_file.write_text(content, encoding='utf-8')
        return str(output_file)
    
    async def _save_results_incrementally(self, result: Dict[str, Any], output_path: str, output_format: str = "json"):
        """Save results with incremental save (Innovation 5)."""
        output_file = Path(output_path)
        output_file.parent.mkdir(parents=True, exist_ok=True)
        
        # Save incrementally based on format
        temp_file = output_file.with_suffix('.tmp')
        
        if output_format.lower() == "json":
            # Custom JSON encoder for datetime objects
            class DateTimeEncoder(json.JSONEncoder):
                def default(self, obj):
                    if isinstance(obj, datetime):
                        return obj.isoformat()
                    elif hasattr(obj, 'value'):  # Enum ì²˜ë¦¬
                        return obj.value
                    elif hasattr(obj, '__dict__'):  # ê°ì²´ì˜ ê²½ìš°
                        return str(obj)
                    return super().default(obj)
            
            with open(temp_file, 'w', encoding='utf-8') as f:
                json.dump(result, f, indent=2, ensure_ascii=False, cls=DateTimeEncoder)
        elif output_format.lower() == "yaml":
            import yaml
            with open(temp_file, 'w', encoding='utf-8') as f:
                yaml.dump(result, f, default_flow_style=False, allow_unicode=True)
        elif output_format.lower() == "txt":
            with open(temp_file, 'w', encoding='utf-8') as f:
                f.write(f"Research Results\n")
                f.write(f"===============\n\n")
                f.write(f"Query: {result.get('query', 'N/A')}\n")
                f.write(f"Timestamp: {result.get('timestamp', 'N/A')}\n\n")
                if 'content' in result:
                    f.write(f"Content:\n{result['content']}\n\n")
                elif 'synthesis_results' in result:
                    synthesis = result['synthesis_results']
                    if isinstance(synthesis, dict) and 'content' in synthesis:
                        f.write(f"Content:\n{synthesis['content']}\n\n")
                if 'sources' in result:
                    f.write(f"Sources:\n")
                    for i, source in enumerate(result['sources'], 1):
                        f.write(f"{i}. {source.get('title', 'N/A')} - {source.get('url', 'N/A')}\n")
        else:
            raise ValueError(f"Unsupported output format: {output_format}")
        
        # Atomic move
        temp_file.replace(output_file)
        
        logger.info(f"âœ… Results saved incrementally to: {output_file} (format: {output_format})")
    
    def _display_results(self, result: Dict[str, Any]):
        """Display results with enhanced formatting."""
        print("\nğŸ“‹ Research Results with 9 Core Innovations:")
        print("=" * 80)
        
        # ì‹¤íŒ¨ ìƒíƒœ í™•ì¸ ë° í‘œì‹œ
        if not result.get('success', True):
            print("\nâŒ ì—°êµ¬ ì‹¤í–‰ ì‹¤íŒ¨")
            print("=" * 80)
            print(f"\nì˜¤ë¥˜: {result.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}")
            print(f"ì‹¤íŒ¨ ë‹¨ê³„: {result.get('failed_agent', 'unknown')}")
            print(f"\nì„¸ì…˜ ID: {result.get('session_id', 'N/A')}")
            print("\nì¬ì‹œë„ ë°©ë²•:")
            print("1. ê°™ì€ ì¿¼ë¦¬ë¡œ ë‹¤ì‹œ ì‹œë„: python main.py --request 'YOUR_QUERY'")
            print("2. ë‹¤ë¥¸ ê²€ìƒ‰ì–´ë¡œ ì‹œë„")
            print("3. ë„¤íŠ¸ì›Œí¬ ì—°ê²° í™•ì¸")
            print("=" * 80)
            return
        
        # Display main research content
        if 'content' in result and result['content']:
            print("\nğŸ“ Research Content:")
            print("-" * 60)
            print(result['content'])
            print("-" * 60)
        elif 'synthesis_results' in result:
            synthesis = result['synthesis_results']
            if isinstance(synthesis, dict) and 'content' in synthesis:
                print("\nğŸ“ Research Content:")
                print("-" * 60)
                print(synthesis['content'])
                print("-" * 60)
            else:
                print(f"\nğŸ“ Synthesis: {synthesis}")
        else:
            print("\nâŒ No research content found in results")
        
        # Display research metadata
        if 'metadata' in result:
            metadata = result['metadata']
            print(f"\nğŸ“Š Research Metadata:")
            print(f"  â€¢ Model Used: {metadata.get('model_used', 'N/A')}")
            print(f"  â€¢ Execution Time: {metadata.get('execution_time', 'N/A'):.2f}s")
            print(f"  â€¢ Cost: ${metadata.get('cost', 0):.4f}")
            print(f"  â€¢ Confidence: {metadata.get('confidence', 'N/A')}")
        
        # Display synthesis results
        if 'synthesis_results' in result and isinstance(result['synthesis_results'], dict):
            synthesis = result['synthesis_results']
            if 'synthesis_results' in synthesis:
                print(f"\nğŸ“ Synthesis: {synthesis.get('synthesis_results', 'N/A')}")
        
        # Display innovation stats
        if 'innovation_stats' in result:
            stats = result['innovation_stats']
            print(f"\nğŸš€ Innovation Statistics:")
            print(f"  â€¢ Adaptive Supervisor: {stats.get('adaptive_supervisor', 'N/A')}")
            print(f"  â€¢ Hierarchical Compression: {stats.get('hierarchical_compression', 'N/A')}")
            print(f"  â€¢ Multi-Model Orchestration: {stats.get('multi_model_orchestration', 'N/A')}")
            print(f"  â€¢ Continuous Verification: {stats.get('continuous_verification', 'N/A')}")
            print(f"  â€¢ Streaming Pipeline: {stats.get('streaming_pipeline', 'N/A')}")
            print(f"  â€¢ Universal MCP Hub: {stats.get('universal_mcp_hub', 'N/A')}")
            print(f"  â€¢ Adaptive Context Window: {stats.get('adaptive_context_window', 'N/A')}")
            print(f"  â€¢ Production-Grade Reliability: {stats.get('production_grade_reliability', 'N/A')}")
        
        # Display system health
        if 'system_health' in result:
            health = result['system_health']
            print(f"\nğŸ¥ System Health: {health.get('overall_status', 'Unknown')}")
            print(f"  â€¢ Health Score: {health.get('health_score', 'N/A')}")
            print(f"  â€¢ Monitoring Active: {health.get('monitoring_active', 'N/A')}")
        
        print("=" * 80)
    
    async def run_mcp_server(self):
        """MCP ì„œë²„ ì‹¤í–‰"""
        await self.mcp_hub.initialize_mcp()
    
    async def run_mcp_client(self):
        """MCP í´ë¼ì´ì–¸íŠ¸ ì‹¤í–‰"""
        await self.mcp_hub.initialize_mcp()
    
    def run_web_app(self):
        """ì›¹ ì•± ì‹¤í–‰"""
        return self.web_manager.start_web_app()
    
    async def run_health_check(self):
        """Run comprehensive health check for all system components."""
        logger.info("ğŸ¥ Running comprehensive health check...")
        
        # Check MCP tools health
        if self.config.mcp.enabled:
            # MCP Hub health check
            logger.info("MCP Hub initialized and ready")
        
        # Check system health
        system_health = self.health_monitor.get_system_health()
        logger.info(f"System Health: {system_health.get('overall_status', 'Unknown')}")
        
        # Check web app health
        web_health = await self.web_manager.get_web_app_health()
        logger.info(f"Web App Health: {web_health.get('status', 'Unknown')}")
        
        logger.info("âœ… Health check completed")
    
    async def check_mcp_servers(self):
        """MCP ì„œë²„ ì—°ê²° ìƒíƒœ í™•ì¸."""
        logger.info("ğŸ“Š Checking MCP server connections...")
        
        if not self.config.mcp.enabled:
            logger.warning("MCP is disabled")
            return
        
        try:
            # MCP Hub ì´ˆê¸°í™” í™•ì¸ - ì´ë¯¸ ì—°ê²°ëœ ì„œë²„ê°€ ìˆìœ¼ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
            if self.mcp_hub.mcp_sessions:
                logger.info(f"Found {len(self.mcp_hub.mcp_sessions)} existing MCP server connections")
            else:
                logger.info("No existing connections. Will attempt quick connection tests for each server...")
            
            # ì„œë²„ ìƒíƒœ í™•ì¸ (ê° ì„œë²„ì— ëŒ€í•´ ì§§ì€ íƒ€ì„ì•„ì›ƒìœ¼ë¡œ ì—°ê²° ì‹œë„)
            logger.info("Checking MCP server connection status...")
            server_status = await self.mcp_hub.check_mcp_servers()
            
            # ê²°ê³¼ ì¶œë ¥
            print("\n" + "=" * 80)
            print("ğŸ“Š MCP ì„œë²„ ì—°ê²° ìƒíƒœ í™•ì¸")
            print("=" * 80)
            print(f"ì „ì²´ ì„œë²„ ìˆ˜: {server_status['total_servers']}")
            print(f"ì—°ê²°ëœ ì„œë²„: {server_status['connected_servers']}")
            print(f"ì—°ê²°ë¥ : {server_status['summary']['connection_rate']}")
            print(f"ì „ì²´ ì‚¬ìš© ê°€ëŠ¥í•œ Tool ìˆ˜: {server_status['summary']['total_tools_available']}")
            print("\n")
            
            for server_name, info in server_status["servers"].items():
                status_icon = "âœ…" if info["connected"] else "âŒ"
                print(f"{status_icon} ì„œë²„: {server_name}")
                print(f"   íƒ€ì…: {info['type']}")
                
                if info["type"] == "http":
                    print(f"   URL: {info.get('url', 'unknown')}")
                else:
                    cmd = info.get('command', 'unknown')
                    args_preview = ' '.join(info.get('args', [])[:3])
                    print(f"   ëª…ë ¹ì–´: {cmd} {args_preview}...")
                
                print(f"   ì—°ê²° ìƒíƒœ: {'ì—°ê²°ë¨' if info['connected'] else 'ì—°ê²° ì•ˆ ë¨'}")
                print(f"   ì œê³µ Tool ìˆ˜: {info['tools_count']}")
                
                if info["tools"]:
                    print(f"   Tool ëª©ë¡:")
                    for tool in info["tools"][:5]:  # ì²˜ìŒ 5ê°œë§Œ í‘œì‹œ
                        registered_name = f"{server_name}::{tool}"
                        print(f"     - {registered_name}")
                    if len(info["tools"]) > 5:
                        print(f"     ... ë° {len(info['tools']) - 5}ê°œ ë”")
                
                if info.get("error"):
                    print(f"   âš ï¸ ì˜¤ë¥˜: {info['error']}")
                print()
            
            print("=" * 80)
            
            # ìš”ì•½ ì •ë³´ ë¡œê¹…
            logger.info(f"MCP ì„œë²„ í™•ì¸ ì™„ë£Œ: {server_status['summary']['connection_rate']} ì—°ê²°")
            
        except Exception as e:
            logger.error(f"MCP ì„œë²„ í™•ì¸ ì‹¤íŒ¨: {e}")
            import traceback
            logger.error(traceback.format_exc())


async def main():
    """Main function - 9ê°€ì§€ í•µì‹¬ í˜ì‹  í†µí•© ì‹¤í–‰ ì§„ì…ì  (Suna-style CLI)"""
    # Python ì¢…ë£Œ ì‹œ ë°œìƒí•˜ëŠ” async generator ì •ë¦¬ ì˜¤ë¥˜ ë¬´ì‹œ
    def ignore_async_gen_errors(loop, context):
        """anyio cancel scope ë° async generator ì¢…ë£Œ ì˜¤ë¥˜ ë¬´ì‹œ"""
        exception = context.get('exception')
        if exception:
            error_msg = str(exception)
            # anyio cancel scope ì˜¤ë¥˜ëŠ” ë¬´ì‹œ
            if isinstance(exception, RuntimeError) and ("cancel scope" in error_msg.lower() or "different task" in error_msg.lower()):
                return  # ë¬´ì‹œ
            # async generator ì¢…ë£Œ ì˜¤ë¥˜ëŠ” ë¬´ì‹œ
            if isinstance(exception, GeneratorExit) or "async_generator" in error_msg.lower():
                return  # ë¬´ì‹œ
        # ê¸°íƒ€ ì˜¤ë¥˜ëŠ” ê¸°ë³¸ handlerë¡œ ì „ë‹¬
        loop.set_exception_handler(None)
        loop.call_exception_handler(context)
        loop.set_exception_handler(ignore_async_gen_errors)

    # asyncio exception handler ì„¤ì •
    loop = asyncio.get_running_loop()
    loop.set_exception_handler(ignore_async_gen_errors)

    # Suna-style ì„œë¸Œì»¤ë§¨ë“œ êµ¬ì¡°ë¡œ ê°œì„ 
    parser = argparse.ArgumentParser(
        description="SparkleForge - Autonomous Multi-Agent Research System",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
SparkleForge: Where Ideas Sparkle and Get Forged âš’ï¸âœ¨

EXAMPLES:
  # ì—°êµ¬ ì‹¤í–‰
  python main.py run "ì¸ê³µì§€ëŠ¥ì˜ ë¯¸ë˜ ì „ë§"

  # ì›¹ ëŒ€ì‹œë³´ë“œ ì‹œì‘
  python main.py web

  # ì‹œìŠ¤í…œ í—¬ìŠ¤ì²´í¬
  python main.py health

  # MCP ì„œë²„ ìƒíƒœ í™•ì¸
  python main.py mcp status

  # ë„êµ¬ ëª©ë¡ í™•ì¸
  python main.py tools list

  # Docker ì„œë¹„ìŠ¤ ê´€ë¦¬
  python main.py docker up
        """
    )

    # ì„œë¸Œì»¤ë§¨ë“œ ì¶”ê°€
    subparsers = parser.add_subparsers(dest='command', help='Available commands')

    # run ì»¤ë§¨ë“œ
    run_parser = subparsers.add_parser('run', help='Execute research request')
    run_parser.add_argument('query', help='Research query')
    run_parser.add_argument('--output', '-o', help='Output file path')
    run_parser.add_argument('--format', choices=['json', 'markdown', 'html'], default='markdown', help='Output format')
    run_parser.add_argument('--streaming', action='store_true', help='Enable streaming output')

    # web ì»¤ë§¨ë“œ
    web_parser = subparsers.add_parser('web', help='Start web dashboard')
    web_parser.add_argument('--port', default='8501', help='Web server port')
    web_parser.add_argument('--host', default='0.0.0.0', help='Web server host')

    # mcp ì»¤ë§¨ë“œ
    mcp_parser = subparsers.add_parser('mcp', help='MCP server management')
    mcp_subparsers = mcp_parser.add_subparsers(dest='mcp_command', help='MCP commands')

    # mcp status
    mcp_status_parser = mcp_subparsers.add_parser('status', help='Check MCP server status')
    mcp_status_parser.add_argument('--verbose', '-v', action='store_true', help='Verbose output')

    # mcp server
    mcp_server_parser = mcp_subparsers.add_parser('server', help='Start MCP server')

    # health ì»¤ë§¨ë“œ
    health_parser = subparsers.add_parser('health', help='System health check')
    health_parser.add_argument('--detailed', action='store_true', help='Detailed health report')

    # tools ì»¤ë§¨ë“œ
    tools_parser = subparsers.add_parser('tools', help='Tool management')
    tools_subparsers = tools_parser.add_subparsers(dest='tools_command', help='Tool commands')

    # tools list
    tools_list_parser = tools_subparsers.add_parser('list', help='List available tools')
    tools_list_parser.add_argument('--category', help='Filter by category')

    # tools test
    tools_test_parser = tools_subparsers.add_parser('test', help='Test tool functionality')
    tools_test_parser.add_argument('tool_name', help='Tool name to test')

    # docker ì»¤ë§¨ë“œ
    docker_parser = subparsers.add_parser('docker', help='Docker service management')
    docker_subparsers = docker_parser.add_subparsers(dest='docker_command', help='Docker commands')

    # docker up
    docker_up_parser = docker_subparsers.add_parser('up', help='Start Docker services')
    docker_up_parser.add_argument('--build', action='store_true', help='Rebuild images')
    docker_up_parser.add_argument('--profile', action='append', help='Enable specific profiles (e.g., sandbox)')

    # docker down
    docker_down_parser = docker_subparsers.add_parser('down', help='Stop Docker services')
    docker_down_parser.add_argument('--volumes', action='store_true', help='Remove volumes')
    docker_down_parser.add_argument('--images', action='store_true', help='Remove images')

    # docker logs
    docker_logs_parser = docker_subparsers.add_parser('logs', help='Show service logs')
    docker_logs_parser.add_argument('service', nargs='?', help='Specific service name')
    docker_logs_parser.add_argument('--follow', '-f', action='store_true', help='Follow log output')

    # docker status
    docker_status_parser = docker_subparsers.add_parser('status', help='Show service status')

    # docker build
    docker_build_parser = docker_subparsers.add_parser('build', help='Build Docker images')
    docker_build_parser.add_argument('--no-cache', action='store_true', help='Build without cache')

    # docker restart
    docker_restart_parser = docker_subparsers.add_parser('restart', help='Restart Docker services')
    docker_restart_parser.add_argument('service', nargs='?', help='Specific service name')

    # setup ì»¤ë§¨ë“œ
    setup_parser = subparsers.add_parser('setup', help='System setup and configuration')
    setup_parser.add_argument('--force', action='store_true', help='Force reinstallation')

    # cli ì»¤ë§¨ë“œ (CLI ì—ì´ì „íŠ¸ ê´€ë¦¬)
    cli_parser = subparsers.add_parser('cli', help='CLI agent management')
    cli_subparsers = cli_parser.add_subparsers(dest='cli_command', help='CLI agent commands')

    # cli list
    cli_list_parser = cli_subparsers.add_parser('list', help='List available CLI agents')

    # cli test
    cli_test_parser = cli_subparsers.add_parser('test', help='Test CLI agent')
    cli_test_parser.add_argument('agent_name', help='CLI agent name to test')

    # cli run
    cli_run_parser = cli_subparsers.add_parser('run', help='Run query with CLI agent')
    cli_run_parser.add_argument('agent_name', help='CLI agent name')
    cli_run_parser.add_argument('query', help='Query to execute')
    cli_run_parser.add_argument('--mode', help='Execution mode')
    cli_run_parser.add_argument('--files', nargs='*', help='Related files')

    # í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•œ ê¸°ì¡´ ì¸ìë“¤ (deprecated)
    parser.add_argument("--request", "--query", dest="legacy_request", help="Legacy: Use 'run' command instead")
    parser.add_argument("--web", action="store_true", dest="legacy_web", help="Legacy: Use 'web' command instead")
    parser.add_argument("--mcp-server", action="store_true", dest="legacy_mcp_server", help="Legacy: Use 'mcp server' command instead")
    parser.add_argument("--health-check", action="store_true", dest="legacy_health", help="Legacy: Use 'health' command instead")
    parser.add_argument("--check-mcp-servers", action="store_true", dest="legacy_mcp_status", help="Legacy: Use 'mcp status' command instead")
    mode_group.add_argument("--daemon", action="store_true", help="Start as long-running daemon (24/7 mode)")
    
    # Optional arguments
    parser.add_argument("--output", help="Output file path for research results")
    parser.add_argument("--output-format", choices=["text", "json", "stream-json"], default="text", help="Output format for headless mode")
    parser.add_argument("--config", help="Configuration file path")
    parser.add_argument("--checkpoint", help="Checkpoint file path to save/restore conversation")
    parser.add_argument("--restore", help="Restore conversation from checkpoint file")
    
    # Long-running daemon options
    parser.add_argument("--auto-save-interval", type=int, default=300, help="Auto-save interval in seconds (default: 300)")
    parser.add_argument("--max-memory-mb", type=float, default=2048, help="Max memory usage in MB before restart (default: 2048)")
    parser.add_argument("--max-uptime-hours", type=float, default=0, help="Max uptime in hours before restart (0=unlimited, default: 0)")
    parser.add_argument("--no-auto-restart", action="store_true", help="Disable auto-restart on errors")
    parser.add_argument("--format", choices=["json", "yaml", "txt"], default="json", help="Output format for research results")
    parser.add_argument("--streaming", action="store_true", help="Enable streaming pipeline for real-time results")
    parser.add_argument("--verbose", "-v", action="store_true", help="Enable verbose logging")
    
    args = parser.parse_args()

    # í•˜ìœ„ í˜¸í™˜ì„± ì²˜ë¦¬
    if args.legacy_request:
        args.command = 'run'
        args.query = args.legacy_request
        args.streaming = True
    elif args.legacy_web:
        args.command = 'web'
    elif args.legacy_mcp_server:
        args.command = 'mcp'
        args.mcp_command = 'server'
    elif args.legacy_health:
        args.command = 'health'
    elif args.legacy_mcp_status:
        args.command = 'mcp'
        args.mcp_command = 'status'

    # ê¸°ë³¸ê°’ ì„¤ì •
    if not args.command:
        args.command = 'run'  # ê¸°ë³¸ì€ run ì»¤ë§¨ë“œ
        if not hasattr(args, 'query') or not args.query:
            # ì¿¼ë¦¬ê°€ ì—†ìœ¼ë©´ ì¸í„°ë™í‹°ë¸Œ ëª¨ë“œë¡œ ì „í™˜
            args.command = 'interactive'

    # ì„œë¸Œì»¤ë§¨ë“œ ì²˜ë¦¬
    if args.command == 'run':
        await handle_run_command(args)
    elif args.command == 'web':
        await handle_web_command(args)
    elif args.command == 'mcp':
        await handle_mcp_command(args)
    elif args.command == 'health':
        await handle_health_command(args)
    elif args.command == 'tools':
        await handle_tools_command(args)
    elif args.command == 'docker':
        await handle_docker_command(args)
    elif args.command == 'setup':
        await handle_setup_command(args)
    elif args.command == 'cli':
        await handle_cli_command(args)
    elif args.command == 'interactive':
        await handle_interactive_command(args)
    else:
        parser.print_help()
    
    # REPL ëª¨ë“œì—ì„œëŠ” ëª¨ë“  ë¡œê·¸ë¥¼ ì™„ì „íˆ ì–µì œ (ERRORë§Œ í‘œì‹œ)
    if is_repl_mode:
        import warnings
        # ëª¨ë“  ë¡œê±°ë¥¼ ERROR ë ˆë²¨ë¡œ ì„¤ì • (WARNING, INFO, DEBUG ëª¨ë‘ ì–µì œ)
        logging.getLogger().setLevel(logging.ERROR)
        
        # íŠ¹ì • ëª¨ë“ˆë“¤ì˜ ë¡œê±°ë„ ERRORë¡œ ì„¤ì •
        for logger_name in [
            '__main__', 'src', 'src.core', 'src.core.era_server_manager',
            'src.core.agent_orchestrator', 'src.core.mcp_integration',
            'src.core.shared_memory', 'src.core.skills_manager',
            'src.core.prompt_refiner_wrapper', 'root',
            'streamlit', 'streamlit.runtime', 'local_researcher'
        ]:
            logging.getLogger(logger_name).setLevel(logging.ERROR)
        
        # warningsë„ ì™„ì „íˆ ì–µì œ
        warnings.filterwarnings('ignore')
    else:
        # Set logging level
        if args.verbose:
            logging.getLogger().setLevel(logging.DEBUG)
    
    # Create logs directory
    logs_dir = project_root / "logs"
    logs_dir.mkdir(exist_ok=True)

    # Initialize enhanced systems
    from src.utils.output_manager import (
        UserCenteredOutputManager,
        set_output_manager,
        OutputLevel,
        OutputFormat
    )
    from src.core.error_handler import ErrorHandler, set_error_handler
    from src.core.progress_tracker import ProgressTracker, set_progress_tracker

    # ì¶œë ¥ ë§¤ë‹ˆì € ì´ˆê¸°í™”
    output_manager = UserCenteredOutputManager(
        output_level=OutputLevel.USER,
        output_format=OutputFormat.TEXT,
        enable_colors=True,
        stream_output=True,
        show_progress=True
    )
    set_output_manager(output_manager)

    # ì—ëŸ¬ í•¸ë“¤ëŸ¬ ì´ˆê¸°í™”
    error_handler = ErrorHandler(
        log_errors=True,
        enable_recovery=True
    )
    set_error_handler(error_handler)

    # ì§„í–‰ ìƒí™© ì¶”ì ê¸° ì´ˆê¸°í™” (ì„¸ì…˜ë³„ë¡œ ìƒì„±)
    session_id = f"session_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    progress_tracker = ProgressTracker(
        session_id=session_id,
        enable_real_time_updates=True,
        update_interval=1.0
    )
    set_progress_tracker(progress_tracker)

    # ì§„í–‰ ìƒí™© ì¶”ì ê¸° ì½œë°± ì„¤ì • (ì¶œë ¥ ë§¤ë‹ˆì €ì™€ ì—°ë™)
    last_stage = [None]  # í´ë¡œì €ë¥¼ ìœ„í•œ ë¦¬ìŠ¤íŠ¸ (nonlocal ëŒ€ì‹ )
    last_progress = [0]
    
    async def progress_callback(workflow_progress):
        """ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸ ì‹œ ì¶œë ¥ ë§¤ë‹ˆì €ì— í‘œì‹œ."""
        try:
            progress_pct = int(workflow_progress.overall_progress * 100)
            stage_name = workflow_progress.current_stage.value

            # ë‹¨ê³„ê°€ ë³€ê²½ë˜ì—ˆì„ ë•Œë§Œ start_progress í˜¸ì¶œ
            if last_stage[0] != stage_name:
                last_stage[0] = stage_name
                eta_str = ""
                if workflow_progress.estimated_completion:
                    eta_seconds = max(0, int(workflow_progress.estimated_completion - time.time()))
                    eta_str = f" (ì˜ˆìƒ {eta_seconds}ì´ˆ ë‚¨ìŒ)"

                # ì§„í–‰ë¥  ë°” ìŠ¤íƒ€ì¼ë¡œ í‘œì‹œ (ë‹¨ê³„ ë³€ê²½ ì‹œì—ë§Œ)
                await output_manager.start_progress(
                    stage_name,
                    100,
                    f"{progress_pct}% ì™„ë£Œ",
                    workflow_progress.estimated_completion
                )
            
            # ì§„í–‰ë¥ ì´ ì‹¤ì œë¡œ ë³€ê²½ë˜ì—ˆì„ ë•Œë§Œ ì—…ë°ì´íŠ¸ (1% ì´ìƒ ì°¨ì´)
            if abs(progress_pct - last_progress[0]) >= 1 or progress_pct == 100:
                last_progress[0] = progress_pct
                await output_manager.update_progress(progress_pct)

        except Exception as e:
            logger.warning(f"Progress callback failed: {e}")

    progress_tracker.add_progress_callback(progress_callback)
    
    # ì‹œìŠ¤í…œ ì´ˆê¸°í™” (REPL ëª¨ë“œê°€ ì•„ë‹ ë•Œë§Œ ì „ì²´ ì´ˆê¸°í™”)
    system = None
    if not is_repl_mode:
        # ERA ì„œë²„ ì´ˆê¸°í™” ë° ì‹œì‘
        try:
            from src.core.researcher_config import get_era_config
            from src.core.era_server_manager import get_era_server_manager
            
            era_config = get_era_config()
            if era_config.enabled:
                # ERA server ì´ˆê¸°í™”
                era_manager = get_era_server_manager()
                await era_manager.ensure_server_running_with_retry()
            else:
                logger.debug("ERA is disabled in configuration")
        except ImportError as e:
            logger.debug(f"ERA modules not available: {e}")
        except Exception as e:
            logger.warning(f"âš ï¸ ERA initialization failed: {e}")
            logger.debug("Code execution features may be limited", exc_info=True)
        
        # Initialize system
        system = AutonomousResearchSystem()
    
    try:
        # ì²´í¬í¬ì¸íŠ¸ ë³µì› (ìˆëŠ” ê²½ìš°)
        if args.restore:
            from src.core.checkpoint_manager import CheckpointManager
            checkpoint_manager = CheckpointManager()
            restored_state = await checkpoint_manager.restore_checkpoint(args.restore)
            if restored_state:
                logger.info(f"âœ… Restored checkpoint: {args.restore}")
                # ë³µì›ëœ ìƒíƒœë¥¼ ì‚¬ìš©í•˜ì—¬ ê³„ì† ì§„í–‰
            else:
                logger.error(f"âŒ Failed to restore checkpoint: {args.restore}")
                return
        
        # Headless ëª¨ë“œ (ë¹„ëŒ€í™”í˜•)
        if args.prompt:
            from src.core.autonomous_orchestrator import AutonomousOrchestrator
            orchestrator = AutonomousOrchestrator()
            
            result = await orchestrator.run_research(args.prompt)
            
            # ì¶œë ¥ í˜•ì‹ì— ë”°ë¼ ê²°ê³¼ ì¶œë ¥
            if args.output_format == "json":
                import json
                output = json.dumps(result, indent=2, ensure_ascii=False)
                print(output)
            elif args.output_format == "stream-json":
                # ìŠ¤íŠ¸ë¦¬ë° JSON (newline-delimited)
                import json
                if isinstance(result, dict):
                    for key, value in result.items():
                        print(json.dumps({key: value}, ensure_ascii=False))
                else:
                    print(json.dumps(result, ensure_ascii=False))
            else:
                # ê¸°ë³¸ í…ìŠ¤íŠ¸ ì¶œë ¥
                if isinstance(result, dict):
                    if "content" in result:
                        print(result["content"])
                    elif "final_synthesis" in result:
                        print(result["final_synthesis"].get("content", ""))
                    else:
                        print(str(result))
                else:
                    print(str(result))
            
            # ì²´í¬í¬ì¸íŠ¸ ì €ì¥ (ìš”ì²­ëœ ê²½ìš°)
            if args.checkpoint:
                from src.core.checkpoint_manager import CheckpointManager
                checkpoint_manager = CheckpointManager()
                checkpoint_id = await checkpoint_manager.save_checkpoint(
                    state={"result": result, "prompt": args.prompt},
                    metadata={"mode": "headless", "output_format": args.output_format}
                )
                logger.info(f"âœ… Checkpoint saved: {checkpoint_id}")
            
            return
        
        # ê¸°ë³¸ ë™ì‘: REPL ëª¨ë“œ (ì•„ë¬´ ì˜µì…˜ë„ ì—†ìœ¼ë©´)
        # ë˜ëŠ” --cli ì˜µì…˜ì´ ìˆìœ¼ë©´
        if is_repl_mode:
            try:
                from src.cli.repl_cli import REPLCLI
                cli = REPLCLI()
                await cli.run()
            except (EOFError, KeyboardInterrupt, SystemExit):
                # ì •ìƒ ì¢…ë£Œ
                pass
            finally:
                pass
            return
        
        # Interactive ëª¨ë“œ (ê¸°ì¡´)
        if args.interactive:
            from src.cli.interactive_cli import InteractiveCLI
            from src.core.scheduler import get_scheduler
            
            # ìŠ¤ì¼€ì¤„ëŸ¬ ì´ˆê¸°í™” ë° ì‹œì‘
            scheduler = get_scheduler()
            
            # ì‹¤í–‰ ì½œë°± ì„¤ì •
            async def execute_scheduled_query(user_query: str, session_id: str):
                from src.core.autonomous_orchestrator import AutonomousOrchestrator
                orchestrator = AutonomousOrchestrator()
                return await orchestrator.execute_full_research_workflow(user_query)
            
            scheduler.set_execution_callback(execute_scheduled_query)
            await scheduler.start()
            
            cli = InteractiveCLI()
            try:
                await cli.run()
            finally:
                await scheduler.stop()
            return
        
        if args.request:
            # CLI Research Mode with 8 innovations
            logger.info("ğŸš€ Starting Local Researcher with enhanced systems...")

            # ì§„í–‰ ìƒí™© ì¶”ì  ì‹œì‘
            await progress_tracker.start_tracking()
            
            # ì´ˆê¸°í™” ë‹¨ê³„ ëª…ì‹œì ìœ¼ë¡œ ì„¤ì •
            from src.core.progress_tracker import WorkflowStage
            progress_tracker.set_workflow_stage(WorkflowStage.INITIALIZING, {"message": "ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘..."})

            # ì›Œí¬í”Œë¡œìš° ì‹œì‘ ì•Œë¦¼
            await output_manager.output(
                f"ğŸ”¬ ì—°êµ¬ ì£¼ì œ: {args.request}",
                level=OutputLevel.USER
            )
            await output_manager.output(
                "ì‹¤ì‹œê°„ ì§„í–‰ ìƒí™© ì¶”ì  ë° í–¥ìƒëœ ì—ëŸ¬ ì²˜ë¦¬ê°€ í™œì„±í™”ë˜ì—ˆìŠµë‹ˆë‹¤.",
                level=OutputLevel.SERVICE
            )

            # ì—°êµ¬ ì‹¤í–‰
            await system.run_research(
                args.request, 
                args.output, 
                streaming=args.streaming,
                output_format=args.format
            )
            
        elif args.web:
            # Web Application Mode with Streaming Pipeline
            system.run_web_app()
            
        elif args.mcp_server:
            # MCP Server Mode with Universal MCP Hub - ì‹¤ì œ ì—°ê²° ìˆ˜í–‰
            logger.info("Initializing MCP servers...")
            try:
                await system.mcp_hub.initialize_mcp()
                logger.info("âœ… MCP servers initialized")
                
                # ì—°ê²°ëœ ì„œë²„ ìƒíƒœ ì¶œë ¥
                if system.mcp_hub.mcp_sessions:
                    print("\n" + "=" * 80)
                    print("âœ… MCP ì„œë²„ ì—°ê²° ì™„ë£Œ")
                    print("=" * 80)
                    for server_name in system.mcp_hub.mcp_sessions.keys():
                        tools_count = len(system.mcp_hub.mcp_tools_map.get(server_name, {}))
                        print(f"âœ… {server_name}: {tools_count} tools available")
                    print("=" * 80)
                    print("\nMCP Hub is running. Press Ctrl+C to stop.")
                    
                    # ê³„ì† ì‹¤í–‰ ëŒ€ê¸°
                    try:
                        await asyncio.sleep(3600)  # 1ì‹œê°„ ëŒ€ê¸° (ë˜ëŠ” Ctrl+Cë¡œ ì¢…ë£Œ)
                    except KeyboardInterrupt:
                        logger.info("Shutting down MCP Hub...")
                else:
                    logger.warning("âš ï¸ No MCP servers connected")
                    sys.exit(1)
            except Exception as e:
                logger.error(f"Failed to initialize MCP servers: {e}")
                sys.exit(1)
            
        elif args.mcp_client:
            # MCP Client Mode with Smart Tool Selection
            success = await system.run_mcp_client()
            if not success:
                sys.exit(1)
            
        elif args.health_check:
            # Health Check Mode
            await system.run_health_check()
        
        elif args.check_mcp_servers:
            # MCP ì„œë²„ í™•ì¸ ëª¨ë“œ
            await system.check_mcp_servers()
            
    except KeyboardInterrupt:
        logger.info("Operation cancelled by user (KeyboardInterrupt)")
        if system is not None:
            system._shutdown_requested = True
            try:
                await system._graceful_shutdown()
            except Exception as e:
                logger.error(f"Error during shutdown: {e}")
        # sys.exit(0) ì œê±° - asyncio.run()ì´ ìë™ìœ¼ë¡œ ì²˜ë¦¬
    except asyncio.CancelledError:
        # ì·¨ì†Œëœ ê²½ìš° ì •ë¦¬ í›„ ì¢…ë£Œ
        logger.info("Operation cancelled")
        if system is not None:
            system._shutdown_requested = True
            try:
                await system._graceful_shutdown()
            except Exception as e:
                logger.error(f"Error during shutdown: {e}")
        # asyncio.CancelledErrorëŠ” ë‹¤ì‹œ raiseí•˜ì—¬ ì •ìƒì ì¸ ì·¨ì†Œ íë¦„ ìœ ì§€
        raise
    except Exception as e:
        # í–¥ìƒëœ ì—ëŸ¬ ì²˜ë¦¬
        from src.core.error_handler import ErrorContext, ErrorCategory, ErrorSeverity
        import traceback

        error_context = ErrorContext(
            component="main",
            operation="run_research" if args.request else "system_operation",
            session_id=session_id
        )

        await error_handler.handle_error(
            e,
            category=ErrorCategory.UNKNOWN,
            severity=ErrorSeverity.HIGH,
            context=error_context,
            custom_message=f"ì‹œìŠ¤í…œ ì‹¤í–‰ ì¤‘ ì¹˜ëª…ì  ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
        )

        if system is not None:
            system._shutdown_requested = True
            try:
                await system._graceful_shutdown()
            except Exception as e2:
                logger.error(f"Error during shutdown: {e2}")
        # ì—ëŸ¬ ë°œìƒ ì‹œ ì¢…ë£Œ ì½”ë“œ 1ë¡œ ì¢…ë£Œ
        sys.exit(1)
    finally:
        # ì§„í–‰ ìƒí™© ì¶”ì  ì¤‘ì§€ ë° ìš”ì•½ ì¶œë ¥
        try:
            await progress_tracker.stop_tracking()

            if args.request:
                # ì›Œí¬í”Œë¡œìš° ì™„ë£Œ ìš”ì•½
                await output_manager.complete_progress(success=True)
                await output_manager.output_workflow_summary()

                # ì§„í–‰ ìƒí™© í†µê³„ ì¶œë ¥
                stats = progress_tracker.get_statistics()
                await output_manager.output(
                    f"ğŸ“ˆ ì„¸ì…˜ í†µê³„: {stats['total_agents_created']}ê°œ ì—ì´ì „íŠ¸ ìƒì„±, "
                    f"{stats['agents_completed']}ê°œ ì™„ë£Œ, {stats['agents_failed']}ê°œ ì‹¤íŒ¨",
                    level=OutputLevel.SERVICE
                )

        except Exception as e:
            logger.warning(f"Failed to finalize progress tracking: {e}")

        # ìµœì¢… ì •ë¦¬ ë³´ì¥ (systemì´ ì´ˆê¸°í™”ëœ ê²½ìš°ì—ë§Œ)
        if system is not None and hasattr(system, 'mcp_hub') and system.mcp_hub and hasattr(system.mcp_hub, 'mcp_sessions'):
            try:
                if system.mcp_hub.mcp_sessions:
                    logger.info("Final cleanup of MCP connections...")
                    await system.mcp_hub.cleanup()
                
                # MCP ë°±ê·¸ë¼ìš´ë“œ í—¬ìŠ¤ì²´í¬ ì„œë¹„ìŠ¤ ì¢…ë£Œ (ì„ íƒì )
                if hasattr(system, 'mcp_health_service') and system.mcp_health_service:
                    try:
                        await system.mcp_health_service.stop()
                        logger.info("âœ… MCP Health Background Service stopped")
                    except Exception as e:
                        logger.debug(f"Error stopping MCP health service: {e}")
            except Exception as e:
                logger.debug(f"Error in final cleanup: {e}")


# ì„œë¸Œì»¤ë§¨ë“œ í•¸ë“¤ëŸ¬ í•¨ìˆ˜ë“¤
async def handle_run_command(args):
    """ì—°êµ¬ ì‹¤í–‰ ì»¤ë§¨ë“œ ì²˜ë¦¬"""
    logger.info(f"ğŸ”¬ Starting research: {args.query}")

    try:
        # Autonomous Orchestrator ì´ˆê¸°í™”
        orchestrator = AutonomousOrchestrator()

        # ì—°êµ¬ ì‹¤í–‰
        result = await orchestrator.execute_research(
            research_query=args.query,
            output_format=args.format,
            streaming=args.streaming
        )

        # ê²°ê³¼ ì¶œë ¥
        if args.output:
            with open(args.output, 'w', encoding='utf-8') as f:
                if args.format == 'json':
                    json.dump(result, f, ensure_ascii=False, indent=2)
                else:
                    f.write(result)
            logger.info(f"âœ… Results saved to {args.output}")
        else:
            print(result)

    except Exception as e:
        logger.error(f"âŒ Research failed: {e}")
        return 1
    return 0


async def handle_web_command(args):
    """ì›¹ ëŒ€ì‹œë³´ë“œ ì‹œì‘ ì»¤ë§¨ë“œ ì²˜ë¦¬"""
    logger.info("ğŸŒ Starting web dashboard...")

    web_manager = WebAppManager()
    os.environ["STREAMLIT_PORT"] = args.port
    os.environ["STREAMLIT_ADDRESS"] = args.host

    try:
        web_manager.start_web_app()
    except KeyboardInterrupt:
        logger.info("ğŸ›‘ Web dashboard stopped")
    except Exception as e:
        logger.error(f"âŒ Failed to start web dashboard: {e}")
        return 1
    return 0


async def handle_mcp_command(args):
    """MCP ê´€ë¦¬ ì»¤ë§¨ë“œ ì²˜ë¦¬"""
    if args.mcp_command == 'status':
        logger.info("ğŸ” Checking MCP server status...")

        try:
            # MCP Hub ì´ˆê¸°í™” ë° ìƒíƒœ í™•ì¸
            from src.core.mcp_integration import get_mcp_hub
            mcp_hub = get_mcp_hub()

            if args.verbose:
                await mcp_hub.initialize_mcp()

            server_status = await mcp_hub.check_mcp_servers()
            mcp_hub.print_server_status(server_status, verbose=args.verbose)

        except Exception as e:
            logger.error(f"âŒ MCP status check failed: {e}")
            return 1

    elif args.mcp_command == 'server':
        logger.info("ğŸš€ Starting MCP server...")

        try:
            # MCP ì„œë²„ ì‹œì‘
            mcp_manager = MCPManager()
            await mcp_manager.start_mcp_server()
        except KeyboardInterrupt:
            logger.info("ğŸ›‘ MCP server stopped")
        except Exception as e:
            logger.error(f"âŒ Failed to start MCP server: {e}")
            return 1

    return 0


async def handle_health_command(args):
    """ì‹œìŠ¤í…œ í—¬ìŠ¤ì²´í¬ ì»¤ë§¨ë“œ ì²˜ë¦¬"""
    logger.info("ğŸ¥ Running system health check...")

    try:
        health_monitor = HealthMonitor()

        if args.detailed:
            # ìƒì„¸ í—¬ìŠ¤ì²´í¬
            health_report = await health_monitor.run_comprehensive_health_check()
            health_monitor.print_detailed_health_report(health_report)
        else:
            # ê°„ë‹¨í•œ í—¬ìŠ¤ì²´í¬
            is_healthy = await health_monitor.quick_health_check()
            if is_healthy:
                logger.info("âœ… System is healthy")
            else:
                logger.error("âŒ System has issues")
                return 1

    except Exception as e:
        logger.error(f"âŒ Health check failed: {e}")
        return 1
    return 0


async def handle_tools_command(args):
    """ë„êµ¬ ê´€ë¦¬ ì»¤ë§¨ë“œ ì²˜ë¦¬"""
    if args.tools_command == 'list':
        logger.info("ğŸ”§ Listing available tools...")

        try:
            from src.core.mcp_integration import get_mcp_hub
            mcp_hub = get_mcp_hub()
            await mcp_hub.initialize_mcp()

            # ë„êµ¬ ëª©ë¡ ì¶œë ¥
            tools_by_category = {}
            for tool_name, tool_info in mcp_hub.tools.items():
                category = tool_info.get('category', 'unknown')
                if category not in tools_by_category:
                    tools_by_category[category] = []
                tools_by_category[category].append(tool_name)

            for category, tools in tools_by_category.items():
                if not args.category or args.category == category:
                    print(f"\nğŸ“‚ {category.upper()}:")
                    for tool in sorted(tools):
                        print(f"  - {tool}")

        except Exception as e:
            logger.error(f"âŒ Failed to list tools: {e}")
            return 1

    elif args.tools_command == 'test':
        logger.info(f"ğŸ§ª Testing tool: {args.tool_name}")

        try:
            from src.core.mcp_integration import get_mcp_hub
            mcp_hub = get_mcp_hub()
            await mcp_hub.initialize_mcp()

            # ë„êµ¬ í…ŒìŠ¤íŠ¸
            result = await mcp_hub.test_tool(args.tool_name)
            if result.get('success'):
                print(f"âœ… Tool {args.tool_name} is working")
            else:
                print(f"âŒ Tool {args.tool_name} failed: {result.get('error')}")
                return 1

        except Exception as e:
            logger.error(f"âŒ Tool test failed: {e}")
            return 1

    return 0


async def handle_docker_command(args):
    """Docker ì„œë¹„ìŠ¤ ê´€ë¦¬ ì»¤ë§¨ë“œ ì²˜ë¦¬ (Suna-style)"""
    import subprocess
    import os

    # Docker Compose ëª…ë ¹ì–´ ìë™ ê°ì§€
    def get_docker_compose_cmd():
        """Docker Compose ëª…ë ¹ì–´ ìë™ ê°ì§€"""
        # Docker Compose v2 (docker compose)
        try:
            subprocess.run(['docker', 'compose', 'version'], capture_output=True, check=True)
            return ['docker', 'compose']
        except (subprocess.CalledProcessError, FileNotFoundError):
            pass

        # Docker Compose v1 (docker-compose)
        try:
            subprocess.run(['docker-compose', 'version'], capture_output=True, check=True)
            return ['docker-compose']
        except (subprocess.CalledProcessError, FileNotFoundError):
            pass

        return None

    # Docker ì„¤ì¹˜ í™•ì¸
    def check_docker():
        """Docker ì„¤ì¹˜ ìƒíƒœ í™•ì¸"""
        try:
            subprocess.run(['docker', '--version'], capture_output=True, check=True)
            return True
        except (subprocess.CalledProcessError, FileNotFoundError):
            return False

    # Docker Compose íŒŒì¼ ì¡´ì¬ í™•ì¸
    def check_compose_file():
        """docker-compose.yaml íŒŒì¼ ì¡´ì¬ í™•ì¸"""
        compose_files = ['docker-compose.yaml', 'docker-compose.yml']
        for filename in compose_files:
            if (project_root / filename).exists():
                return filename
        return None

    # Docker í™˜ê²½ í™•ì¸
    if not check_docker():
        logger.error("âŒ Docker is not installed or not running")
        logger.info("Please install Docker: https://docs.docker.com/get-docker/")
        return 1

    compose_cmd = get_docker_compose_cmd()
    if not compose_cmd:
        logger.error("âŒ Docker Compose is not installed")
        logger.info("Please install Docker Compose: https://docs.docker.com/compose/install/")
        return 1

    compose_file = check_compose_file()
    if not compose_file:
        logger.error("âŒ docker-compose.yaml file not found")
        logger.info("Please ensure docker-compose.yaml exists in the project root")
        return 1

    if args.docker_command == 'up':
        logger.info("ğŸ³ Starting Docker services...")
        logger.info(f"Using Docker Compose: {' '.join(compose_cmd)}")
        logger.info(f"Compose file: {compose_file}")

        try:
            cmd = compose_cmd + ['-f', compose_file, 'up', '-d']
            if args.build:
                cmd.append('--build')
                logger.info("ğŸ”¨ Building images...")

            # í”„ë¡œí•„ ì§€ì› (ì˜ˆ: sandbox)
            if hasattr(args, 'profile') and args.profile:
                for profile in args.profile:
                    cmd.extend(['--profile', profile])
                    logger.info(f"ğŸ”§ Enabling profile: {profile}")

            # í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ (.env íŒŒì¼)
            env = os.environ.copy()
            env_file = project_root / '.env'
            if env_file.exists():
                logger.info("ğŸ“„ Loading environment from .env file")
                # .env íŒŒì¼ì—ì„œ í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ (ê°„ë‹¨í•œ êµ¬í˜„)
                with open(env_file, 'r') as f:
                    for line in f:
                        line = line.strip()
                        if line and not line.startswith('#') and '=' in line:
                            key, value = line.split('=', 1)
                            env[key] = value

            result = subprocess.run(cmd, cwd=str(project_root), env=env)
            if result.returncode == 0:
                logger.info("âœ… Docker services started successfully")
                logger.info("ğŸŒ Services:")
                logger.info("   - Backend API: http://localhost:8000")
                logger.info("   - Frontend: http://localhost:8501")
                logger.info("   - Redis: localhost:6379")
                logger.info("ğŸ“Š View logs: python main.py docker logs")
                logger.info("ğŸ“Š Check status: python main.py docker status")
            else:
                logger.error("âŒ Failed to start Docker services")
                return 1

        except Exception as e:
            logger.error(f"âŒ Docker command failed: {e}")
            return 1

    elif args.docker_command == 'down':
        logger.info("ğŸ³ Stopping Docker services...")

        try:
            cmd = compose_cmd + ['-f', compose_file, 'down']
            if hasattr(args, 'volumes') and args.volumes:
                cmd.append('--volumes')
                logger.info("ğŸ—‘ï¸ Removing volumes...")
            if hasattr(args, 'images') and args.images:
                cmd.append('--rmi')
                cmd.append('all')
                logger.info("ğŸ–¼ï¸ Removing images...")

            result = subprocess.run(cmd, cwd=str(project_root))
            if result.returncode == 0:
                logger.info("âœ… Docker services stopped successfully")
            else:
                logger.error("âŒ Failed to stop Docker services")
                return 1

        except Exception as e:
            logger.error(f"âŒ Docker command failed: {e}")
            return 1

    elif args.docker_command == 'logs':
        service_name = getattr(args, 'service', None)
        logger.info(f"ğŸ“Š Showing Docker service logs{f' for {service_name}' if service_name else ''}...")

        try:
            cmd = compose_cmd + ['-f', compose_file, 'logs']
            if service_name:
                cmd.append(service_name)
            if hasattr(args, 'follow') and args.follow:
                cmd.append('-f')

            if hasattr(args, 'follow') and args.follow:
                subprocess.run(cmd, cwd=str(project_root))
            else:
                result = subprocess.run(cmd, cwd=str(project_root), capture_output=True, text=True)
                if result.returncode == 0:
                    print(result.stdout)
                else:
                    logger.error("âŒ Failed to get logs")
                    return 1
        except KeyboardInterrupt:
            logger.info("ğŸ›‘ Stopped log monitoring")
        except Exception as e:
            logger.error(f"âŒ Failed to show logs: {e}")
            return 1

    elif args.docker_command == 'status':
        logger.info("ğŸ“Š Checking Docker service status...")

        try:
            cmd = compose_cmd + ['-f', compose_file, 'ps']
            result = subprocess.run(cmd, cwd=str(project_root), capture_output=True, text=True)
            if result.returncode == 0:
                print("ğŸ³ Docker Services Status:")
                print("=" * 50)
                print(result.stdout)
            else:
                logger.error("âŒ Failed to get service status")
                return 1
        except Exception as e:
            logger.error(f"âŒ Failed to check status: {e}")
            return 1

    elif args.docker_command == 'build':
        logger.info("ğŸ”¨ Building Docker images...")

        try:
            cmd = compose_cmd + ['-f', compose_file, 'build']
            if hasattr(args, 'no_cache') and args.no_cache:
                cmd.append('--no-cache')
                logger.info("ğŸ§¹ Building without cache...")

            result = subprocess.run(cmd, cwd=str(project_root))
            if result.returncode == 0:
                logger.info("âœ… Docker images built successfully")
            else:
                logger.error("âŒ Failed to build Docker images")
                return 1
        except Exception as e:
            logger.error(f"âŒ Build failed: {e}")
            return 1

    elif args.docker_command == 'restart':
        service_name = getattr(args, 'service', None)
        logger.info(f"ğŸ”„ Restarting Docker services{f' ({service_name})' if service_name else ''}...")

        try:
            cmd = compose_cmd + ['-f', compose_file, 'restart']
            if service_name:
                cmd.append(service_name)

            result = subprocess.run(cmd, cwd=str(project_root))
            if result.returncode == 0:
                logger.info("âœ… Docker services restarted successfully")
            else:
                logger.error("âŒ Failed to restart Docker services")
                return 1
        except Exception as e:
            logger.error(f"âŒ Restart failed: {e}")
            return 1

    else:
        logger.error(f"âŒ Unknown Docker command: {args.docker_command}")
        logger.info("Available commands: up, down, logs, status, build, restart")
        return 1

    return 0


async def handle_setup_command(args):
    """ì‹œìŠ¤í…œ ì„¤ì • ì»¤ë§¨ë“œ ì²˜ë¦¬"""
    logger.info("âš™ï¸ Running system setup...")

    try:
        # ê°„ë‹¨í•œ ì„¤ì • í™•ì¸
        required_files = [
            'pyproject.toml',
            'requirements.txt',
            'src/core/configs/researcher_config.yaml'
        ]

        missing_files = []
        for file_path in required_files:
            if not (project_root / file_path).exists():
                missing_files.append(file_path)

        if missing_files:
            logger.error(f"âŒ Missing required files: {missing_files}")
            return 1

        # í™˜ê²½ ë³€ìˆ˜ í™•ì¸
        required_env_vars = ['OPENROUTER_API_KEY']
        missing_env_vars = []
        for env_var in required_env_vars:
            if not os.getenv(env_var):
                missing_env_vars.append(env_var)

        if missing_env_vars:
            logger.warning(f"âš ï¸ Missing environment variables: {missing_env_vars}")
            logger.info("Please set these in your .env file or environment")

        logger.info("âœ… System setup completed")

    except Exception as e:
        logger.error(f"âŒ Setup failed: {e}")
        return 1
    return 0


async def handle_interactive_command(args):
    """ì¸í„°ë™í‹°ë¸Œ ëª¨ë“œ ì²˜ë¦¬"""
    logger.info("ğŸ’¬ Starting interactive mode...")

    try:
        # ê°„ë‹¨í•œ REPL êµ¬í˜„
        print("SparkleForge Interactive Mode")
        print("Type 'help' for commands, 'quit' to exit")
        print("-" * 50)

        while True:
            try:
                query = input("ğŸ” Research query: ").strip()
                if not query:
                    continue
                if query.lower() in ['quit', 'exit', 'q']:
                    break
                if query.lower() == 'help':
                    print("Commands:")
                    print("  help  - Show this help")
                    print("  quit  - Exit interactive mode")
                    print("  <query> - Execute research query")
                    continue

                # ì—°êµ¬ ì‹¤í–‰
                await handle_run_command(type('Args', (), {'query': query, 'output': None, 'format': 'markdown', 'streaming': True})())

            except KeyboardInterrupt:
                break
            except Exception as e:
                logger.error(f"âŒ Error: {e}")

        logger.info("ğŸ‘‹ Goodbye!")

    except Exception as e:
        logger.error(f"âŒ Interactive mode failed: {e}")
        return 1
    return 0


async def handle_cli_command(args):
    """CLI ì—ì´ì „íŠ¸ ê´€ë¦¬ ì»¤ë§¨ë“œ ì²˜ë¦¬"""
    from src.core.cli_agents.cli_agent_manager import get_cli_agent_manager
    from src.core.researcher_config import initialize_cli_agents

    # CLI ì—ì´ì „íŠ¸ ì´ˆê¸°í™”
    if not initialize_cli_agents():
        logger.warning("âš ï¸ CLI agents not enabled or failed to initialize")

    cli_manager = get_cli_agent_manager()

    if args.cli_command == 'list':
        logger.info("ğŸ¤– Available CLI Agents:")

        try:
            available_agents = cli_manager.get_available_agents()
            if not available_agents:
                logger.info("  No CLI agents configured")
                return 0

            for agent_name in available_agents:
                agent_info = cli_manager.get_agent_info(agent_name)
                if agent_info:
                    status = "âœ… Available" if agent_info.get('instance') else "âš ï¸ Configured"
                    logger.info(f"  - {agent_name}: {status}")
                    if agent_info.get('type'):
                        logger.info(f"    Type: {agent_info['type']}")
                    if agent_info.get('command'):
                        logger.info(f"    Command: {agent_info['command']}")
                else:
                    logger.info(f"  - {agent_name}: âŒ Not configured")

        except Exception as e:
            logger.error(f"âŒ Failed to list CLI agents: {e}")
            return 1

    elif args.cli_command == 'test':
        agent_name = args.agent_name
        logger.info(f"ğŸ§ª Testing CLI agent: {agent_name}")

        try:
            # í—¬ìŠ¤ì²´í¬
            agent = cli_manager.create_agent(agent_name)
            if not agent:
                logger.error(f"âŒ CLI agent not available: {agent_name}")
                return 1

            is_healthy = await agent.health_check()
            if is_healthy:
                logger.info(f"âœ… CLI agent {agent_name} is healthy")
                # ì¶”ê°€ ì •ë³´ í‘œì‹œ
                info = agent.get_info()
                logger.info(f"   Name: {info.get('name')}")
                logger.info(f"   Command: {info.get('command')}")
                logger.info(f"   Timeout: {info.get('timeout')}s")
            else:
                logger.error(f"âŒ CLI agent {agent_name} is not healthy")
                return 1

        except Exception as e:
            logger.error(f"âŒ CLI agent test failed: {e}")
            return 1

    elif args.cli_command == 'run':
        agent_name = args.agent_name
        query = args.query
        logger.info(f"ğŸš€ Running query with CLI agent: {agent_name}")
        logger.info(f"   Query: {query}")

        try:
            # ì‹¤í–‰ ì˜µì…˜ ì¤€ë¹„
            kwargs = {}
            if hasattr(args, 'mode') and args.mode:
                kwargs['mode'] = args.mode
            if hasattr(args, 'files') and args.files:
                kwargs['files'] = args.files

            # CLI ì—ì´ì „íŠ¸ë¡œ ì¿¼ë¦¬ ì‹¤í–‰
            result = await cli_manager.execute_with_agent(agent_name, query, **kwargs)

            if result.get('success'):
                logger.info("âœ… CLI agent execution successful")
                logger.info("ğŸ“„ Response:")
                print(result.get('response', ''))

                # ë©”íƒ€ë°ì´í„° í‘œì‹œ
                metadata = result.get('metadata', {})
                if metadata:
                    logger.info("ğŸ“Š Metadata:")
                    for key, value in metadata.items():
                        if key != 'execution_time':  # ì‹¤í–‰ ì‹œê°„ì€ ë³„ë„ë¡œ í‘œì‹œ
                            logger.info(f"   {key}: {value}")

                execution_time = metadata.get('execution_time', 0)
                if execution_time:
                    logger.info(f"â±ï¸ Execution time: {execution_time:.2f}s")

                confidence = result.get('confidence', 0)
                logger.info(f"ğŸ¯ Confidence: {confidence:.2f}")

            else:
                logger.error("âŒ CLI agent execution failed")
                error_msg = result.get('error', 'Unknown error')
                logger.error(f"   Error: {error_msg}")
                return 1

        except Exception as e:
            logger.error(f"âŒ CLI agent execution failed: {e}")
            return 1

    else:
        logger.error(f"âŒ Unknown CLI command: {args.cli_command}")
        logger.info("Available commands: list, test, run")
        return 1

    return 0


def main_entry():
    """Entry point for sparkle command."""
    asyncio.run(main())


if __name__ == "__main__":
    main_entry()